\section{Methodology}
\label{methodology}

\subsection{Sources}
\label{sources}
As there is no single authoritative source on software testing terminology,
we need to look at many to see how various terms are used in practice.
% Unfortunately, this brings to light a variety of discrepancies.
Starting from some set of sources, we then use
``snowball sampling'' (a ``method of \dots\ sample selection \dots\ used to
locate hidden populations'' \citep{Johnson2014}) to gather further sources%
\seeParAlways{undef-terms}. Sources with a similar degree of
``trustworthiness'' are grouped into categories; sources that are more
``trustworthy'':
\begin{enumerate}
    \item have gone through a peer-review process,
    \item are written by numerous, well-respected authors,
    \item are informed by many sources, and
    \item are accepted and used in the field of software.
\end{enumerate}

We therefore create the following categories, given in order of descending
trustworthiness: \stds{}, \metas{}, \texts{}, and \papers{}. \ifnotpaper
    Additionally, some information comes from \nameref{infers}. \fi Each category is
given a unique colour to better track how their information appears in relevant
graphs (see \Cref{fig:recovery-graph-current,fig:recovery-graph-proposed,%
    fig:scal-graph-current,fig:scal-graph-proposed,fig:perf-graph}). A summary
of how many sources comprise each category is given in \Cref{fig:sourceSummary}.

\subsubsection{\stdSources{1}}
\label{stds}
\stdSources{2}
\begin{itemize}
    \item Colored \textcolor{green}{green}
    \item Information on software development and testing from
          standards bodies \ifnotpaper\else (such as IEEE and ISO)\fi
    \item Written by reputable organizations for use in software engineering;
          for example, ``the purpose of the ISO/IEC/IEEE 29119 series is to
          define an internationally agreed set of standards for software
          testing that can be used by any organization when performing any form
          of software testing'' \ifnotpaper(\fi\citealp[p.~vii]{IEEE2022}%
          \ifnotpaper; similar in \citeyear[p.~ix]{IEEE2016})\fi
\end{itemize}

\subsubsection{\metaSources{1}}
\label{metas}
\metaSources{2}
\begin{itemize}
    \item Colored \textcolor{blue}{blue}
    \item Collections of relevant terminology (such as \acs{istqb}'s glossary,
          the \acs{swebok}, and \ifnotpaper \citeauthor{DoğanEtAl2014}\else
              Doğan et al.\fi's literature review \citeyearpar{DoğanEtAl2014})
    \item Built up from various sources, including \stds{}, and often written
          by a large organization (such as \acs{istqb}); the \acs{swebok} is
          ``proposed as a suitable foundation for government licensing, for the
          regulation of software engineers, and for the development of
          university curricula in software engineering''
          \citep[p.~xix]{KanerEtAl2011}
\end{itemize}

\subsubsection{\textSources{1}}
\label{texts}
\textSources{2}
\begin{itemize}
    \item Colored \textcolor{Maroon}{maroon}
    \item Textbooks trusted at McMaster \citep{Patton2006, PetersAndPedrycz2000,
              vanVliet2000} were the original (albeit ad hoc and arbitrary)
          starting point
    \item Written by smaller sets of authors, but with a formal review process
          before publication
    \item Used as resources for teaching software engineering and may be used
          as guides in industry
\end{itemize}

\subsubsection{\paperSources{1}}
\label{papers}
\paperSources{2}
\begin{itemize}
    \item Colored black
    \item Mainly consists of academic papers: journal articles, conference
          papers, reports \citep{Kam2008,Gerrard2000a,Gerrard2000b}, and a
          thesis \citep{Bas2024}
          % \item Includes less-formal classifications (such as
          %       \citep{KuļešovsEtAl2013})
    \item Written by much smaller sets of authors with unknown peer review
          processes
    \item Much less widespread than other categories of sources
    \item Many of these sources were investigated to ``fill in''
          missing definitions\seeParAlways{undef-terms}
    \item Also included (for brevity) are some less-than-academic sources to
          investigate how terms are used in practice, such as websites
          \citep{LambdaTest2024,Pandey2023} and a booklet \citep{SPICE2022}
\end{itemize}

\ifnotpaper
    \subsubsection{Inferences}
    \label{infers}
    While not as clear-cut as the other source categories, some information is
    inferred from various sources, such as ``surface-level'' analysis that
    follows straightforwardly without being explicitly stated in the text.
    Inferred relations are colored \textcolor{gray}{gray} and
    inferred discrepancies are given in \Cref{infer-discreps}.
\fi

\begin{figure}
    \centering
    \begin{tikzpicture}
        \pie[sum=auto, after number=, text=legend, thick,
            scale=\ifnotpaper0.7\else0.5\fi,
            every label/.style={align=left, scale=0.7}]
        {\stdSources{3}/\stds{},
            \metaSources{3}/\metas{},
            \textSources{3}/\texts{},
            \paperSources{3}/\papers{}}
    \end{tikzpicture}
    \caption{Summary of how many sources comprise each source category.}
    \label{fig:sourceSummary}
\end{figure}

% Moved here to display nicely in paper
\ifnotpaper\else\ieeeTestTermsTable{}\fi

\subsection{Terminology}

This research was intended to describe the current state of testing
terminology instead of prematurely applying any classifications to reduce bias.
Therefore, the notions of \nameref{categories-observ} and \nameref{par-chd-rels}
arose naturally from the literature. Even though these are ``results'' of this
research, they are defined here for clarity since they are used throughout this
thesis. We also define the notion of \nameref{rigidity}.

\subsubsection{Categories of Testing Approaches}
\label{categories-observ}

Different sources categorize software testing approaches in different ways%
\ifnotpaper
    ; while it is useful to record and think about these
    categorizations (see \Cref{testing-categories}), following one (or more)
    during the research
    stage could lead to bias and a prescriptive categorization, instead of letting
    one emerge descriptively during the analysis stage. Since these categorizations
    are not mutually exclusive, it also means that more than one could be useful
    (both in general and to this specific project).\newline \else.\fi\
\ifnotpaper \citet{IEEE2022} \else ISO/IEC and IEEE \cite{IEEE2022} \fi provide
a classification for different kinds of tests (see \refIEEETestTerms{}).
\ifnotpaper A deeper rationale for a proposed classification will be given
    during the analysis stage. \else Since
    this seems to be widely used (``test level'' and ``test type'' in particular)
    and is useful when focusing on a particular subset of testing, this terminology
    is used for now. \fi

\ifnotpaper
However, other sources \citep{BarbosaEtAl2006, SouzaEtAl2017} provide alternate
categories (see \refOtherTestTerms{}) which may be beneficial to investigate to
determine if this categorization is sufficient.

A ``metric'' categorization was considered at one point, but was decided
to be out of the scope of this project (see \Cref{scope}\thesisissueref{21,22}).
Related testing approaches may be grouped into a ``class'' or ``family'' to
group those with ``commonalities and well-identified variabilities that can be
instantiated'', where ``the commonalities are large and the variabilities
smaller''\thesisissueref{64}. Examples of these are the classes of
combinatorial \citep[p.~15]{IEEE2021} and data flow testing \citetext{p.~3} and the
family of performance-related testing \cite[p.~1187]{Moghadam2019}\footnote{The
    original source describes ``performance testing \dots\ as a family of
    performance-related testing techniques'', but it makes more sense to
    consider ``performance-related testing'' as the ``family'' with
    ``performance testing'' being one of the
    variabilities (see \Cref{perf-test-rec}).}, and may also be
implied for security testing, a test type that consists of ``a number of
techniques\footnote{This may or may not be \distinctIEEE{technique}}''
\cite[p.~40]{IEEE2021}.

It also seems that these categories are orthogonal. For example, ``a test type
can be performed at a single test level or across several test levels''
\ifnotpaper
    (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021})%
\else
    \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021}%
\fi, and ``Keyword-Driven Testing [sic] can be applied at all testing levels
(e.g. [sic] component testing, system testing) and for various types of testing
(e.g. [sic] functional testing\seeSectionFoot{corr-func-test}, reliability
testing)'' \citeyearpar[p.~4]{IEEE2016}. Due to this, a specific
test approach can be derived by combining test approaches from different
categories\ifnotpaper; see \Cref{orthogonal-tests} for some examples
of this\fi.

\begin{bigLandscape}
    \ieeeTestTermsTable{}
    \newpage
    \otherTestTermsTable{}
\end{bigLandscape}
% These were moved earlier to display nicely in paper
\fi

\subsubsection{Parent-Child Relations}
\label{par-chd-rels}
Many test approaches are multi-faceted and can be ``specialized'' into others,
such as \nameref{perf-test-rec}. These ``specializations'' will be referred to
as ``children'' or ``sub-approaches'' of the multi-faceted ``parent''. This
nomenclature also extends to other \nameref{categories-observ} from
\Cref{tab:ieeeTestTerms}, such as ``sub-type''.

\subsubsection{Rigidity}
\label{rigidity}

Since there is a considerable degree of nuance introduced by the use of natural
language, not all discrepancies are equal! To capture this nuance and provide a
more complete picture, we make a distinction between explicit and implicit
discrepancies, such as in \Cref{tab:discrepClss,tab:discrepCats}. A piece of
information is ``implicit'' if:
\begin{itemize}
    \item it is not directly given by a source but seems to be implied, and/or
    \item it is only true some of the time (e.g., under certain conditions).
\end{itemize}
Discrepancies based on implicit information are themselves implicit. These are
automatically detected when \hyperref[graph-gen]{generating graphs} and
\hyperref[discrep-analysis]{analyzing discrepancies} by looking for indicators
of uncertainty, such as question marks, ``~(Testing)'' (which indicates
that a test approach isn't explicitly denoted as such), and the keywords
``implied'', ``inferred'', ``can be'', ``ideally'', ``usually'', ``most'',
``likely'', ``often'', ``if'', and ``although''
\seeSrcCode{50380a3}{csvToGraph}{124}{140}. These words were used when creating
the glossaries to capture varying degrees of nuance, such as when a test
approach ``can be'' a child of another or is a synonym of another ``most of the
time'', but isn't always. As an example, \Cref{tab:parSyns} contains relations
that are explicit, implicit, and both; implicit relations are marked by the
phrase ``implied by''.

\subsection{Procedure}

To track terminology used in the literature, we build a glossary of test
approaches, including the term itself, its definition, and
any synonyms or parents. Any additional notes, such as questions or sources to investigate
further, are also recorded. Approach categorizations, such as those found in
\Cref{tab:ieeeTestTerms} and some outliers (e.g., ``artifact''), are tracked
for future investigation.

Most relevant sources are analyzed in their entirety to systematically extract
terminology, with the exception of some sources that were only partially
investigated. This is the case for sources chosen for a specific area of
interest or based on a test approach that was determined to be out-of-scope,
such as some sources given in \Cref{undef-terms}.
Heuristics are used to guide this process, by investigating:

\begin{itemize}
    \item glossaries and lists of terms,
    \item testing-related terms (e.g., terms containing ``test(ing)'',
          \ifnotpaper ``review(s)'', ``audit(s)'', \fi
          ``validation'', or ``verification''),
    \item terms that had emerged as part of already-discovered
          testing approaches, \emph{especially} those that were ambiguous
          or prompted further discussion (e.g., terms containing
          ``performance'', ``recovery'', ``component'', ``bottom-up'',
          \ifnotpaper ``boundary'', \fi or ``configuration''), and
    \item terms that implied testing approaches%
          \ifnotpaper\footnote{
                  Since these methods for deriving test approaches only arose
                  as research progressed, some examples would have been missed
                  during the first pass(es) of resources investigated earlier
                  in the process. While reiterating over them would be ideal,
                  this may not be possible due to time constraints.
              } (see \Cref{derived-tests})\fi.
\end{itemize}

When terms have multiple definitions, either the clearest and most concise
version is kept, or they are merged to paint a more complete picture.
If any discrepancies or ambiguities
arise, they are reasonably investigated and always documented. If a
testing approach is mentioned but not defined, it is added to the
glossary to indicate it should be investigated further (see
\Cref{undef-terms}). A similar methodology
is used for tracking software qualities, albeit in a separate
document\ifnotpaper\ (see \Cref{qual-test})\fi.

During the first pass of data collection, all software-testing-focused terms
are included. Some of them are less applicable to test case automation
\ifnotpaper(such as \Cref{static-test}\thesisissueref{39}) \fi or too
broad\ifnotpaper (such as \Cref{attacks}\thesisissueref{55})\fi, so they
will be omitted over the course of analysis.

\ifnotpaper
    During this investigation, some terms came up that seemed to be relevant to
    testing but were so vague, they didn't provide any new information. These were
    decided to be not worth tracking\thesisissueref{39,44,28} and are listed below:

    \begin{itemize}
        \item \textbf{Evaluation:} the ``systematic determination of the extent
              to which an entity meets its specified criteria''
              \citep[p.~167]{IEEE2017}
        \item \textbf{Product Analysis:} the ``process of evaluating a product by
              manual or automated means to determine if the product has certain
              characteristics'' \citep[p.~343]{IEEE2017}
        \item \textbf{Quality Audit:} ``a structured, independent process to
              determine if project activities comply with organizational and
              project policies, processes, and procedures'' \citep[p.~361]{IEEE2017}
              \todo{OG PMBOK}
        \item \textbf{Software Product Evaluation:} a ``technical operation that
              consists of producing an assessment of one or more characteristics
              of a software product according to a specified procedure''
              \citep[p.~424]{IEEE2017}
    \end{itemize}
\fi
\subsection{Undefined Terms}
\label{undef-terms}

The search process led to some testing approaches being
mentioned without definition;
\citep{IEEE2022} and \citep{Firesmith2015} in particular introduced many.
Once \stds{} had been exhausted, we devised a strategy to
look for sources that explicitly define these terms, consistent with
our snowballing approach. This uncovers new approaches, both in and out of
scope (such as \acf{emsec} testing, HTML testing, and aspects of loop testing and
orthogonal array testing\ifnotpaper; see \Cref{scope}\fi).

The following terms (and their respective related terms) were explored%
\ifnotpaper\ in the following sources\fi, bringing the number of testing
approaches from \the\TotalBefore{} to \the\TotalAfter{} and the number of
\emph{undefined} terms from \the\UndefBefore{} to \the\UndefAfter{} (the
assumption can be made that about \the\numexpr 100 - 100 * (\UndefAfter -
\UndefBefore) / (\TotalAfter - \TotalBefore)\relax\% of added terms also
included a definition):

\input{build/undefTerms}

\ifnotpaper\else
    \subsection{Tools}  % Part of separate chapter in thesis 
    \graphGenDesc{}
    % Moved here to display nicely in paper
    \discrepClssTable{}
    \discrepCatsTable{}
\fi
