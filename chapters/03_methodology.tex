\section{Methodology}
\label{methodology}

At a high level\todo{Is this sufficient as a chapter ``roadmap'', or should I
    include one explicitly? How would I do this without duplication?}, our
methodology follows the following steps:

\input{build/methodOverview}

\subsection{Sources}
\label{sources}
As there is no single authoritative source on software testing terminology,
we need to look at many to see how various terms are used in practice.
% Unfortunately, this brings to light a variety of discrepancies.
Starting from some set of sources, we then use
``snowball sampling'', a ``method of \dots{} sample selection \dots{} used to
locate hidden populations'' \citep{Johnson2014}, to gather further sources
(see \Cref{undef-terms}). Sources are then grouped into tiers based on their
format, method of publication, and ``trustworthiness'', where more trustworthy
sources:
\begin{enumerate}
    \item have gone through a peer-review process,
    \item are written by numerous, well-respected authors,
    \item are informed by many sources, and
    \item are accepted and used in the field of software.
\end{enumerate}

We therefore create the following tiers, given in order of descending
trustworthiness: established standards (\Cref{stds}), terminology collections
(\Cref{metas}), textbooks (\Cref{texts}), and papers and other documents
(\Cref{papers}). A summary of how many sources comprise each tier is given in
\Cref{fig:sourceSummary}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \pie[sum=auto, after number=, text=legend, thick,
            scale=\ifnotpaper0.7\else0.5\fi,
            every label/.style={align=left, scale=0.7}]
        {\stdSources{3}/\stds{},
            \metaSources{3}/\metas{},
            \textSources{3}/\texts{},
            \paperSources{3}/\papers{}}
    \end{tikzpicture}
    \caption{Summary of how many sources comprise each source tier.}
    \label{fig:sourceSummary}
\end{figure}

\ifnotpaper\newpage\fi

\subsubsection{\stdSources{1}}
\label{stds}
% Colored \textcolor{green}{green}

These are documents written for the field of software engineering by reputable
standards bodies, namely ISO, the \acf{iec}, and IEEE. We only consider those
about software development and testing for the purposes of this research (see
\Cref{scope}). For example, ``the purpose of the ISO/IEC/IEEE 29119 series is
to define an internationally agreed set of standards for software testing that
can be used by any organization when performing any form of software testing''
\ifnotpaper(\fi\citealp[p.~vii]{IEEE2022}\ifnotpaper; similar in
\citeyear[p.~ix]{IEEE2016})\fi. This tier is composed of \stdSources{2}.

\subsubsection{\metaSources{1}}
\label{metas}
% Colored \textcolor{blue}{blue}

These are collections of software testing terminology built up from multiple
sources, including established standards (see \Cref{stds}). These documents are
made to be widely applicable; the \acf{swebok} is ``proposed as a suitable
foundation for government licensing, for the regulation of software engineers,
and for the development of university curricula in software engineering''
\citep[p.~xix]{KanerEtAl2011}. They are often written by a large organization,
such as the \acf{istqb}, but not always: we include \citeauthor{Firesmith2015}'s
taxonomy \citeyearpar{Firesmith2015} and \citeauthor{DoğanEtAl2014}'s literature
review \citeyearpar{DoğanEtAl2014} because of their broad scope and
applicability\todo{I should probably expand on this}.
This tier is composed of \metaSources{2}.

\subsubsection{\textSources{1}}
\label{texts}
% Colored \textcolor{Maroon}{maroon}

We consider textbooks to be more trustworthy than papers (see \Cref{papers})
because they are widely used as resources for teaching software engineering and
may be used as guides in industry. Although textbooks have smaller sets of
authors, they follow a formal review process before publication. Textbooks that
are trusted at McMaster \citep{Patton2006, PetersAndPedrycz2000, vanVliet2000}
served as the original (albeit ad hoc and arbitrary) starting point of this
research; we investigate other books as they arise. For example, \citetISTQB{}
\multiAuthHelper{cite} \citep{GerrardAndThompson2002} as the original source
for \ifnotpaper their \else its \fi definition of ``scalability'' (see
\Cref{scal-test-rec}); we verified\todo{Present tense?} this by
looking at this original source. This tier is composed of \textSources{2}.

\subsubsection{\paperSources{1}}
\label{papers}
% Colored black

Documents in this source tier are written by much smaller sets of authors
% with unknown peer review processes % TODO: investigate
and are much less widespread than higher source tiers. We investigated%
\todo{Present tense?} many of
these sources to ``fill in'' missing definitions (see \Cref{undef-terms}).
While most documents are journal articles and conference papers, the following
document types are also present; some less-than-academic sources show how terms
are used in practice and are included in this source tier for brevity%
\thesisissueref{89}:

\begin{itemize}
    \item Report \citep{Kam2008,Gerrard2000a,Gerrard2000b}
    \item Thesis \citep{Bas2024}
          % \item A less-formal classification \citep{KuļešovsEtAl2013}
    \item Website \citep{LambdaTest2024,Pandey2023}
    \item Booklet \citep{SPICE2022}
    \item \ifnotpaper \else ChatGPT \fi \citet{ChatGPT2024} (with its claims
          supported by \citet{RusEtAl2008})
\end{itemize}

The full set of sources that comprise this tier is \paperSources{2}.

% Moved here to display nicely in paper
\ifnotpaper\else\ieeeTestTermsTable{}\fi

\subsection{Terminology}

This research was intended to describe the current state of testing
terminology instead of prematurely applying any classifications to reduce bias.
Therefore, the notions of test approach categories (\Cref{categories-observ}),
synonyms (\Cref{syn-rels}), and parent-child relations (\Cref{par-chd-rels})
arose naturally from the literature. Even though these are ``results'' of this
research, they are defined here for clarity since they are used throughout this
\ifnotpaper thesis\else paper\fi. We also define the notion of ``rigidity'' in
\Cref{rigidity}.

\subsubsection{Approach Categories}
\label{categories-observ}

Different sources categorize software testing approaches differently\ifnotpaper;
while it is useful to record and think about these categorizations, following
one (or more) during the research
stage could lead to bias and a prescriptive categorization instead of letting
one emerge descriptively during the analysis stage. Since these categorizations
are not mutually exclusive, it also means that more than one could be useful
(both in general and for this research).\par \citet{IEEE2022} \else. For
example, ISO/IEC and IEEE \cite{IEEE2022} \fi provide the classification of test
approaches in \Cref{tab:ieeeTestTerms}\ifnotpaper, while other sources
\citep{BarbosaEtAl2006, SouzaEtAl2017} use alternative categories (see
\Cref{tab:otherTestTerms}). These will provide other perspectives when
determining if the categorization in \Cref{tab:ieeeTestTerms} is sufficient.
Nevertheless, these \else. These \fi categories (``test level'' and ``test
type'' in particular) seem to be widely used. For example, in addition to the
IEEE sources (given in \Cref{tab:ieeeTestTerms}), six additional sources give
unit testing, integration testing, system testing, and acceptance testing as
examples of test levels \ifnotpaper
    (\citealp[pp.~5-6 to 5-7]{SWEBOK2024}; \citealpISTQB{};
    \citealp[p.~218]{KuļešovsEtAl2013}\todo{OG Black, 2009};
    \citealp[p.~807-808]{Perry2006}; \citealp[pp.~443-445]{PetersAndPedrycz2000};
    \citealp[pp.~9,~13]{Gerrard2000a})\else
    \cite{ISTQB}, \cite[pp.~5-6 to 5-7]{SWEBOK2024},
    \cite[pp.~9,~13]{Gerrard2000a}, \cite[p.~807-808]{Perry2006},
    \cite[pp.~443-445]{PetersAndPedrycz2000}, \citealp[p.~218]{KuļešovsEtAl2013}%
    \todo{OG Black, 2009}\fi, although they may use a different term for ``test
level'' (see \Cref{tab:ieeeTestTerms}). Because of their widespread use and
their usefulness when focusing on a particular subset of testing, these
categories are used for now. \ifnotpaper In addition to this categorization, a
    ``metric'' category was considered\thesisissueref{21,22} but decided to be
    out of scope for this project, instead being captured by coverage-driven
    testing (see \Cref{cov-test}) and experience-based testing
    \citep[p.~34]{IEEE2022}. \fi We did, however, note the potential
significance of an ``artifact'' category\thesisissueref{44,119,39}, since some
terms could refer to the application of a test approach and/or the resulting
document(s). Because of this, a test approach being categorized as a category
from \Cref{tab:ieeeTestTerms} \emph{and} an artifact is \emph{not} a
discrepancy\thesisissueref{119}\ifnotpaper\ (and is therefore omitted from
    \Cref{tab:multiCats})\fi.

One important side effect of the particularity of these terms is that they can
be ``overloaded''; for example, someone could reasonably yet imprecisely use
any of these four categories as a synonym for ``approach''. Even the prompt in
\ifnotpaper \citep[emphasis added]{ChatGPT2024} \else \cite{ChatGPT2024} \fi was
imprecise, asking for the ``\emph{type} of software testing that focuses on
looking for bugs where others have already been found.'' Interestingly, ChatGPT
later ``corrected'' this by calling detect-based testing an ``approach''!
Because of this, careful consideration needs to be given to discrepancies of
this nature. For example, \citet[p.~45\ifnotpaper, emphasis added\fi]{Kam2008}'s
definition of ``interface testing'' is ``an integration \emph{test type} that is
concerned with testing \dots{} interfaces'', but since he does not define
``test type'', it may not have special significance. \ifnotpaper For this
reason, these ``categorizations'' are marked with a question mark (?)
and included in \Cref{tab:infMultiCats} instead of in \Cref{tab:multiCats}.

Related testing approaches may be grouped into a ``class'' or ``family'' to
group those with ``commonalities and well-identified variabilities that can be
instantiated'', where ``the commonalities are large and the variabilities
smaller'' \citep{classFamilyDisc}. Examples of these are the classes of
combinatorial \citep[p.~15]{IEEE2021} and data flow testing \citetext{p.~3} and
the family of performance-related testing \perfAsFamily{}, and is implied for
security testing, a test type that consists of ``a number of
techniques\footnote{This may or may not be \distinctIEEE{technique}}''
\cite[p.~40]{IEEE2021}. This is explored in more detail in
\Cref{classFamilyDiscrep}.

It also seems that the categories given in \Cref{tab:ieeeTestTerms} are
orthogonal. For example, ``a test type can be performed at a single test level
or across several test levels''
\ifnotpaper
    (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021})%
\else
    \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021}%
\fi, and ``Keyword-Driven Testing [sic] can be applied at all testing levels
(e.g. [sic] component testing, system testing) and for various types of testing
(e.g. [sic] functional testing\footnote{See \Cref{corr-func-test}.}, reliability
testing)'' \citeyearpar[p.~4]{IEEE2016}. Due to this, a specific
test approach can be derived by combining test approaches from different
categories\ifnotpaper; see \Cref{orth-test} for some examples
of this\fi.

\begin{bigLandscape}
    \ieeeTestTermsTable{}
    \newpage
    \otherTestTermsTable{}
\end{bigLandscape}
% These were moved earlier to display nicely in paper

The literature provides many other ways to categorize test approaches. While
these are less-defined and as such are not used, they are given here for
completeness. Note that ``there is a lot of overlap between different classes
of testing'' \citep[p.~8]{Firesmith2015}, meaning that ``one category [of test
        techniques] might deal with combining two or more techniques''
\citep[p.~5-10]{SWEBOK2024}. For example, ``performance, load and stress
testing might considerably overlap in many areas'' \citep[p.~1187]{Moghadam2019}.
A side effect of this is that it is difficult to ``untangle'' these classes;
for example, take the following sentence: ``whitebox fuzzing extends dynamic
test generation based on symbolic execution and constraint solving from unit
testing to whole-application security testing''
\citep[p.~23]{GodefroidAndLuchaup2011}!

Despite these challenges, it is useful to understand the differences between
testing classes because tests from multiple subsets within the same category,
such as functional and structural, ``use different sources of information and
have been shown to highlight different problems'' \citep[p.~5-16]{SWEBOK2024}.
However, some subsets, such as deterministic and random, may have ``conditions
that make one approach more effective than the other''
\citep[p.~5-16]{SWEBOK2024}. The following categories may also be more relevant
in specific situations or to specific teams than the ones given by ISO/IEC and
IEEE in \Cref{tab:ieeeTestTerms}.

\begin{itemize}
    \item Visibility of code: black-, white-, or grey-box
          (specificational/functional, structural, or a mix of the two)
          (\citealp[p.~8]{IEEE2021}; \citealp[pp.~5-10,~5-16]{SWEBOK2024};
          \citealp[p.~601, called ``testing approaches'' and (stepwise) code
              reading replaced ``grey-box testing'']{SharmaEtAl2021};
          \todo{OG [3, 4, 5, 8]}
          \citealp[pp.~57-58]{AmmannAndOffutt2017};
          \citealp[p.~213]{KuļešovsEtAl2013};
          \citealp[pp.~53,~218]{Patton2006}; \citealp[p.~69]{Perry2006};
          \citealp[pp.~4-5, called ``testing methods'']{Kam2008})
    \item Source of information for design: specification, structure, or
          experience \citep[p.~8]{IEEE2021}
          \begin{itemize}
              \item Source of test data: specification-, implementation-,
                    or error-oriented \citep[p.~440]{PetersAndPedrycz2000}
          \end{itemize}
    \item Test case selection process: deterministic or random
          \citep[p.~5-16]{SWEBOK2024}
    \item Coverage criteria: input space partitioning, graph coverage, logic
          coverage, or syntax-based testing \citep[pp.~18-19]{AmmannAndOffutt2017}
    \item Question: what-, when-, where-, who-, why-, how-, and how-well-based
          testing; these are then divided into a total of ``16 categories of
          testing types''\notDefDistinctIEEE{type}
          \citep[p.~17]{Firesmith2015}
    \item Execution of code: static or dynamic
          (\citealp[p.~214]{KuļešovsEtAl2013}; \citealp[p.~12]{Gerrard2000a};
          \citealp[p.~53]{Patton2006}); we also consider this categorization
          meaningful (see \Cref{static-test})
    \item Goal of testing: verification or validation
          (\citealp[p.~214]{KuļešovsEtAl2013}; \citealp[pp.~69-70]{Perry2006})
    \item Property of code \citep[p.~213]{KuļešovsEtAl2013} or test target
          \citep[pp.~4-5]{Kam2008}: functional or non-functional
    \item Human involvement: manual or automated
          \citep[p.~214]{KuļešovsEtAl2013}
    \item Structuredness: scripted or exploratory
          \citep[p.~214]{KuļešovsEtAl2013}
    \item Coverage requirement: data or control flow \citep[pp.~4-5]{Kam2008}
    \item Test factor: (``attributes of the software that, if they are wanted,
          pose a risk to the success of the software''; also called ``quality
          factor'' or ``quality attribute'' \citep[p.~40]{Perry2006}):
          correctness, file integrity, authorization, audit trail, continuity
          of processing, service levels (e.g., response time), access control,
          compliance, reliability, ease of use, maintainability, portability,
          coupling (e.g., with other applications in a given environment),
          performance, and ease of operation (e.g., documentation, training)
          \citep[pp.~40-41]{Perry2006}
    \item Adequacy criterion: coverage-, fault-, or error-based
          (``based on knowledge of the typical errors that people make'')
          \citep[pp.~398-399]{vanVliet2000}
    \item Priority\footnote{In the context of testing e-business projects.}:
          smoke, usability, performance, or functionality testing
          \citep[p.~12]{Gerrard2000a}
    \item Category of test ``type''\gerrardDistinctIEEE{type}: static testing,
          test browsing, functional testing, non-functional testing, or large
          scale integration (testing) \citep[p.~12]{Gerrard2000a}
    \item Purpose: correctness, performance, reliability, or security
          \citep{Pan1999}
\end{itemize}

Additionally, Engström ``investigated classifications of research''
\citep[p.~1]{engström_mapping_2015} on the following four testing techniques.
``They are neither orthogonal nor necessarily useful for the purpose of
identifying relevant evaluation points from a problem perspective'' (p.~1), so
it is unclear why they were included at all:

\begin{itemize}
    \item \textbf{Combinatorial testing:} how the system under test is
          modelled, ``which combination strategies are used to generate test
          suites and how test cases are prioritized'' (pp.~1-2)
    \item \textbf{Model-based testing:} the information represented and
          described by the test model (p.~2)
    \item \textbf{Search-based testing:} ``how techniques%
          \notDefDistinctIEEE{technique} had been empirically evaluated
          (i.e. objective and context)'' (p.~2)
    \item \textbf{Unit testing:} ``source of information (e.g. code,
          specifications or testers intuition)'' (p.~2)
\end{itemize}
\fi

\subsubsection{Synonym Relations}
\label{syn-rels}

The same approach often has many names. For example,
\emph{specification-based testing} is also called\todo{more in Umar2000}:
\begin{enumerate}
    \item Black-Box Testing
          \ifnotpaper
              (\citealp[p.~9]{IEEE2022}; \citeyear[p.~8]{IEEE2021};
              \citeyear[p.~431]{IEEE2017}; \citealp[p.~5-10]{SWEBOK2024};
              \citealpISTQB{}; \citealp[p.~46 (without hyphen)]{Firesmith2015};
              \citealp[p.~344]{SakamotoEtAl2013}; \citealp[p.~399]{vanVliet2000})
          \else
              \cite[p.~9]{IEEE2022}, \cite{ISTQB}, \cite[p.~431]{IEEE2017},
              \cite[p.~5-10]{SWEBOK2024}, \cite[p.~8]{IEEE2021},
              % \cite[p.~46 (without hyphen)]{Firesmith2015},
              \cite[p.~399]{vanVliet2000},
              \cite[p.~344]{SakamotoEtAl2013}
          \fi
    \item Closed-Box Testing
          \ifnotpaper
              (\citealp[p.~9]{IEEE2022}; \citeyear[p.~431]{IEEE2017})
          \else
              \cite[p.~9]{IEEE2022}, \cite[p.~431]{IEEE2017}
          \fi
    \item Functional Testing\footnote{This may be an outlier; see
              \Cref{spec-func-test}.}
          \ifnotpaper
              (\citealp[p.~196]{IEEE2017}; \citealp[p.~44]{Kam2008};
              \citealp[p.~399]{vanVliet2000}; implied by
              \citealp[p.~129]{IEEE2021}; \citeyear[p.~431]{IEEE2017})
          \else
              \cite[p.~196]{IEEE2017}, \cite[p.~399]{vanVliet2000},
              \cite[p.~44]{Kam2008} (implied by \cite[p.~431]{IEEE2017},
              \cite[p.~129]{IEEE2021})
          \fi
    \item Domain Testing \citep[p.~5-10]{SWEBOK2024}
    \item Input Domain-Based Testing (implied by \citealp[pp.~4-7 to
              4-8]{SWEBOK2014})
\end{enumerate}

These synonyms are the same as synonyms in natural language; while they may
emphasize different aspects or express mild variations, their core meaning
is nevertheless the same. Throughout our work, we use the terms
``specification-based testing'' and ``structure-based testing'' to articulate
the source of the information for designing test cases, but a team or project
also using gray-box testing may prefer the terms ``black-box'' and ``white-box
testing'' for consistency. Thus, synonyms are not inherently problematic,
although they can be (see \Cref{syns}).

Synonym relations are often given explicitly in the literature. For example,
\citet[p.~9]{IEEE2022} \multiAuthHelper{list} ``black-box testing'' and
``closed box testing'' beneath the glossary entry for ``specification-based
testing'', meaning they are synonyms. ``Black-box testing'' is likewise given
under ``functional testing'' in \citeyearpar[p.~196]{IEEE2017}, meaning it is
also a synonym for ``specification-based testing'' through transitivity%
\todo{Is this clear/correct? Should I explain this more?}.
However, these relations can also be less ``rigid'' (see \Cref{rigidity});
``functional testing'' is listed in a \emph{cf.} footnote to the glossary entry
for ``specification-based testing'' \citeyearpar[p.~431]{IEEE2017}, which
supports the previous claim but would not necessarily indicate a synonym
relation on its own.

Similarly, \citet[p.~5-10]{SWEBOK2024} says ``\emph{specification-based
    techniques} \dots{} [are] sometimes also called domain
testing techniques'' in the \acs{swebok} V4, from which the synonym of
``domain testing'' follows logically. However, its predecessor V3 only
\emph{implies} the more specific ``input domain-based testing'' as a synonym.
The section on test techniques says ``the classification of testing techniques
presented here is based on how tests are generated: from the software
engineer's intuition and experience, the specifications, the code structure
\dots'' \citep[p.~4\=/7]{SWEBOK2014}, and the first three subsections on the
following page are ``Based on the Software Engineer's Intuition and
Experience'', ``Input Domain-Based Techniques'', and ``Code-Based Techniques''
\citetext{p.~4\=/8}. The order of the introductory list lines up with these
sections, implying that ``input domain-based techniques'' are ``generated[]
from \dots{} the specifications'' (i.e., that input domain-based testing is the
same as specification-based testing). Furthermore, the examples of input
domain-based techniques given---equivalence partitioning, pairwise testing,
boundary-value analysis, and random testing---are all given as children%
\footnote{
    Pairwise testing is given as a child of combinatorial testing, which is
    itself a child of specification-based testing, by \ifnotpaper
        \citep[Fig.~2]{IEEE2021} and \citep[pp.~5\=/11 to 5\=/12]{SWEBOK2024}%
    \else
        \cite[pp.~5\=/11 to 5\=/12]{SWEBOK2024} and \cite[Fig.~2]{IEEE2021}%
    \fi, making it a ``grandchild'' of specification-based testing according to
    these sources.
} of specification-based testing \ifnotpaper
    (\citealp{IEEE2022}; \citeyear[Fig.~2]{IEEE2021}; \citealpISTQB{})\else
    \cite{IEEE2022,ISTQB}, \cite[Fig.~2]{IEEE2021}\fi; even V4 agrees with
this \citep[pp.~5\=/11 to 5\=/12]{SWEBOK2024}!

\subsubsection{Parent-Child Relations}
\label{par-chd-rels}
Many test approaches are multi-faceted and can be ``specialized'' into others,
such as performance-related testing (see \Cref{perf-test-rec}). These
``specializations'' will be referred to as ``children'' or ``sub-approaches''
of the multi-faceted ``parent''. This nomenclature also extends to other
categories given in \Cref{categories-observ,tab:ieeeTestTerms}, such as
``sub-type''. One example of these specializations is the ``stronger than''
relation (also called the ``subsumes'' relation) described by
\citet[p.~432]{vanVliet2000}: when comparing adequacy criteria (which
``specif[y] requirements for testing'' \citetext{p.~402}), ``criterion X
is stronger than criterion Y if, for all programs P and all test sets T,
X-adequacy implies Y-adequacy''. While this relation only ``compares the
thoroughness of test techniques, not their ability to detect faults''
\citetext{p.~434}, it is sufficient to consider one a child of the other.

\subsubsection{Rigidity}
\label{rigidity}

Since there is a considerable degree of nuance introduced by the use of natural
language, not all discrepancies are equal! To capture this nuance and provide a
more complete picture, we make a distinction between explicit and implicit
discrepancies, such as in \Cref{tab:sntxDiscreps,tab:smntcDiscreps}. A piece of
information may be considered ``implicit'' for any of the following reasons
which are \emph{not} mutually exclusive:

%% Maybe convert to \paragraph ?
\begin{enumerate}
    \item \textbf{The information is implied.} The implicit categorizations
          of ``test type'' by \citet[pp.~53--58]{Firesmith2015} (see
          \Cref{tab:multiCats\ifnotpaper,tab:infMultiCats\fi}) are an example
          of this. The given test approaches are not explicitly called ``test
          types'', as the term is used more loosely to refer to different kinds
          of testing---what should be called ``test approaches'' as per
          \Cref{tab:ieeeTestTerms}. However, this set of test approaches are
          ``based on the associated quality characteristic and its associated
          quality attributes'' \citetext{p.~53}, implying that they are
          test types.
    \item \textbf{The information is not universal.}
          \refHelper \citet[p.~372\ifnotpaper, emphasis added\fi]{IEEE2017}%
          \todo{OG ISO/IEC, 2014} \multiAuthHelper{define} ``regression
          testing'' as ``testing required to determine that a change to a
          system component has not adversely affected \emph{functionality,
              reliability or performance} and has not introduced additional
          defects''. While reliability testing, for example, is not
          \emph{always} a subset of regression testing (since it may be
          performed in other ways), it \emph{can be} accomplished by regression
          testing, so there is sometimes a parent-child relation (defined in
          \Cref{par-chd-rels}) between them. \ifnotpaper
          \citet[p.~5-8\ifnotpaper, emphasis added\fi]{SWEBOK2024} provides a
          similar list: ``regression testing \dots{} \emph{may} involve
          functional and non-functional testing, such as reliability,
          accessibility, usability, maintainability, conversion, migration, and
          compatibility testing.'' \fi
    \item \textbf{The information is conditional.}
          As a more specific case of information not being universal, sometimes
          prerequisities must be satisfied for information to apply. For
          example, branch condition combination testing is equivalent
          to (and is therefore a synonym of) exhaustive testing \emph{if} ``each
          subcondition is viewed as a single input'' \citep[p.~464]{PetersAndPedrycz2000}.
          Likewise, statement testing can be used for (and is therefore a child
          of) unit testing \emph{if} there are ``less than 5000 lines of code''
          \citetext{p.~481\todo{OG Miller et al., 1994}}. \ifnotpaper
              \par This can also apply more abstractly at the taxonomy level,
              where a parent-child relation only makes sense if the parent test
              approach exists. This occurs when a source gives a relation
              between qualities but at least one of them does not have an
              explicit approach associated with it (although it may be derived;
              see \Cref{cov-test}). For example, \citet{ISO_IEC2023a} provides
              relations involving dependability and modifiability; these are
              tracked as qualities, not approaches, since only the qualities
              are described. Since the prerequisite of the relevant approach
              existing is \emph{not} satisfied, these relations are omitted
              from any generated graphs. \fi
    \item \textbf{The information is dubious.}
          This happens when there is reason to doubt the information provided.
          If a source claims one thing that is not true, related claims lose
          credibility. For example, the incorrect claim that ``white-box
          testing'', ``grey-box testing'', and ``black-box testing'' are
          synonyms for ``module testing'', ``integration testing'', and
          ``system testing'', respectively, \ifnotpaper (see
              \discrepref{dubious-syns}) \fi casts doubt on the claim that
          ``red-box testing'' is a synonym for ``acceptance testing''
          \citep[p.~18]{SneedAndGöschl2000}\todo{OG Hetzel88}\ifnotpaper\
              (see \discrepref{dubious-red-box-syn})\fi. Doubts such as this
          can also originate from other sources. \refHelper
          \citet[p.~48]{Kam2008} gives ``user scenario testing'' as a synonym
          of ``use case testing'', even though ``an actor [in use case testing]
          can be \dots{} another system'' \citep[p.~20]{IEEE2021}, which does
          not fit as well with the label ``user scenario testing''. However,
          since a system can be seen as a ``user'' of the test item, this
          synonym relation is treated as implicit instead of as an outright
          discrepancy.
\end{enumerate}

Discrepancies based on implicit information are themselves implicit. These are
automatically detected when generating graphs and analyzing discrepancies
(see \ifnotpaper \Cref{graph-gen,discrep-analysis}, respectively\else
    \Cref{tools}\fi) by looking for indicators
of uncertainty, such as question marks, ``~(Testing)'' (which indicates that a
test approach isn't explicitly denoted as such; note the inclusion of the
space), and the keywords ``implied'', ``inferred'', ``can be'', ``should be'',
``ideally'', ``usually'', ``most'', ``likely'', ``often'', ``if'', and ``although''
\seeSrcCode{55f4bf2}{helpers}{16}{32}. These words were used when creating
the glossaries to capture varying degrees of nuance, such as when a test
approach ``can be'' a child of another or is a synonym of another ``most of the
time'', but isn't always. As an example, \Cref{tab:parSyns} contains relations
that are explicit, implicit, and both; implicit relations are marked by the
phrase ``implied by''.

\subsection{Procedure}
\label{procedure}

To track terminology used in the literature, we build a glossary of test
approaches, including the term itself, its definition, and
any synonyms or parents (see \Cref{par-chd-rels}). Any other notes, such as
uncertainties, prerequisites, and other resources to investigate, are also
recorded. If an approach is assigned a category, such as those found in
\Cref{tab:ieeeTestTerms} and some outliers (e.g., ``artifact''%
\thesisissueref{39,44}), this is also tracked for future investigation.

Most sources are analyzed in their entirety to systematically extract
terminology, especially established standards (see \Cref{stds}). Sources that
were only partially investigated include those chosen for a specific area of
interest or based on a test approach that was determined to be out-of-scope,
such as some sources given in \Cref{undef-terms}.
Heuristics are used to guide this process, by investigating:

\begin{itemize}
    \item glossaries and lists of terms,
    \item testing-related terms (e.g., terms containing ``test(ing)'',
          \ifnotpaper ``review(s)'', ``audit(s)'', \fi
          ``validation'', or ``verification''),
    \item terms that had emerged as part of already-discovered
          testing approaches, \emph{especially} those that were ambiguous
          or prompted further discussion (e.g., terms containing
          ``performance'', ``recovery'', ``component'', ``bottom-up'',
          \ifnotpaper ``boundary'', \fi or ``configuration''), and
    \item terms that implied testing approaches%
          \ifnotpaper\footnote{
                  Since these methods for deriving test approaches only arose
                  as research progressed, some examples would have been missed
                  during the first pass(es) of resources investigated earlier
                  in the process. While reiterating over them would be ideal,
                  this may not be possible due to time constraints.
              } (see \Cref{derived-tests})\fi.
\end{itemize}

When terms are given similar definitions by multiple sources, the clearest and
most concise version is kept. If definitions from different sources overlap,
provide different information, or contradict, they are merged to paint a more
complete picture. When contradictions or other discrepancies (see
\Cref{discreps}) arise, they are investigated and documented. Any test
approaches that are mentioned but not defined are added to the glossary to
indicate they should be investigated further (see \Cref{undef-terms}).
Similar methodologies are used for tracking software qualities \ifnotpaper (see
    \Cref{qual-test}) \fi and supplementary terminology that is shared by
multiple approaches or is too complicated to explain inline; these are tracked
in separate documents. The name, definition, and synonym(s) of all terms are
tracked, as well as any precedence for a related test type for a given software
quality.

During the first pass of data collection, all software-testing-focused terms
are included. Some of them are less applicable to test case automation
\ifnotpaper (such as static testing; see \Cref{static-test}\thesisissueref{39})
\fi or too broad\ifnotpaper\ (such as attacks; see \Cref{attacks}%
    \thesisissueref{55})\fi, so they will be omitted during future analysis.
\ifnotpaper
    However, some terms came up that seemed to be relevant to
    testing but were so vague, they didn't provide any new information. These were
    decided to be not worth tracking\thesisissueref{39,44,28} and are listed below:

    \begin{itemize}
        \item \textbf{Evaluation:} the ``systematic determination of the extent
              to which an entity meets its specified criteria''
              \citep[p.~167]{IEEE2017}
        \item \textbf{Product Analysis:} the ``process of evaluating a product by
              manual or automated means to determine if the product has certain
              characteristics'' \citep[p.~343]{IEEE2017}
        \item \textbf{Quality Audit:} ``a structured, independent process to
              determine if project activities comply with organizational and
              project policies, processes, and procedures'' \citep[p.~361]{IEEE2017}
              \todo{OG PMBOK}
        \item \textbf{Software Product Evaluation:} a ``technical operation that
              consists of producing an assessment of one or more characteristics
              of a software product according to a specified procedure''
              \citep[p.~424]{IEEE2017}
    \end{itemize}

    \phantomsection{}\label{infers}
    Throughout this process, information can be inferred from ``surface-level''
    analysis that follows straightforwardly but isn't explicitly stated by any
    source. Examples of this are large scale integration testing and legacy
    system integration testing, described by \citeauthor{Gerrard2000a} in
    \citeyearpar[p.~30]{Gerrard2000b} and (\citeyear[Tab.~2]{Gerrard2000a};
    \citeyear[Tab.~1]{Gerrard2000b}), respectively. While he never explicitly
    says so, it can be inferred that these approaches are children of
    integration testing and system integration testing, respectively.
    Although these data do not come from the literature, they are documented
    for completeness; inferred discrepancies are given in \Cref{infer-discreps}
    and inferred relations, if any, are included in \recFigs{}.
\fi

\subsection{Undefined Terms}
\label{undef-terms}

The search process led to some testing approaches being
mentioned without definition;
\citep{IEEE2022} and \citep{Firesmith2015} in particular introduced many.
Once the standards in \Cref{stds} had been exhausted, we devised a strategy to
look for sources that explicitly define these terms, consistent with
our snowballing approach. This uncovers new approaches, both in and out of
scope (such as \acf{emsec} testing\ifnotpaper, HTML testing,\fi\ and aspects
of orthogonal array testing\ifnotpaper\ and loop testing\fi; see \Cref{scope}).

The following terms (and their respective related terms) were explored
in the following sources, bringing the number of testing
approaches from \the\TotalBefore{} to \the\TotalAfter{} and the number of
\emph{undefined} terms from \the\UndefBefore{} to \the\UndefAfter{} (the
assumption can be made that about \the\numexpr 100 - 100 * (\UndefAfter -
\UndefBefore) / (\TotalAfter - \TotalBefore)\relax\% of added terms also
included a definition):

\input{build/undefTerms}
