\section{Methodology}
\label{method}

This process initially involved looking through textbooks that were trusted at
McMaster \citep{Patton2006, PetersAndPedrycz2000, vanVliet2000}. However, this
process was somewhat ad hoc and arbitrary, meaning it wouldn't be as systematic
as required. Going forward, this process will be more rigorous, starting from
more established sources of software testing terminology in approximately the
following order:
(\citealp{IEEE2022, SWEBOK2024, SWEBOK2014, IEEE2017, IEEE2013, ISO_IEC2023b,
      IEEE2012, ISO_IEC2023a}; \citealpISTQB{}; \citealp{Firesmith2015, IEEE2021}).

Sources from standards organizations (i.e., \citeauthor{IEEE2022}) were decided
to be the most valuable, followed by ``meta-level'' commentaries or collections
of terminology, often based on these standards along with other sources. This
is reflected in how their information is displayed in any graphs: green lines
correspond with standards and blue lines with less-standard collections.
Other sources that focus on cataloging and categorizing testing terminology
\citep[e.g.,][]{KuļešovsEtAl2013}, were also examined to see how well they
agreed with the ``standard'' terminology, and these, along with sources
investigated to ``fill in'' missing definitions\seeSectionPar{undef-terms}{}
and any ``surface-level'' analysis that followed trivially, are represented by
black lines. Since this research began from McMaster-trusted sources, comparing
them to the rest of the literature may prove interesting; these relations are
coloured maroon.

I went through these resources by going through them looking for relevant
terminology, taking special care with glossaries and lists of terms. Of
particular note were terms that included ``test(ing)'', ``validation'',
``verification'', ``review'', ``audit'', or terms that had come up before
as part of already-discovered testing approaches, such as ``performance'',
``recovery'', ``component'', ``bottom-up'', ``boundary'', and ``configuration''.
A type of ``coverage'' was assumed to imply a type of testing that aimed to
maximize this coverage (e.g., ``path testing'' is testing that ``aims to
execute all entry-to-exit control flow paths in a SUT's control flow graph''
\citep[p.~5013]{SWEBOK2024}, thus maximizing the path coverage; see also
\thesisissueref{63}, \citet[Fig.~1]{SharmaEtAl2021}).
If a term's definition had already been recorded, either the ``new'' one
replaced it if the ``old'' one wasn't as clear/concise or parts of both were
merged to paint a more complete picture. If any discrepancies or ambiguities
arose, they were investigated to a reasonable extent and documented. If a
testing approach was mentioned but not defined, it was still added to the
glossary to indicate it should be investigated further. A similar methodology
was used for tracking software qualities, albeit in a separate
document\seeSectionPar{chap:testing:sec:derived-tests}{}.

During this investigation, some terms came up that seemed to be relevant to
testing but were so vague, they didn't provide any new information. These were
decided to be not worth tracking (see \thesisissueref{39}, \thesisissueref{44},
\thesisissueref{28}) and are listed below:

\begin{itemize}
      \item \textbf{Evaluation:} the ``systematic determination of the extent
            to which an entity meets its specified criteria''
            \citep[p.~167]{IEEE2017}
      \item \textbf{Product Analysis:} the ``process of evaluating a product by
            manual or automated means to determine if the product has certain
            characteristics'' \citep[p.~343]{IEEE2017}
      \item \textbf{Quality Audit:} ``a structured, independent process to
            determine if project activities comply with organizational and
            project policies, processes, and procedures'' \citep[p.~361]{IEEE2017}
            \todo{OG PMBOK}
      \item \textbf{Software Product Evaluation:} a ``technical operation that
            consists of producing an assessment of one or more characteristics
            of a software product according to a specified procedure''
            \citep[p.~424]{IEEE2017}
\end{itemize}

However, over the course of this research, our scope was adjusted to include
some terms for our initial list of test approaches to be filtered out later,
such as types of attacks (see \thesisissueref{55}), meaning that some entries
were missed during the first pass(es) of these resources. While reiterating
over these resources would be ideal, this may not be possible due to time
constraints.

\subsection{Undefined Terms}
\label{undef-terms}

% Define values to be easily reused and used in calculation!

\newcount\TotalBefore
\newcount\TotalAfter
\newcount\UndefBefore
\newcount\UndefAfter

\TotalBefore=431
\UndefBefore=152

\TotalAfter=514
\UndefAfter=170

This process also led to some testing approaches without definitions;
\citep{IEEE2022} and \citep{Firesmith2015} in particular introduced many.
Once more ``standard'' sources had been exhausted, a strategy was proposed to
look for sources that explicitly defined these terms, with the added benefit of
uncovering more terms to explore, potentially in different domains (see
\thesisissueref{57}). This also uncovered some out-of-scope testing approaches,
including EMSEC testing, HTML testing, and aspects of loop testing and
orthogonal array testing\seeSectionPar{chap:testing:sec:scope}{}; since these
are out of scope, relevant sources were not investigated fully.

The following terms (and their respective related terms)
were explored%
\ifnotpaper
      { in the following sources}%
\fi, bringing the number of testing
approaches from \the\TotalBefore~to \the\TotalAfter~and the number of
\emph{undefined} terms from \the\UndefBefore~to \the\UndefAfter~(the assumption
can be made that about \the\numexpr 100 - 100 * (\UndefAfter - \UndefBefore) /
(\TotalAfter - \TotalBefore)\relax\% of added terms also included a definition):

\input{build/undefTerms}

Different sources categorized software testing approaches in different ways;
while it is useful to record and think about these
categorizations\seeSectionPar{testing-categories}{}, following one (or more)
during the research
stage could lead to bias and a prescriptive categorization, instead of letting
one emerge descriptively during the analysis stage. Since these categorizations
are not mutually exclusive, it also means that more than one could be useful
(both in general and to this specific project); more careful thought should be
given to which are ``best'', and this should happen during the analysis stage.
