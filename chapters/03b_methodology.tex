\section{Methodology}
\label{method}

\subsection{Sources}
\label{sources}
As there is no single authoritative source on software testing terminology,
we need to look at many. Unfortunately, this brings to light a variety of
discrepancies. Starting from some set of sources, we then use
``snowballing'' % needs a reference!
to gather further sources\seeParAlways{undef-terms}. Groups of sources with
information that shows up in a relevant graph are given a unique colour to
better visualize them; see \Cref{fig:recovery-graph-current,%
      fig:recovery-graph-proposed,fig:scal-graph-current,%
      fig:scal-graph-proposed,fig:perf-graph}.

\begin{enumerate}
      \setcounter{enumi}{-1}
      \item Trusted textbooks
            \citep{Patton2006, PetersAndPedrycz2000, vanVliet2000}
            \begin{itemize}
                  \item Ad hoc and arbitrary; not systematic
                        % \item Colored \textcolor{Maroon}{maroon},
            \end{itemize}
      \item Established standards, such as IEEE, ISO/IEC, and the \acf{swebok}
            \begin{itemize}
                  \item Standards organizations \citep{IEEE2022, IEEE2017,
                              IEEE2013, ISO_IEC2023b, IEEE2012, ISO_IEC2023a,
                              IEEE2021, ISO_IEC2018, ISO2021, ISO2015}
                        colored \textcolor{green}{green}
                  \item ``Meta-level'' commentaries or collections of
                        terminology (often based on these standards)
                        \ifnotpaper
                              (\citealp{SWEBOK2024, SWEBOK2014};
                              \citealpISTQB{}; \citealp{Firesmith2015})
                        \else
                              \cite{SWEBOK2024, SWEBOK2014, ISTQB, Firesmith2015}
                        \fi colored \textcolor{blue}{blue},
            \end{itemize}
      \item Other resources: less-formal classifications of terminology
            \ifnotpaper
                  \citep[e.g.,][]{KuļešovsEtAl2013}%
            \else
                  (such as \citep{KuļešovsEtAl2013})%
            \fi%
            , sources investigated to
            ``fill in'' missing definitions\seeSectionParAlways{undef-terms},
            and testing-related resources that emerged for unrelated reasons
            \begin{itemize}
                  \item Colored black, along with any ``surface-level''
                        analysis that followed straightforwardly.
            \end{itemize}
\end{enumerate}

\subsection{Procedure}

To track terminology used in the literature, we build a glossary of test
approaches, including the term itself, its definition, and
any synonyms or parents. Many test approaches are multi-faceted and can be
``specialized'' into others, such as \nameref{perf-test-rec}. These
``specializations'' will be referred to as ``children'' or
``sub-approaches\footnote{This nomenclature extends to other categories of
      approaches from \Cref{tab:ieeeTestTerms}, such as ``sub-type''.}''
of the multi-faceted
``parent''. Any additional notes, such as questions or sources to investigate
further, are also recorded. Approach categorizations, such as those found in
\Cref{tab:ieeeTestTerms} and some outliers (e.g., ``artifact''), are tracked
for future investigation.

All sources are analyzed in their entirety to systematically extract
terminology. (Some sources given in \Cref{undef-terms}
were only partially investigated to focus on the area of interest or since
the test approach was determined to be out-of-scope.)
Heuristics are used to guide this process, by investigating:

\begin{itemize}
      \item glossaries and lists of terms,
      \item testing-related terms\\
            e.g., terms that included ``test(ing)'', ``validation'',
            ``verification'', ``review(s)'', or ``audit(s)'',
      \item terms that had emerged as part of already-discovered
            testing approaches, \emph{especially} those that were ambiguous
            or prompted further discussion\\
            e.g., terms that included ``performance'', ``recovery'',
            ``component'', ``bottom-up'', ``boundary'', or ``configuration'', and
      \item terms that implied testing approaches%
            \ifnotpaper\footnote{
                        Since these methods for deriving test approaches only arose
                        as research progressed, some examples would have been missed
                        during the first pass(es) of resources investigated earlier
                        in the process. While reiterating over them would be ideal,
                        this may not be possible due to time constraints.
                  }\fi%
            \seeSectionParAlways{derived-tests}.
\end{itemize}

When terms have multiple definitions, either the clearest and most concise
version is kept, or they are merged to paint a more complete picture.
If any discrepancies or ambiguities
arise, they are reasonably investigated and always documented. If a
testing approach is mentioned but not defined, it is added to the
glossary to indicate it should be investigated further%
\seeSectionParAlways{undef-terms}. A similar methodology
is used for tracking software qualities, albeit in a separate
document\seeSectionParAlways{qual-test}.

During the first pass of data collection, all software-testing-focused terms
are included. Some of them are less applicable to test case automation
\ifnotpaper{(such as \Cref{static-test}, \thesisissueref{39})}\fi or too
broad (such as \Cref{attacks}\ifnotpaper{, \thesisissueref{55}}\fi), so they
will be omitted over the course of analysis.

\ifnotpaper
      During this investigation, some terms came up that seemed to be relevant to
      testing but were so vague, they didn't provide any new information. These were
      decided to be not worth tracking\seeThesisIssuePar{39}[, \thesisissueref{44},
            \thesisissueref{28}] and are listed below:

      \begin{itemize}
            \item \textbf{Evaluation:} the ``systematic determination of the extent
                  to which an entity meets its specified criteria''
                  \citep[p.~167]{IEEE2017}
            \item \textbf{Product Analysis:} the ``process of evaluating a product by
                  manual or automated means to determine if the product has certain
                  characteristics'' \citep[p.~343]{IEEE2017}
            \item \textbf{Quality Audit:} ``a structured, independent process to
                  determine if project activities comply with organizational and
                  project policies, processes, and procedures'' \citep[p.~361]{IEEE2017}
                  \todo{OG PMBOK}
            \item \textbf{Software Product Evaluation:} a ``technical operation that
                  consists of producing an assessment of one or more characteristics
                  of a software product according to a specified procedure''
                  \citep[p.~424]{IEEE2017}
      \end{itemize}
\fi
\subsection{Undefined Terms}
\label{undef-terms}

% Define values to be easily reused and used in calculation!

\newcount\TotalBefore
\newcount\TotalAfter
\newcount\UndefBefore
\newcount\UndefAfter

\TotalBefore=432
\UndefBefore=153

\TotalAfter=515
\UndefAfter=171

The search process led to some testing approaches being
mentioned without definition;
\citep{IEEE2022} and \citep{Firesmith2015} in particular introduced many.
Once ``standard'' sources had been exhausted, we devised a strategy to
look for sources that explicitly defined these terms, consistent with
our snowballing approach. This uncovered new approaches, both in and out of
scope (such as \acf{emsec} testing, HTML testing, and aspects of loop testing and
orthogonal array testing\seeSectionPar{scope}).

The following terms (and their respective related terms)
were explored%
\ifnotpaper
      { in the following sources}%
\fi, bringing the number of testing
approaches from \the\TotalBefore~to \the\TotalAfter~and the number of
\emph{undefined} terms from \the\UndefBefore~to \the\UndefAfter~(the assumption
can be made that about \the\numexpr 100 - 100 * (\UndefAfter - \UndefBefore) /
(\TotalAfter - \TotalBefore)\relax\% of added terms also included a definition):

\input{build/undefTerms}

We then develop a tool to automatically generate graphs of the relations
between test approaches. All child-parent relations are graphed, as well as
synonym relations where either:
\begin{enumerate}
      \item both terms are present in the glossary, or
      \item one term is synonyms with more than one term that is present in the
            glossary\seeParAlways{multiSyns}.
\end{enumerate}
\Cref{fig:recovery-graph-current,fig:scal-graph-current} are modified versions
of these graphs generated based on the existing literature, focused on specific
subsets of testing terminology. This tool was also expanded to be able to make
changes to these generated graphs based on our \nameref{recs}.
\Cref{fig:recovery-graph-proposed,fig:scal-graph-proposed,fig:perf-graph} are
modified versions of these proposed graphs.
