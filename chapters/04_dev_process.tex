\chapter{Development Process}
\label{chap:dev-proc}

The following is a rough outline of the steps I have gone through this far for
this project:

\begin{itemize}
      \item start development of system tests (this was pushed too later to focus
            on unit tests)
      \item test inputting default values as \texttt{float}s and \texttt{int}s
      \item check constraints for valid input
      \item check constraints for invalid input
      \item test calculation of:
            \begin{itemize}
                  \item \texttt{t\_flight}
                  \item \texttt{p\_land}
                  \item \texttt{d\_offset}
                  \item \texttt{s}
            \end{itemize}
      \item test valid output of writing output
      \item test for projectile going long
      \item integrate system tests into existing unit tests
      \item test for assumption violation of g
            \begin{itemize}
                  \item code generation could be flawed, so we can't assume
                        assumptions are respected
                  \item test cases shouldn't necessarily match what is done by the
                        code; for example, g = 0 shouldn't really give a
                        \texttt{ZeroDivisionError}; it should be a \texttt{ValueError}
                  \item this inspired the potential for
                        \nameref{chap:dev-proc:code-assertions}
            \end{itemize}
      \item test that calculations stop on a constraint violation; this is a
            requirement should be met by the software
      \item test for empty input file
      \item start creation of test summary (for \texttt{InputParameters} module)
            \begin{itemize}
                  \item it was difficult to judge test case coverage/quality from
                        the code itself
                  \item this is not really a test plan, as it doesn't capture the
                        testing philosophy
                  \item rationale for each test explains why it supports coverage
                        and how Drasil derived (would derive) it
            \end{itemize}
      \item start researching testing
      \item start the \nameref{chap:dev-proc:gen-reqs} subproject
\end{itemize}

\section{Improvements to Manual Test Code}

Even though this code will eventually be generated by Drasil, it is important
that it is still human-readable, for the benefit of those reading the code
later. This is one of the goals of Drasil (see \issueref{3417} for an example
of a similar issue). As such, the following improvements were discovered and
implement in the manually created testing code:

\begin{itemize}
      \item use pytest's parameterization
      \item reuse functions/data for consistency
      \item improve import structure
      \item use \texttt{conftest} for running code before all tests of a module
\end{itemize}

\subsection{Testing with Mocks}

When testing code, it is common to first test lower-level modules, then assume
that these modules work when testing higher-level modules. An example would be
using an input module to set up test cases for a calculation module after
testing the input module. This makes sense when writing test cases manually
since it reduces the amount of code that needs to be written and still provides
a reasonably high assurance in the software; if there is an issue with the
input module that affects the calculation module tests, the issue would be
revealed when testing the input module.

However, since these test cases will be generated by Drasil, they can be
consistently generated with no additional effort. This means that the testing
of each module can be done completely independently, increasing the confidence
in the tests.

\section{The Use of Assertions in Code}
\label{chap:dev-proc:code-assertions}

While assertions are often only used when testing, they can also be used in
the code itself to enforce constraints or preconditions; they act like
documentation that determines behaviour! For example, they could be used to
ensure that assumptions about values (like the value for gravitational
acceleration) are respected by the code, which gives a higher degree of
confidence in the code.

\section{Generating Requirements}
\label{chap:dev-proc:gen-reqs}

I structured my manually created test cases around \acs{projectile}'s functional
requirements, as these are the most objective aspects of the generated code to
test automatically. As such, I created a test case to ensure that if an input
constraint was violated, the calculations would stop.
\todo{add reference requirement and test case} However, this test case failed,
since the actual implementation of the code did \emph{not} stop upon an input
constraint violation. This was because the code choice for what to do on a
constraint violation was "disconnected" from the manually written requirement
(\issueref{3523}) \todo{add relevant code}. This problem has been encountered
before (\issueref{3259}) and presented a good
opportunity for generation to encourage reusability and consistency. However,
since it makes sense to first verify outputs before actually outputting them
and inserting generated requirements among manually created ones seemed
challenging, it made sense to first generate an output requirement.

While working on Drasil in the summer of 2019, I implemented the generation
of an input requirement across most examples (\pullref{1844}).
I had also attempted to generate an output requirement, but due to time
constraints, this was not feasible. The
main issue with this change was the desire to capture the source of each output
for traceability; this source was attached to the \texttt{InstanceModel}
(or rarely, \texttt{DataDefinition}) and not the underlying \texttt{Quantity}
that was used for a program's outputs. The way I had attempted to do this was
to add the reference as a \texttt{Sentence} in a tuple.

Taking another look at this four years later allowed us to
see that we should be storing the outputs of a program as their underlying
models, allowing us to keep the source information with it
\todo{cite Dr.~Smith}. While there is some discussion about how this might
change in the future \todo{add refs to `underlying Theory' comment and `not
      all outputs be IMs' comment}, for now, all outputs of a program should be
\texttt{InstanceModel}s. Since this change required adding the
\texttt{Referable} \todo{add constraints} constraints to the output field of
\texttt{SystemInformation}, the outputs of all examples needed to be updated to
satisfy this constraint; this meant that generating the output requirement of
each example was nearly trivial once the outputs were specified correctly.
After modifying \texttt{DataDefinition}s in GlassBR that were outputs to be
\texttt{InstanceModel}s (\issueref{3569}; \pullref{3583}), reorganizing the
requirements of SWHS (\issueref{3589}; \pullref{3607}), and clarifying the
outputs of SWHS (\issueref{3589}), \acs{sglpend} (\issueref{3533}),
\acs{dblpend} (\issueref{3533}), \acs{gamephysics} (\issueref{3609}), and
\acs{ssp} (\issueref{3630}), the output requirement was ready to be generated.
