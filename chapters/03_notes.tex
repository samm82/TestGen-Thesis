\chapter{Notes}
\label{chap:notes}

\section{A Survey of Metaprogramming Languages}
\label{chap:notes:sec:metalang-survey}

\begin{itemize}
      \item Often done with \acfp{ast}, \todo{investigate more: Steele 1990?}
            although other bases are used:
            \begin{itemize}
                  \item \acfp{sst}, used by Dylan
                        \cite[p.~113:6]{lilis_survey_2019}
            \end{itemize}
      \item Allows for improvements in:
            \begin{itemize}
                  \item ``performance by generating efficient specialized
                        programs based on specifications instead of using
                        generic but inefficient programs''
                        \cite[p.~113:2]{lilis_survey_2019}
                  \item reasoning about object programs through ``analyzing
                        and discovering object-program characteristics that
                        enable applying further optimizations as well as
                        inspecting and validating the behavior of the object
                        program'' \cite[p.~113:2]{lilis_survey_2019}
                  \item code reuse through capturing ``code patterns that cannot
                        be abstracted'' \cite[p.~113:2]{lilis_survey_2019}
            \end{itemize}
\end{itemize}

\subsection{Definitions}

``\emph{Metaprogramming} is the process of writing computer programs, called
\emph{metaprograms}, that [can] \dots generate new programs or modify existing
ones'' \cite[p.~113:1]{lilis_survey_2019}. ``It constitutes a flexible and
powerful reuse solution for the ever-growing size and complexity of software
systems'' \cite[p.~113:31]{lilis_survey_2019}.

\begin{itemize}
      \item Metalanguage: ``the language in which the metaprogram is written''
            \cite[p.~113:1]{lilis_survey_2019}
      \item Object language: ``the language in which the generated or
            transformed program is written'' \cite[p.~113:1]{lilis_survey_2019}
      \item Homogeneous metaprogramming: when ``the object language and the
            metalanguage are the same'' \cite[p.~113:1]{lilis_survey_2019}
      \item Heterogeneous metaprogramming: when ``the object language and the
            metalanguage are \dots different'' \cite[p.~113:1]{lilis_survey_2019}
\end{itemize}

\subsection{Metaprogramming Models}
\subsubsection{Macro Systems \cite[p.~113:3-7]{lilis_survey_2019}}
\begin{itemize}
      \item Map specified input sequences in a source file to corresponding
            output sequences (``macro expansion'') until no input sequences
            remain \cite[p.~113:3]{lilis_survey_2019}; this process can be:
            \begin{enumerate}
                  \item procedural (involving algorithms; this is more common
                        \cite[p.~113:31]{lilis_survey_2019}), or
                  \item pattern-based (only using pattern matching)
                        \cite[p.~113:4]{lilis_survey_2019}
            \end{enumerate}
      \item Must avoid variable capture (unintended name conflicts) by being
            ``hygienic'' \cite[p.~113:4]{lilis_survey_2019}; this may be
            overridden to allow for ``intentional variable capture'', such as
            Scheme's \emph{syntax-case} macro \cite[p.~113:5]{lilis_survey_2019}
\end{itemize}

\paragraph{Lexical Macros}
\begin{itemize}
      \item Language agnostic \cite[p.~113:3]{lilis_survey_2019}
      \item Usually only sufficient for basic metaprogramming since changes to
            the code without considering its meaning ``may cause unintended side
            effects or name clashes and may introduce difficult-to-solve bugs''
            \cite[p.~113:5]{lilis_survey_2019}
      \item Marco was the first safe, language-independent macro system that
            ``enforce[s] specific rules that can be checked by special oracles''
            for given languages (as long as the languages ``produce descriptive
            error messages'') \cite[p.~113:6]{lilis_survey_2019}
\end{itemize}

\paragraph{Syntactic Macros}
\begin{itemize}
      \item ``Aware of the language syntax and semantics''
            \cite[p.~113:3]{lilis_survey_2019}
      \item MS\textsuperscript{2} ``was the first programmable syntactic macro
            system for syntactically rich languages'', including by using ``a
            type system to ensure that all generated code fragments are
            syntactically correct'' \cite[p.~113:5]{lilis_survey_2019}
\end{itemize}

\subsubsection{Reflection Systems \cite[p.~113:7-9]{lilis_survey_2019}}
\begin{itemize}
      \item ``Perform computations on [themselves] in the same way as for the
            target application, enabling one to adjust the system behavior
            based on the needs of its execution''
            \cite[p.~113:7]{lilis_survey_2019}
      \item Means that the system can ``observe and possibly modify its
            structure and behaviour'' \cite[p.~22]{stuikys_taxonomy_2013};
            these processes are called ``introspection'' and ``intercession'',
            respectively \cite[p.~113:7]{lilis_survey_2019}
            \begin{itemize}
                  \item The representation of a system can either be structural
                        or behavioural (e.g., variable assignment)
                        \cite[p.~113:7]{lilis_survey_2019}
            \end{itemize}
      \item ``Runtime code generation based on source text can be impractical,
            inefficient, and unsafe, so alternatives have been explored based
            on \acsp{ast} and quasi-quote operators, offering a structured approach
            that is subject to typing for expressing and combining code at
            runtime'' \cite[p.~113:8]{lilis_survey_2019}
      \item ``Not limited to runtime systems'', as some ``compile-time systems
            \dots rely on some form of structural introspection to perform code
            generation'' \cite[p.~113:9]{lilis_survey_2019}
\end{itemize}

\subsubsection{\acfp{mop} \cite[p.~113:9-11]{lilis_survey_2019}}
\begin{itemize}
      \item ``Interfaces to the language enabling one to incrementally transform
            the original language behavior and implementation''
            \cite[p.~113:9]{lilis_survey_2019}
      \item Three different approaches:
            \begin{itemize}
                  \item Metaclass-based Approach: ``Classes are considered to be
                        objects of metaclasses, called metaobjects, that are
                        responsible for the overall behavior of the object
                        system'' \cite[p.~113:9]{lilis_survey_2019}
                  \item Metaobject-based Approach: ``Classes and metaobjects are
                        distinct'' \cite[p.~113:9]{lilis_survey_2019}
                  \item Message Reification Approach: used with message passing
                        \cite[p.~113:9]{lilis_survey_2019}
            \end{itemize}
      \item Can either be runtime (more common) or compile-time (e.g., OpenC++);
            the latter protocols ``operate as advanced macro systems that perform
            code transformation based on metaobjects rather than on text or
            \acsp{ast}'' \cite[p.~113:11]{lilis_survey_2019}
\end{itemize}

\paragraph{Dynamic Shells}
``Pseudo-objects with methods and instance variables that may be attached to
other objects'' that ``offer efficient and type-safe \acs{mop} functionality for
statically typed languages'' \cite[p.~113:10]{lilis_survey_2019}.

\paragraph{Dynamic Extensions}
``Offer similar functionality [to dynamic shells] but for classes, allowing a
program to replace the methods of a class and its subclasses by the methods of
another class at runtime'' \cite[p.~113:10]{lilis_survey_2019}.

\subsubsection{\acf{aop} \cite[p.~113:11-13]{lilis_survey_2019}}
\begin{itemize}
      \item The use of \emph{aspects}: ``modular units \dots [that] contain
            information about the additional behavior, called \emph{advice},
            that will be added to the base program by the aspect as well as
            the program locations, called \emph{join points}, where this extra
            behavior is to be inserted based on some matching criteria, called
            \emph{pointcuts}'' \cite[p.~113:12]{lilis_survey_2019}
      \item Weaving: the process of ``combining the base program with aspect
            code \dots [to form] the final code'' \cite[p.~113:12]{lilis_survey_2019}
      \item Two variants:
            \begin{enumerate}
                  \item Static \acs{aop}: when weaving takes place at compile
                        time, usually with ``a separate language and a custom
                        compiler, called [an] \emph{aspect weaver}''; results in
                        better performance \cite[p.~113:12]{lilis_survey_2019}
                  \item Dynamic \acs{aop}: when weaving takes place at runtime
                        by instrumenting ``the bytecode \dots to be able to
                        weave the aspect code''; provides more flexibility
                        \cite[p.~113:12]{lilis_survey_2019}
            \end{enumerate}
      \item This model originates from reflecting and \acsp{mop} (AspectS and
            AspectL ``support AOP by building respectively on the runtime
            \acsp{mop} of Smalltalk and Lisp'') \cite[p.~113:12]{lilis_survey_2019}
      \item While ``\acs{aop} can support metaprogramming by inserting code
            before, after, or around matched join points, as well as introducing
            data members and methods through intertype declarations'', it is
            usually done the other way around, as most \acs{aop} frameworks
            ``rely on metaprogramming techniques'' \cite[p.~113:12]{lilis_survey_2019}
\end{itemize}

\subsubsection{Generative Programming \cite[p.~113:13-17]{lilis_survey_2019}}
\begin{itemize}
      \item ``A software development paradigm based on modeling software system
            families such that, given a particular requirements specification,
            a highly customized and optimized intermediate or end-product can
            be automatically manufactured on demand from elementary, reusable
            implementation components by means of configuration knowledge''
            \todo{get original source from Czarnecki and Eisenecker 2000}
      \item Often done with using \acsp{ast} \cite[p.~113:31]{lilis_survey_2019}
      \item Most ``support code templates and quasi-quotes''
            \cite[p.~113:31]{lilis_survey_2019}
      \item Related to macro systems, but normal code and metacode are distinct
\end{itemize}

\paragraph{Template Systems \cite[p.~113:13-14]{lilis_survey_2019}}
\begin{itemize}
      \item Template code is instantiated with specific parameters to generate
            ALL code in a target language; ``no free-form source code generation
            is allowed'' \cite[p.~113:13]{lilis_survey_2019}
            \todo{clarify what ``free-form source code generation'' means}
      \item It is possible, though complex, to express any ``to express any
            generative metaprogram'', as long as ``the appropriate
            metaprogramming logic for type manipulation'' is present
            \cite[p.~113:14]{lilis_survey_2019}
\end{itemize}

\paragraph{\acs{ast} Transformations \cite[p.~113:14-15]{lilis_survey_2019}}
\begin{itemize}
      \item ``Offer code templates through quasi-quotation to support \acs{ast}
            creation and composition and complement them with \acs{ast}
            traversal or transformation features'' \cite[p.~113:14]{lilis_survey_2019}
\end{itemize}

\paragraph{Compile-Time Reflections \cite[p.~113:15-16]{lilis_survey_2019}}
\begin{itemize}
      \item ``Offer compile-time reflection features to enable generating code
            based on existing code structures'' while trying to ensure that ``the
            generator will always produce well-formed code'' (this is not always
            fully possible; for example, Genoupe ``cannot guarantee that the
            generated code is always well typed'')
            \cite[p.~113:15]{lilis_survey_2019}
\end{itemize}

\paragraph{Class Compositions \cite[p.~113:16-17]{lilis_survey_2019}}
\begin{itemize}
      \item Offer ``flexibility and expressiveness'' through composition
            approaches \cite[p.~113:16]{lilis_survey_2019}
            \begin{itemize}
                  \item \emph{Mixins}: \todo{Investigate}
                  \item \emph{Traits}: ``support a uniform, expressive, and
                        type-safe way for metaprogramming without resorting to
                        \acsp{ast}'' and offer ``compile-time pattern-based
                        reflection'' through parameterization
                        \cite[p.~113:16]{lilis_survey_2019}
            \end{itemize}
      \item Includes \emph{feature-oriented programming} approaches
            \todo{Investigate?}
\end{itemize}

\subsubsection{\acf{msp} \cite[p.~113:17-20]{lilis_survey_2019}}
\begin{itemize}
      \item ``Makes \dots [levels of evaluation] accessible to the programmer
            through \dots \emph{staging annotations}'' to ``specify the
            evaluation order of the program computations'' and work with these
            computation stages \cite[p.~113:17]{lilis_survey_2019}
      \item Related to program generation and procedural macro systems
            \cite[p.~113:17]{lilis_survey_2019}; macros are often implemented
            as multistage computations \cite[p.~113:18]{lilis_survey_2019}
      \item Languages that use \acs{msp} are called \emph{\acfp{msl}} or
            \emph{two-stage languages}, depending on how many stages of
            evaluation are offered \cite[p.~113:17]{lilis_survey_2019};
            \acsp{msl} are more common \cite[p.~113:31]{lilis_survey_2019}
            \begin{itemize}
                  \item C++ first instantiates templates, then translates
                        nontemplate code \cite[p.~113:19]{lilis_survey_2019}
                  \item Template Haskell evaluates ``the top-level splices
                        to generate object-level code'' at compile time, then
                        executes the object-level code at runtime
                        \cite[p.~113:19]{lilis_survey_2019}
            \end{itemize}
      \item Often involves \emph{\acf{csp}}, which allows ``values \dots
            available in the current stage'' to be used in future stages
            \cite[p.~113:17]{lilis_survey_2019}
            \begin{itemize}
                  \item If this is used, \emph{cross-stage safety} is often
                        also used to prevent ``variables bound at some stage
                        \dots [from being] used at an earlier stage''
                        \cite[p.~113:17]{lilis_survey_2019}
            \end{itemize}
      \item Usually homogeneous, but there are exceptions; MetaHaskell, a
            modular framework \cite[p.~113:19]{lilis_survey_2019} with a type
            system, allows for ``heterogeneous metaprogramming with multiple
            object languages'' \cite[p.~113:18]{lilis_survey_2019}
      \item ``Type safety \dots comes at the cost of expressiveness''
            \cite[p.~113:19]{lilis_survey_2019}
\end{itemize}

\subsection{Phase of Evaluation}
\begin{itemize}
      \item ``In theory, any combination of them [the phases of evaluation] is
            viable; however, in practice most metalanguages offer only one or
            two of the options'' \cite[p.~113:20]{lilis_survey_2019}
      \item ``The phase of evaluation does not necessarily dictate the adoption
            of a particular metaprogramming model; however, there is a
            correlation between the two'' \cite[p.~113:20]{lilis_survey_2019}
\end{itemize}

\subsubsection{Preprocessing-Time Evaluation \cite[p.~113:20-21]{lilis_survey_2019}}
\begin{itemize}
      \item In \acf{pptmp}, ``metaprograms present in the original source are
            evaluated during the preprocessing phase and the resulting source
            file contains only normal program code and no metacode''
            \cite[p.~113:20]{lilis_survey_2019}
      \item These systems are called \emph{source-to-source preprocessors}
            \cite[p.~113:20]{lilis_survey_2019} and are usually examples of
            generative programming \cite[p.~113:21]{lilis_survey_2019}
            \begin{itemize}
                  \item ``All such cases involve syntactic transformations''
                        \cite[p.~113:21]{lilis_survey_2019}, usually using
                        \acsp{ast}
            \end{itemize}
      \item ``Translation can reuse the language compiler or interpreter
            without the need for any extensions'' \cite[p.~113:20]{lilis_survey_2019}
      \item Varying levels of complexity (e.g., these systems ``may be fully
            aware of the language syntax and semantics'')
            \cite[p.~113:20]{lilis_survey_2019}
      \item Includes all lexical macro systems \cite[p.~113:20]{lilis_survey_2019}
            and some ``static \acs{aop} and generative programming systems"
            \cite[p.~113:31]{lilis_survey_2019}
      \item Typically doesn't use reflection (Reflective Java is an exception),
            \acsp{mop}, or dynamic \acs{aop} \cite[p.~113:21]{lilis_survey_2019}
\end{itemize}

\subsubsection{Compilation-Time Evaluation \cite[p.~113:21-23]{lilis_survey_2019}}
\begin{itemize}
      \item In \acf{ctmp}, ``the language compiler is extended to handle
            metacode translation and execution'' \cite[p.~113:22]{lilis_survey_2019}
            \begin{itemize}
                  \item There are many ways of extending the compiler,
                        including ``plugins, syntactic additions, procedural or
                        rewrite-based \acs{ast} transformations, or multistage
                        translation'' \cite[p.~113:22]{lilis_survey_2019}
                  \item Metacode execution can be done by ``interpreting the
                        source metacode \dots or compiling the source metacode
                        to binary and then executing it''
                        \cite[p.~113:22]{lilis_survey_2019}
            \end{itemize}
      \item These systems are usually examples of generative programming but
            can also use macros, \acsp{mop}, \acs{aop}
            \cite[p.~113:22]{lilis_survey_2019}, and/or reflection
            \cite[p.~113:23]{lilis_survey_2019}
\end{itemize}

\subsubsection{Execution-Time Evaluation \cite[p.~113:23-25]{lilis_survey_2019}}
\begin{itemize}
      \item \acf{rtmp} ``involves extending the language execution system and
            offering runtime libraries to enable dynamic code generation and
            execution'' and is ``the only case where it is possible to extend the
            system based on runtime state and execution''
            \cite[p.~113:23]{lilis_survey_2019}
      \item Includes ``most reflection systems, \acsp{mop}, \acs{msp} systems,
            and dynamic \acs{aop} systems" \cite[p.~113:31]{lilis_survey_2019}
\end{itemize}

\subsection{Metaprogram Source Location}
\subsubsection{Embedded in the Subject Program \cite[p.~113:25-26]{lilis_survey_2019}}
\begin{itemize}
      \item Usually occurs with macros, templates, \acsp{msl}, reflection,
            \acsp{mop}, and \acs{aop} \cite[p.~113:25]{lilis_survey_2019}
\end{itemize}

\paragraph{Context Unaware \cite[p.~113:25]{lilis_survey_2019}}
\begin{itemize}
      \item Occurs when metaprograms only need to know their input parameters
            to generate \acsp{ast} \cite[p.~113:25]{lilis_survey_2019}
      \item Very common: supported by ``most \acs{ctmp} systems''
            \cite[p.~113:31]{lilis_survey_2019} and ``for most macro
            systems\dots, generative programming systems \dots and \acsp{msl}
            \dots it is the only available option'' \cite[p.~113:25]{lilis_survey_2019}
\end{itemize}

\paragraph{Context Aware \cite[p.~113:25-26]{lilis_survey_2019}}
\begin{itemize}
      \item ``Typically involves providing access to the respective program
            \acs{ast} node and allowing it to be traversed'' as ``an extra \dots
            parameter to the metaprogram'' \cite[p.~113:25]{lilis_survey_2019}
      \item Allows for code transformation ``at multiple different locations
            reachable from the initial context'' \cite[p.~113:25]{lilis_survey_2019}
      \item Very uncommon \cite[p.~113:25,~31]{lilis_survey_2019}
\end{itemize}

\paragraph{Global \cite[p.~113:26]{lilis_survey_2019}}
\begin{itemize}
      \item Involves ``scenarios that collectively introduce, transform, or
            remove functionality for the entire program''
            \cite[p.~113:26]{lilis_survey_2019}
      \item Usually occurs with reflection, \acsp{mop}, and \acs{aop}
            \cite[p.~113:26]{lilis_survey_2019}; offered by ``most \acs{rtmp}
            systems'' \cite[p.~113:31]{lilis_survey_2019}
      \item Can be used with ``any \acs{pptmp} or \acs{ctmp} system that
            provides access to the full program \acs{ast}''
            \cite[p.~113:26]{lilis_survey_2019}
      \item ``Can also be seen as a context-aware case where the context is the
            entire program'' \cite[p.~113:26]{lilis_survey_2019}
\end{itemize}

\subsubsection{External to the Subject Program \cite[p.~113:27]{lilis_survey_2019}}
\begin{itemize}
      \item Occurs when metaprograms ``are specified as separate transformation
            programs applied through \acs{pptmp} systems or supplied to the
            compiler together with the target program to be translated as extra
            parameters'' \cite[p.~113:27]{lilis_survey_2019}
      \item Includes many instances of \acs{aop} \cite[p.~113:27]{lilis_survey_2019}
\end{itemize}

\subsection{Relation to the Object Language}
\begin{itemize}
      \item Each metaprogramming language has two layers:
            \begin{enumerate}
                  \item ``The basic object language''
                  \item ``The metaprogramming elements for implementing the
                        metaprograms'' (the \emph{metalayer})
                        \cite[p.~113:27]{lilis_survey_2019}
            \end{enumerate}
      \item Sometimes the metalayer of a language is added to a language later,
            independently of the object language \cite[p.~113:27]{lilis_survey_2019}
\end{itemize}

\subsubsection{Metalanguage Indistinguishable from the Object Language
      \cite[p.~113:28-29]{lilis_survey_2019}}
\begin{itemize}
      \item Two categories:
            \begin{enumerate}
                  \item ``Object language and metalanguage \dots use the same
                        constructs through the same syntax''
                  \item ``Metalanguage constructs \dots [are] modeled using
                        object language syntax and applied through special
                        language or execution system features''
                        \cite[p.~113:28]{lilis_survey_2019}
                        \begin{itemize}
                              \item Includes many examples of \acsp{mop} and
                                    \acs{aop} \cite[p.~113:28]{lilis_survey_2019}
                        \end{itemize}
            \end{enumerate}
\end{itemize}

\subsubsection{Metalanguage Extends the Object Language
      \cite[p.~113:29]{lilis_survey_2019}}
\begin{itemize}
      \item Allows for reuse of ``the original language['s] \dots well-known
            features instead of adopting custom programming constructs''
            \cite[p.~113:29]{lilis_survey_2019}
      \item ``Typically involve new syntax and functionality used to
            differentiate normal code from metacode''
            \cite[p.~113:29]{lilis_survey_2019}
      \item Often used in quasi-quote constructs, two-stage and multistage
            languages, and \acsp{mop} \cite[p.~113:29]{lilis_survey_2019}
      \item Used with \acsp{msl} ``as the base languages are extended with
            staging annotations to deliver \acs{msp} functionality''
            \cite[p.~113:31]{lilis_survey_2019}
\end{itemize}

\subsubsection{Metalanguage Different from the Object Language
      \cite[p.~113:29-31]{lilis_survey_2019}}
\begin{itemize}
      \item Allows for ``the metalanguage syntax and constructs \dots [to be]
            selected to better reflect the metalanguage concepts to ease their
            use in developing metaprograms and enable them to become more
            concise and understandable'' \cite[p.~113:29]{lilis_survey_2019}
      \item However, it can lead to ``different development practices and
            disable[s] the potential for design or code reuse between them [the
                        languages]'', as well as requiring users to know how to
            use both languages \cite[p.~113:30]{lilis_survey_2019}
      \item Used by some \acs{aop} and generative metaprogramming systems
            \cite[p.~113:30]{lilis_survey_2019}
\end{itemize}

\section{Overview of Generative Software Development}
\label{chap:notes:sec:overview-of-gen-soft-dev}

``System family engineering seeks to exploit the commonalities
among systems from a given problem domain while managing the
variabilities among them in a systematic way''
\cite[p.~326]{czarnecki_overview_2004}. ``Generative software development is a
system-family approach \dots that focuses on automating the creation of
system-family members \dots from a specification written in [a \acf{dsl}]''
\cite[p.~327]{czarnecki_overview_2004}. ``\acsp{dsl} come in a wide variety of
forms, \dots [including] textual \dots [and] diagrammatic''
\cite[p.~328]{czarnecki_overview_2004}.

``System family engineering distinguishes between at least two kinds of
development processes: \emph{domain engineering} and \emph{application
      engineering}'' \cite[p.~328]{czarnecki_overview_2004}. ``Domain
engineering \dots is concerned with the development of reusable assets such as
components, generators, \acsp{dsl}, analysis and design models, user documentation,
etc.'' \cite[p.~328-329]{czarnecki_overview_2004}. It includes ``determining
the scope of the family to be built, identifying the common and variable
features among the family members'', and ``the development of a common
architecture for all the members of the system family''
\cite[p.~329]{czarnecki_overview_2004}. Application engineering includes
``requirements elicitation, analysis, and specification'' and ``the manual or
automated construction of the system from the reusable assets''
\cite[p.~329]{czarnecki_overview_2004}. The assets from domain engineering are
used to build the system development by application engineering, which provides
domain engineering which the requirements to analyze for commonalities and
create reusable assets for \cite[p.~329]{czarnecki_overview_2004}.

\acf{aop} ``provides more powerful localization and encapsulation mechanisms
than traditional component technologies'' but there is still the need to
``configure aspects and other components to implement abstract features''
\cite[p.~338]{czarnecki_overview_2004}. \acs{aop} ``cover[s] the solution space
and only a part of the configuration knowledge'', although ``aspects can also
be found in the problem space'' \cite[p.~338]{czarnecki_overview_2004}.

\subsection{Definitions}
\begin{itemize}
      \item Generative domain model: ``a mapping between \emph{problem space}
            and \emph{solution space}'' which ``takes a specification and
            returns the corresponding implementation''
            \cite[p.~330]{czarnecki_overview_2004}
            \begin{itemize}
                  \item Configuration view: ``the problem space consists of
                        domain-specific concepts and their features'' such as
                        ``illegal feature combinations, default settings, and
                        default dependencies'' \cite[p.~331]{czarnecki_overview_2004}.
                        ``An application programmer creates a configuration of
                        features by selecting the desired ones, [sic] which
                        then is mapped to a configuration of components''
                        \cite[p.~331]{czarnecki_overview_2004}
                  \item Transformational view: ``a problem space is represented
                        by a \dots [\acs{dsl}], whereas the solution space is
                        represented by an implementation language''
                        \cite[p.~331]{czarnecki_overview_2004}. ``A program in
                        a \dots [\acs{dsl}]'' is transformed into ``its
                        implementation in the implementation language''
                        \cite[p.~331]{czarnecki_overview_2004}
            \end{itemize}
      \item Problem space: ``a set of domain-specific abstractions that can be
            used to specify the desired system-family member''
            \cite[p.~330]{czarnecki_overview_2004}
      \item Solution space: ``consists of implementation-oriented abstractions,
            which can be instantiated to create implementations of the [desired]
            specifications'' \cite[p.~330]{czarnecki_overview_2004}
      \item Network of domains: the graph built from ``spaces and mappings \dots
            where each implementation of a domain exposes a \acs{dsl}, which may
            be implemented by transformations to \acsp{dsl} exposed by other
            domain implementations'' \cite[p.~332-333]{czarnecki_overview_2004}
      \item Feature modeling: ``a method and notation to elicit and represent
            common and variable features of the systems in a system family''
            \cite[p.~333]{czarnecki_overview_2004}. Can be used during domain
            analysis as ``the starting point in the development of both
            system-family architecture and \acsp{dsl}''
            \cite[p.~334]{czarnecki_overview_2004}
      \item \acf{mdd}: uses ``abstract representation[s] of a system and the
            portion[s] of the world that interact[] with it'' to ``captur[e]
            every important aspect of a software system''
            \cite[p.~336]{czarnecki_overview_2004}. Often uses \acsp{dsl} and
            sometimes deals with system families, making it related to
            generative software development \cite[p.~336-337]{czarnecki_overview_2004}
\end{itemize}

\section{Structured Program Generation Techniques}
\label{chap:notes:sec:prog-gen-techs}
\begin{itemize}
      \item Program transformer: something that ``modifies an existing program,
            instead of generating a new one'' (for example, by making a
            program's code adhere to style guides); the term ``program
            generator'' often includes program transformers
            \cite[p.~1]{smaragdakis_structured_2017}
      \item Generators are used ``to automate, elevate, modularize or otherwise
            facilitate program development'' \cite[p.~2]{smaragdakis_structured_2017}
      \item Why is it beneficial ``to statically check the generator and be
            sure that no type error arises during its \emph{run time}''
            \cite[p.~2]{smaragdakis_structured_2017} instead of just checking
            the generated program(s)?
            \begin{itemize}
                  \item ``An error in the generated program can be very hard to
                        debug and may require full understanding of the
                        generator itself'' \cite[p.~2]{smaragdakis_structured_2017}
                  \item Errors can occur in the generator from ``mismatched
                        assumptions''; for example, ``the generator fails to
                        take into account some input case, so that, even though
                        the generator writer has tested the generator under
                        several inputs, other inputs result in badly-formed
                        programs'' \cite[p.~6]{smaragdakis_structured_2017}
            \end{itemize}

\end{itemize}

\subsection{Techniques for Program Generation
      \cite[p.~3-5]{smaragdakis_structured_2017}}
\begin{enumerate}
      \item Generation as text: ``producing character strings containing the
            text of a program, which is subsequently interpreted or compiled''
            \cite[p.~3]{smaragdakis_structured_2017}
      \item Syntax tree manipulation: building up code using constructors in a
            syntactically meaningful way that preserves its structure
      \item Code templates/quoting: involves ``language constructs for
            generating program fragments in the target language \dots as well
            as for supplying values to fill in holes in the generated syntax
            tree'' \cite[p.~4]{smaragdakis_structured_2017}
      \item Macros: ``reusable code templates with pre-set rules for
            parameterizing them'' \cite[p.~4]{smaragdakis_structured_2017}
      \item Generics: Mechanisms with ``the ability to parameterize a code
            template with different static types''
            \cite[p.~5]{smaragdakis_structured_2017}
      \item Specialized languages: Languages with specific features for
            program generators, such as \acs{aop} and
            \emph{inter-type declarations} \cite[p.~5]{smaragdakis_structured_2017}
\end{enumerate}

\subsection{Kinds of Generator Safety \cite[p.~5-8]{smaragdakis_structured_2017}}
\begin{itemize}
      \item Lexical and syntactic well-formedness: ``any generated/transformed
            program is guaranteed to pass the lexical analysis and parsing
            phases of a traditional compiler''; usually done ``by encoding the
            syntax of the object language using the type system of the host
            language'' \cite[p.~6]{smaragdakis_structured_2017}
      \item Scoping and hygiene: avoiding issues with scope and unintentional
            variable capture
      \item Full well-formedness: ensuring that any generated/transformed
            program is guaranteed to be fully well-formed (e.g.,
            ``guaranteed to pass any static check in the target language''
            \cite[p.~8]{smaragdakis_structured_2017})
\end{itemize}

\subsection{Methods for Guaranteeing Fully Structured Generation
      \cite[p.~8-20]{smaragdakis_structured_2017}}
\begin{enumerate}
      \item \acf{msp}: ``the generator and the generated program \dots are
            type-checked by the same type system[] and some parts of the
            program are merely evaluated later (i.e., generated)''; similar
            to \emph{partial evaluation} \cite[p.~9]{smaragdakis_structured_2017}
      \item Class Morphing: similar to \acfp{mop}?
      \item Reflection: (e.g., SafeGen \cite[p.~15]{smaragdakis_structured_2017})
      \item The use of ``a powerful type system that can simultaneously express
            conventional type-level properties of a program and the logical
            structure of a generator under unknown inputs. This typically
            entails the use of dependent types'' (e.g., Ur)
            \cite[p.~16]{smaragdakis_structured_2017}
      \item Macro systems, although ``safety guarantees carry the cost of some
            manual verification effort by the programmer''
            \cite[p.~19]{smaragdakis_structured_2017}
\end{enumerate}

\section{Taxonomy of Fundamental Concepts of Meta-Programming}
\label{chap:notes:sec:meta-programming-taxonomy}
\subsection{Definitions}
\begin{itemize}
      \item Program transformation: ``the process of changing one form of a
            program (source code, specification or model) into another, as well
            as a formal or abstract description of an algorithm that implements
            this transformation'' \cite[p.~18]{stuikys_taxonomy_2013}
            \begin{itemize}
                  \item It may or may not preserve the program's semantics
                        \cite[p.~18]{stuikys_taxonomy_2013}
                  \item In metaprogramming, ``the transformation algorithm
                        describes generation of a particular instance depending
                        upon values of the generic parameters''
                        \cite[p.~18]{stuikys_taxonomy_2013}
                  \item Formal program transformation: ``A stepwise
                        manipulation, which (1) is defined on a programming
                        language domain, (2) uses a formal model to support the
                        refinement, and (3) simultaneously preserves the
                        semantics'' \cite[p.~18]{stuikys_taxonomy_2013}
            \end{itemize}
      \item Code generation: ``the process by which a code generator converts a
            syntactically correct high-level program into a series of
            lower-level instructions''; the input can take many forms
            ``typically consists of a parse tree, abstract syntax tree or
            intermediate language code'' and ``the output \dots could be in any
            language'' \cite[p.~19]{stuikys_taxonomy_2013}
      \item Generic component: ``a software module \dots [that] abstractly and
            concisely represents a set of closely related ('look-alike')
            software components with slightly different properties''
            \cite[p.~19]{stuikys_taxonomy_2013}
      \item Generative component: a generic component that has ``explicitly
            added generative technology'' \cite[p.~24]{stuikys_taxonomy_2013}
      \item Separation of concerns: ``the process of breaking a design problem
            into distinct tasks that are orthogonal and can be implemented
            separately'' \cite[p.~21]{stuikys_taxonomy_2013}
\end{itemize}

\subsection{Other Notes}
\begin{itemize}
      \item Structural meta-programming concepts ``are defined by the
            designer'', ``used during construction of the meta-programming
            systems and artefacts \todo{should I change this if it's British
                  English?}'', and ``depend upon [the] specific \dots
            meta-language'' used \cite[p.~24]{stuikys_taxonomy_2013}
      \item Most processes ``are used in compile time or run time'' except for
            generalization, which ``is used during the creation of the
            meta-programming artefacts'' \cite[p.~24-25]{stuikys_taxonomy_2013}
\end{itemize}

\section{Roadblocks to Meta-Programming}
\label{chap:notes:sec:metaprogramming-roadblocks}
\begin{itemize}
      \item ``Generators are often the technique of last resort''
            \cite[p.~2]{smaragdakis_structured_2017}
      \item ``A major stumbling block to achieving the promised benefits
                  [of meta-programming] is the understanding and learning the
            meta-programming approach. One reason may be that we do not yet
            thoroughly understand the fundamental concepts that define
            meta-programming'' \cite[p.~26]{stuikys_taxonomy_2013}
      \item Meta-programming does not provide instant results; instead, the
            effort and design put in at the beginning of the process later pay
            off potentially large dividends that are not seen right away;
            ``most \dots programmers and designers \dots like to reuse the
            existing software artefacts, but not much is done and [sic]
            invested into designing for reuse'' \cite[p.~26]{stuikys_taxonomy_2013}
            (example, meta-programming was proposed by McIlroy in 1968 but
            ``software factories have not become a reality \dots partly due to
            \dots [this] significant initial investment'')
            \cite[p.~27]{stuikys_taxonomy_2013}
      \item Software development involves ``work[ing] with multiple levels of
            abstraction'', including ``the syntax, semantics, abilities and
            limitations'' of given languages, their implementation details,
            their communication details, and ``impeding mismatches'' between
            them \cite[p.~27]{stuikys_taxonomy_2013}
      \item ``Modification of the generated code usually removes the program
            from the scope of the meta-programming system''
            \cite[p.~27]{stuikys_taxonomy_2013}
\end{itemize}

\section{Roadblocks to Testing Scientific Software
  \cite[p.~67]{kanewala_metamorphic_2019}}
\label{chap:notes:sec:sci-testing-roadblocks}
\begin{itemize}
      \item ``Correct answers are often unknown'': if the results were already
            known, there would be no need to develop software to model them
            \cite[p.~67]{kanewala_metamorphic_2019}
      \item ``Practically difficult to validate the computed output'': complex
            calculations and outputs are difficult to verify
            \cite[p.~67]{kanewala_metamorphic_2019}
      \item ``Inherent uncertainties'': since scientific software models
            scenarios that occur in a chaotic and imperfect world, not every
            factor can be accounted for \cite[p.~67]{kanewala_metamorphic_2019}
      \item ``Choosing suitable tolerances'': difficult to decide what
            tolerance(s) to use when dealing with floating-point numbers
            \cite[p.~67]{kanewala_metamorphic_2019}
      \item ``Incompatible testing tools'': while scientific software is often
            written in languages like FORTRAN, testing tools are often written
            int languages like Java or C++ \cite[p.~67]{kanewala_metamorphic_2019}
\end{itemize}

Out of this list, only the first two apply. The scenarios modelled by Drasil
are idealized and ignore uncertainties like air resistance, wind direction,
and gravitational fluctuations. There are not any instances where special
consideration for floating-point arithmetic must be taken; the default
tolerance used for relevant testing frameworks has been used
\todo{Add example} and is likely sufficient for future testing. On a related
note, the scientific software we are trying to test is already generated in
languages with widely-used testing frameworks. \todo{Add source(s)?}

\section{Software Metrics}
\label{chap:notes:sec:software-metrics}

\begin{itemize}
      \item The following branches of testing started as parts of quality
            testing:
            \begin{itemize}
                  \item Reliability testing \cite[p.~18, ch.~10]{fenton_software_1997}
                  \item Performance testing \cite[p.~18, ch.~7]{fenton_software_1997}
            \end{itemize}
      \item Reliability and maintainability can start to be tested even without
            code by ``measur[ing] structural attributes of representations of the
            software'' \cite[p.~18]{fenton_software_1997}
      \item The US Software Engineering Institute has a checklist for determining
            which types of lines of code are included when counting
            \cite[pp.~30-31]{fenton_software_1997}
      \item Measurements should include an entity to be measured, a specific
            attribute to measure, and the actual measure (i.e., units, starting
            state, ending state, what to include) \cite[p.~36]{fenton_software_1997}
            \begin{itemize}
                  \item These attributes must be defined before they can be
                        measured \cite[p.~38]{fenton_software_1997}
            \end{itemize}
\end{itemize}

\section{Software Testing}
\label{chap:notes:sec:software-testing}

While \cite{patton_software_2006} groups testing into static/dynamic and
white-/black-box, \cite[p.~398-399]{van_vliet_software_2000} gives a
classification ``according to the criterion used to measure the adequacy of a
set of test cases'': coverage-based, fault-based (includes mutation testing),
and error-based (``based on knowledge of the typical errors that people make'').

\subsection{Definitions}

\begin{itemize}
      \item Error: ``a human action that produces an incorrect result''
            \cite[p.~399]{van_vliet_software_2000}
      \item Fault: ``the manifestation of an error'' in the software itself
            \cite[p.~400]{van_vliet_software_2000}
      \item Failure: incorrect output or behaviour resulting from encountering
            a fault; can be defined as not meeting specifications or
            expectations and ``is a relative notion''
            \cite[p.~400]{van_vliet_software_2000}
      \item Verification: ``the process of evaluating a system or component
            to determine whether the products of a given development phase
            satisfy the conditions imposed at the start of that phase''
            \cite[p.~400]{van_vliet_software_2000}
      \item Validation: ``the process of evaluating a system or component
            during or at the end of the development process to determine
            whether it satisfies specified requirements''
            \cite[p.~400]{van_vliet_software_2000}
\end{itemize}

\subsubsection{Documentation}

\begin{itemize}
      \item \acf{vnv} Plan: a document for the ``planning of test activities''
            described by IEEE Standard 1012 \cite[p.~411]{van_vliet_software_2000}
      \item Test Plan: ``a document describing the scope, approach, resources,
            and schedule of intended test activities'' in more detail that the
            \acs{vnv} Plan \cite[p.~412-413]{van_vliet_software_2000}
      \item Test Design documentation: ``specifies \dots the details of the
            test approach and identifies the associated tests''
            \cite[p.~413]{van_vliet_software_2000}
      \item Test Case documentation: ``specifies inputs, predicted outputs and
            execution conditions for each test item''
            \cite[p.~413]{van_vliet_software_2000}
      \item Test Procedure documentation: ``specifies the sequence of actions
            for the execution of each test'' \cite[p.~413]{van_vliet_software_2000}
      \item Test Report documentation: ``provides information on the results of
            testing tasks'', addressing software verification and validation
            reporting \cite[p.~413]{van_vliet_software_2000}
\end{itemize}

\subsection{General Testing Notes}

\begin{itemize}
      \item ``Proving the correctness of software \dots applies only in
            circumstances where software requirements are stated formally'' and
            assumes ``these formal requirements are themselves correct''
            \cite[p.~398]{van_vliet_software_2000}
      \item If faults exist in programs, they ``must be considered faulty, even
            if we cannot devise test cases that reveal the faults''
            \cite[p.~401]{van_vliet_software_2000}
      \item Black-box test cases should be created based on the specification
            \emph{before} creating white-box test cases to avoid being ``biased
            into creating test cases based on how the module works''
            \cite[p.~113]{patton_software_2006}
      \item Simple, normal test cases (test-to-pass) should always be developed
            and run before more complicated, unusual test cases (test-to-fail)
            \cite[p.~66]{patton_software_2006}
\end{itemize}

\subsubsection{Generating Test Cases}

\begin{itemize}
      \item ``A \textbf{test adequacy criterion} \dots specifies requirements
            for testing \dots and can be used \dots as a test case generator....
                  [For example, i]f a 100\% statement coverage has not been achieved
            yet, an additional test case is selected that covers one or more
            statements yet untested'' \cite[p.~402]{van_vliet_software_2000}
      \item \emph{``Test data generators'' are mentioned on
                  \cite[p.~410]{van_vliet_software_2000} but not described}
            \todo{Investigate}
\end{itemize}

\subsection{Static Black-Box (Specification) Testing
      \cite[p.~56-62]{patton_software_2006}}

Most of this section is irrelevant to generating test cases, as they require
human involvement \todo{Describe anyway} (e.g., Pretend to Be the Customer
\cite[p.~57-58]{patton_software_2006}, Research Existing Standards and
Guidelines \cite[p.~58-59]{patton_software_2006}). However, it provides a
``Specification Terminology Checklist'' \cite[p.~61]{patton_software_2006} that
includes some keywords that, if found, could trigger an applicable warning to
the user (similar to the idea behind the correctness/consistency checks
project):

\begin{itemize}
      \item \textbf{Potentially unrealistic:} always, every, all, none, every,
            certainly, therefore, clearly, obviously, evidently
      \item \textbf{Potentially vague:} some, sometimes, often, usually,
            ordinarily, customarily, most, mostly, good, high-quality, fast,
            quickly, cheap, inexpensive, efficient, small, stable
      \item \textbf{Potentially incomplete:} etc., and so forth, and so on,
            such as, handled, processed, rejected, skipped, eliminated,
            if \dots then \dots (without ``else'' or ``otherwise''),
            to be determined \cite[p.~408]{van_vliet_software_2000}
\end{itemize}

\subsection{Dynamic Black-Box (Behavioural) Testing
      \cite[p.~64-65]{patton_software_2006}}

This is the process of ``entering inputs, receiving outputs, and checking the
results'' \cite[p.~64]{patton_software_2006}. Note that while black-box testing
is usually done at a higher (e.g., system) level, unit testing can also be
black-box \cite[p.~1]{jacob_comparative_2016}. \cite{van_vliet_software_2000}
also calls this ``functional testing''.

\paragraph{Requirements}
\begin{itemize}
      \item Requirements documentation (definition of what the software does)
            \cite[p.~64]{patton_software_2006}; relevant information could be:
            \begin{itemize}
                  \item Requirements: Input-Values and Output-Values
                  \item Input/output data constraints
            \end{itemize}
\end{itemize}

\subsubsection{Exploratory Testing \cite[p.~65]{patton_software_2006}}

An alternative to dynamic black-box testing when a specification is not
available \cite[p.~65]{patton_software_2006}. The software is explored to
determine its features, and these features are then tested
\cite[p.~65]{patton_software_2006}. Finding any bugs using this method is a
positive thing \cite[p.~65]{patton_software_2006}, since despite not knowing
what the software \emph{should} do, you were able to determine that something
is wrong.

This is not applicable to Drasil, because not only does it already generate a
specification, making this type of testing unnecessary, there is also a lot of
human-based trial and error required for this kind of testing
\cite{june_11_meeting}.

\subsubsection{Equivalence Partitioning/Classing \cite[p.~67-69]{patton_software_2006}}

The process of dividing the infinite set of test cases into a finite set that is
just as effective (i.e., that reveals the same bugs) \cite[p.~67]{patton_software_2006}.

\paragraph{Requirements}
\begin{itemize}
      \item Ranges of possible values \cite[p.~67]{patton_software_2006};
            could be obtained through:
            \begin{itemize}
                  \item Input/output data constraints
                  \item Case statements
            \end{itemize}
\end{itemize}

\subsubsection{Data Testing \cite[p.~70-79]{patton_software_2006}}

The process of ``checking that information the user inputs [and] results'',
both final and intermediate, ``are handled correctly'' \cite[p.~70]{patton_software_2006}.

\paragraph{Boundary Conditions \cite[p.~70-74]{patton_software_2006}}

``[S]ituations at the edge of the planned operational limits of the software''
\cite[p.~72]{patton_software_2006}. Often affects types of data (e.g., numeric,
speed, character, location, position, size, quantity
\cite[p.~72]{patton_software_2006}) each with its own set of (e.g., first/last,
min/max, start/finish, over/under, empty/full, shortest/longest,
slowest/fastest, soonest/latest, largest/smallest, highest/lowest,
next-to/farthest-from \cite[p.~72-73]{patton_software_2006}). Data at these
boundaries should be included in an equivalence partition, but so should
data in between them \cite[p.~73]{patton_software_2006}. Boundary conditions
should be tested using ``the valid data just inside the boundary,
\dots the last possible valid data, and \dots the invalid data just outside the
boundary'' \cite[p.~73]{patton_software_2006}.

\subparagraph{Requirements}
\begin{itemize}
      \item Ranges of possible values \cite[p.~67, 73]{patton_software_2006};
            could be obtained through:
            \begin{itemize}
                  \item Case statements
                  \item Input/output data constraints (e.g., inputs that
                        would lead to a boundary output)
            \end{itemize}
\end{itemize}

\subparagraph{Buffer Overruns \cite[p.~201-205]{patton_software_2006}}

\emph{Buffer overruns} are ``the number one cause of software security issues''
\cite[p.~75]{patton_software_2006}. They occur when the size of the destination
for some data is smaller than the data itself, causing existing data (including
code) to be overwritten and malicious code to potentially be injected
\cite[p.~202, 204-205]{patton_software_2006}. They often arise from bad
programming practices in ``languages [sic] such as C and C++, that lack safe
string handling functions'' \cite[p.~201]{patton_software_2006}. Any unsafe
versions of these functions that are used should be replaced with the
corresponding safe versions \cite[p.~203-204]{patton_software_2006}.

\paragraph{Sub-Boundary Conditions \cite[p.~75-77]{patton_software_2006}}
\label{sub-bound-conds}

Boundary conditions ``that are internal to the software [but] aren't necessarily
apparent to an end user'' \cite[p.~75]{patton_software_2006}. These include
powers of two \cite[p.~75-76]{patton_software_2006} and ASCII and Unicode tables
\cite[p.~76-77]{patton_software_2006}.

While this is of interest to the domain of scientific computing, this is too
involved for Drasil right now, and the existing software constraints limit much
of the potential errors from over/underflow \cite{june_11_meeting}. Additionally,
strings are not really used as inputs to Drasil and only occur in output with
predefined values, so testing these values are unlikely to be fruitful.

\subparagraph{Requirements}
\begin{itemize}
      \item Increased knowledge of data type structures (e.g., monoids, rings,
            etc. \cite{june_11_meeting}); this would capture these sub-boundaries,
            as well as other information like relevant tests cases, along with
            our notion of these data types (\texttt{Space})
\end{itemize}

\paragraph{Default, Empty, Blank, Null, Zero, and None
      \cite[p.~77-78]{patton_software_2006}}

These should be their own equivalence class, since ``the software usually
handles them differently'' than ``the valid cases or \dots invalid cases''
\cite[p.~78]{patton_software_2006}.

Since these values may not always be applicable to a given scenario (e.g., a
test case for zero doesn't make sense if there is a constraint that the value
in question cannot be zero), the user should likely be able to select
categories of tests to generate instead of Drasil just generating all possible
test cases based on the inputs \cite{june_11_meeting}.

\subparagraph{Requirements}
\begin{itemize}
      \item Knowledge of an ``empty'' value for each \texttt{Space} (stored
            alongside each type in \texttt{Space}?)
      \item Knowledge of how input data could be omitted from an input
            (e.g., a missing command line argument, an empty line in a file);
            could be obtained from:
            \begin{itemize}
                  \item User responsibilities
            \end{itemize}
      \item Knowledge of how a programming language deals with \texttt{Null}
            values and how these can be passed as arguments
\end{itemize}

\paragraph{Invalid, Wrong, Incorrect, and Garbage Data
      \cite[p.~78-79]{patton_software_2006}}

This is testing-to-fail \cite[p.~77]{patton_software_2006}.

\subparagraph{Requirements}
This seems to be the most open-ended category of testing.
\begin{itemize}
      \item Specification of correct inputs that can be ignored;
            could be obtained through:
            \begin{itemize}
                  \item Input/output data constraints (e.g., inputs that would
                        lead to a violated output constraint)
                  \item Type information for each input (e.g., passing a string
                        instead of a number)
            \end{itemize}
\end{itemize}

\subsubsection{State Testing \cite[p.~79-87]{patton_software_2006}}

The process of testing ``a program's states and the transitions between them''
\cite[p.~79]{patton_software_2006}.

\paragraph{Logic Flow Testing \cite[p.~80-84]{patton_software_2006}}

This is done by creating a state transition diagram that includes:

\begin{itemize}
      \item Every possible unique state
      \item The condition(s) that take(s) the program between states
      \item The condition(s) and output(s) when a state is entered or exited
\end{itemize}

to map out the logic flow from the user's perspective
\cite[p.~81-82]{patton_software_2006}. Next, these states should be
partitioned using one (or more) of the following methods:

\begin{enumerate}
      \item Test each state once
      \item Test the most common state transitions
      \item Test the least common state transitions
      \item Test all error states and error return transitions
      \item Test random state transitions
            \cite[p.~82-83]{patton_software_2006}
\end{enumerate}

For all of these tests, the values of the state variables should be verified
\cite[p.~83]{patton_software_2006}.

\subparagraph{Requirements}
\begin{itemize}
      \item Knowledge of the different states of the program
            \cite[p.~82]{patton_software_2006}; could be obtained through:
            \begin{itemize}
                  \item The program's modules and/or functions
                  \item The program's exceptions
            \end{itemize}
      \item Knowledge about the different state transitions
            \cite[p.~82]{patton_software_2006}; could be obtained through:
            \begin{itemize}
                  \item Testing the state transitions near the beginning of a
                        workflow more?
            \end{itemize}
\end{itemize}

\paragraph{Testing States to Fail \cite[p.~84-87]{patton_software_2006}}

The goal here is to try and put the program in a fail state by doing things
that are out of the ordinary. These include:

\begin{itemize}
      \item Race Conditions and Bad Timing \cite[p.~85-86]{patton_software_2006}
            (Is this relevant to our examples?)
      \item Repetition Testing: ``doing the same operation over and over'',
            potentially up to ``thousands of attempts''
            \cite[p.~86]{patton_software_2006}
      \item Stress Testing: ``running the software under less-than-ideal
            conditions'' \cite[p.~86]{patton_software_2006}
      \item Load testing: running the software with as large of a load as
            possible (e.g., large inputs, many peripherals)
            \cite[p.~86]{patton_software_2006}
\end{itemize}

\subparagraph{Requirements}
\begin{itemize}
      \item Repetition Testing: The types of operations that are likely to lead
            to errors when repeated (e.g., overwriting files?)
      \item Stress testing: can these be automated with pytest or are they
            outside our scope? \todo{Investigate}
      \item Load testing: Knowledge about the types of inputs that could
            overload the system (e.g., upper bounds on values of certain types)
\end{itemize}

\subsubsection{Other Black-Box Testing \cite[p.~87-89]{patton_software_2006}}
\begin{itemize}
      \item Act like an inexperienced user (likely cannot be generated by Drasil)
      \item Look for bugs where they've already been found (keep track of
            previous failed test cases?)
      \item Think like a hacker (is this out of scope?)
      \item Follow experience (this will implicitly be done just by using Drasil)
\end{itemize}

\subsection{Static White-Box Testing (Structural Analysis)
      \cite[p.~91-104]{patton_software_2006}}

\begin{itemize}
      \item ``The process of carefully and methodically reviewing the software
            design, architecture, or code for bugs without executing it''
            \cite[p.~92]{patton_software_2006}
      \item Less common than black-box testing, but often used for ``military,
            financial, factory automation, or medical software, \dots in a
            highly disciplined development model'' or when ``testing software
            for security issues'' \cite[p.~91]{patton_software_2006}; often
            avoided because of ``the misconception that it's too
            time-consuming, too costly, or not productive''
            \cite[p.~92]{patton_software_2006}
      \item Especially effective early on in the development process
            \cite[p.~92]{patton_software_2006}
      \item Can ``find bugs that would be difficult to uncover or isolate with
            dynamic black-box testing'' and ``gives the team's black-box
            testers ideas for test cases to apply''
            \cite[p.~92]{patton_software_2006}
\end{itemize}

\subsubsection{Formal Reviews \cite[p.~92-95]{patton_software_2006}}

\begin{itemize}
      \item ``The process under which static white-box testing is performed''
            \cite[p.~92]{patton_software_2006}; consists of four main parts:

            \begin{enumerate}
                  \item Identify Problems: Find what is wrong or missing
                  \item Follow Rules: There should be a structure to the review,
                        such as ``the amount of code to be reviewed \dots, how
                        much time will be spent \dots, what can be commented on,
                        and so on'', to set expectations; ``if a process is run
                        in an ad-hoc fashion, bugs will be missed and the
                        participants will likely feel that the effort was a
                        waste of time''
                  \item Prepare: Based on the participants' roles, they should
                        know what they will be contributing during the actual
                        review; ``most of the problems found through the review
                        process are found during preparation''
                  \item Write a Report: A summary should be created and provided
                        to the rest of the development team so that they know
                        what problems exist, where they are, etc.
                        \cite[p.~93]{patton_software_2006}
            \end{enumerate}

      \item Reviews improve communication, learning, and camaraderie, as well as
            the quality of code \emph{even before the review}: if a developer
            ``knows that his work is being carefully reviewed by his peers, he
            might make an extra effort to \dots make sure that it's right''
            \cite[p.~93-94]{patton_software_2006}

      \item Many forms:
            \begin{itemize}
                  \item Peer Review: Includes the four main parts of a formal
                        review, but informally and at a smaller scale
                        \cite[p.~94]{patton_software_2006}
                  \item Walkthrough: The author of the code presents it line
                        by line to a small group that ``question anything that
                        looks suspicious'' \cite[p.~95]{patton_software_2006}
                  \item Inspection: Someone who is \emph{not} the author of the
                        code presents it to a small group of people each with a
                        different perspective (e.g., user); changes are made
                        based on issues identified, and a reinspection may take
                        place \cite[p.~95]{patton_software_2006}
            \end{itemize}

      \item Could be used to evaluate code written for generation, as well as
            for the code that is generated
\end{itemize}

\subsubsection{Coding Standards and Guidelines \cite[p.~96-99]{patton_software_2006}}

\begin{itemize}
      \item Code may work but still be incorrect if it doesn't meet certain
            criteria, since these affect its reliability, readability,
            maintainability, and/or portability; e.g., the \texttt{goto},
            \texttt{while}, and \texttt{if-else} commands in C can cause bugs
            if used incorrectly \cite[p.~96]{patton_software_2006}
      \item These guidelines can range in strictness and formality, as long as
            they are agreed upon and followed \cite[p.~96]{patton_software_2006}
      \item This could be checked using linters
\end{itemize}

\subsubsection{Generic Code Review Checklist \cite[p.~99-103]{patton_software_2006}}

\begin{itemize}
      \item Data reference errors: ``bugs caused by using a variable, constant,
            \dots [etc.] that hasn't been properly declared or initialized''
            for its context \cite[p.~99]{patton_software_2006}
      \item Data declaration errors: bugs ``caused by improperly declaring
            or using variables or constants'' \cite[p.~100]{patton_software_2006}
      \item Computation errors: ``essentially bad math''; e.g., type mismatches,
            over/underflow, zero division, out of meaningful range
            \cite[p.~101]{patton_software_2006}
            \label{comp-errors}
      \item Comparison errors: ``very susceptible to boundary condition
            problems''; e.g., correct inclusion, floating point comparisons
            \cite[p.~101]{patton_software_2006}
      \item Control flow errors: bugs caused by ``loops and other control
            constructs in the language not behaving as expected''
            \cite[p.~102]{patton_software_2006}
      \item Subroutine parameter errors: bugs ``due to incorrect passing of data
            to and from software subroutines'' \cite[p.~102]{patton_software_2006}
      \item Input/output errors: e.g., how are errors handled?
            \cite[p.~102-103]{patton_software_2006}
      \item ASCII character handling, portability, compilation warnings
            \cite[p.~103]{patton_software_2006}
\end{itemize}

\paragraph{Requirements}
\begin{itemize}
      \item Data reference errors: know what operations are allowed for each
            type and check that values are only used for those operations
      \item Data declaration errors: I think this will mainly be covered by
            checking for data reference errors and by our generator (e.g., no
            typos in type names)
      \item Computation errors: partially tested dynamically by system tests,
            but could also more formally check for things like type mismatches
            (does \acs{gool} do this already?) or if divisors can ever be zero
      \item Comparison errors: I think this would mainly have to be done
            manually (maybe except for checking for (in)equality between values
            where it can never occur), but we may be able to generate a summary
            of all comparisons for manual verification
      \item Control flow errors: mostly irrelevant since we don't implement
            loops yet; would this include system tests?
      \item Subroutine parameter errors: we could check the types of values
            returned by a subroutine with the expected type (at least for
            languages like Python)
      \item Input/output errors: knowledge of (and more formal specification of)
            requirements would be needed here
      \item ASCII character handling, portability, compilation warnings:
            we could automatically check that the compiler (for languages that
            meaningfully have a compile stage) doesn't output any warnings
            (e.g., by saving output to a file and checking it is what is
            expected from a normal compilation); do we have any string inputs?
\end{itemize}


\subsection{Dynamic White-Box (Structural) Testing
      \cite[p.~105-121]{patton_software_2006}}

\begin{itemize}
      \item ``Using information you gain from seeing what the code does and how
            it works to determine what to test, what not to test, and how to
            approach the testing'' \cite[p.~105]{patton_software_2006}
      \item \emph{Mentions unit, integration, and system testing without
                  formally describing them} \cite[p.~109]{patton_software_2006}
            \todo{Find formal description of unit, integration, and system
                  testing}
      \item Two approaches for testing incrementally:
            \begin{itemize}
                  \item Bottom-up testing: uses \emph{test drivers}: ``tool[s]
                        that generate[] the test environment for a component to
                        be tested'' \cite[p.~410]{van_vliet_software_2000} by
                        ``sending test-case data to the modules under test,
                        read[ing] back the results, and verify[ing] that
                        they're correct'' \cite[p.~109]{patton_software_2006}
                  \item Top-down testing: uses \emph{test stubs}: tools that
                        ``simulate[] the function of a component not yet
                        available'' \cite[p.~410]{van_vliet_software_2000} by
                        providing ``fake'' values to a given module to be
                        tested \cite[p.~110]{patton_software_2006}
            \end{itemize}
\end{itemize}

\subsubsection{Data Coverage \cite[p.~114-116]{patton_software_2006}}

\begin{itemize}
      \item Data flow coverage: ``tracking a piece of data completely through
            the software'' (or a part of it), usually using debugger tools to
            check the values of variables \cite[p.~114]{patton_software_2006}
      \item Sub-boundaries: mentioned previously in \ref{sub-bound-conds}
      \item Formulas and equations: related to
            \hyperref[comp-errors]{computation errors}
      \item Error forcing: setting variables to specific values to see how
            errors are handled; any error forced must have a chance of
            occurring in the real world, even if it is unlikely
            \cite[p.~116]{patton_software_2006}
\end{itemize}

\subsubsection{Code Coverage \cite[p.~117-121]{patton_software_2006}}

``[T]est[ing] the program's states and the program's flow among them''
\cite[p.~117]{patton_software_2006}; allows for redundant and/or missing test
cases to be identified \cite[p.~118]{patton_software_2006}. We discussed that
generating infrastructure for reporting coverage may be a worthwhile goal, and
that it can be known how to increase certain types of coverage (since we know
the structure of the generated code, to some extent, beforehand), but I'm
not sure if all of these are feasible/worthwhile to get to 100\% (e.g., line
coverage).

\begin{itemize}
      \item Statement/line coverage: ``execut[ing] every statement in the
            program at least once''; doesn't guarantee full coverage
            (e.g., all paths might not be taken) \cite[p.~119]{patton_software_2006}
      \item Path coverage: ``[a]ttempting to cover all the paths in the
            software''; \emph{branch coverage} is the simplest form (\emph{the
                  difference is never explained}) \cite[p.~119]{patton_software_2006}
            \todo{Find a formal difference between path and branch coverage}
      \item Condition coverage: ``takes the extra conditions on the branch
            statements into account'' (e.g., all possible inputs to a Boolean
            expression) \cite[p.~120]{patton_software_2006}
\end{itemize}

\subsection{Regression Testing}

\begin{itemize}
      \item Rerunning old tests to test new changes to the system
      \item Various levels:
            \begin{itemize}
                  \item Retest-all: ``all tests are rerun''; ``this may consume
                        a lot of time and effort''
                        \cite[p.~411]{van_vliet_software_2000} (although
                        hopefully not, since we'll be automating testing)
                  \item Selective retest: ``only some of the tests are rerun''
                        after being selected by a \emph{regression test
                              selection technique}; ``[v]arious strategies have
                        been proposed for doing so; few of them have been
                        implemented yet'' \cite[p.~411]{van_vliet_software_2000}
            \end{itemize}
      \item \emph{Does not go into much detail} \todo{Investigate}
\end{itemize}

\subsection{\acf{mt}}
\label{chap:notes:sec:metamorphic-testing}
The use of \acfp{mr} ``to determine whether a test case has passed or failed''
\cite[p.~67]{kanewala_metamorphic_2019}. ``A[n] \acs{mr} specifies how the
output of the program is expected to change when a specified change is made to
the input'' \cite[p.~67]{kanewala_metamorphic_2019}; this is commonly done by
creating an initial test case, then transforming it into a new one by applying
the \acs{mr} (both the initial and the resultant test cases are executed and
should both pass) \cite[p.~68]{kanewala_metamorphic_2019}.

\subsubsection{Benefits of \acs{mt}}
\begin{itemize}
      \item Easier for domain experts; not only do they understand the domain
            (and its relevant \acp{mr}) \cite[p.~69]{kanewala_metamorphic_2019},
            they also may not have an understanding of testing principles
            \cite[p.~68]{kanewala_metamorphic_2019}. \emph{This majorly
                  overlaps with Drasil!}
      \item Easy to implement via scripts \cite[p.~68]{kanewala_metamorphic_2019}.
            \emph{Again, Drasil}
      \item Helps negate the test oracle \cite[p.~68]{kanewala_metamorphic_2019}
            and output validation \cite[p.~69]{kanewala_metamorphic_2019} problems
            from \nameref{chap:notes:sec:sci-testing-roadblocks} (\emph{i.e.,
                  the two that are relevant for Drasil})
      \item Can extend a limited number of test cases (e.g., from an
            experiment that was only able to be conducted a few times)
            \cite[p.~69]{kanewala_metamorphic_2019}
\end{itemize}
