\chapter{Notes}
\label{chap:notes}

\section{Software Metrics}
\label{chap:notes:sec:software-metrics}

\begin{itemize}
      \item The following branches of testing started as parts of quality
            testing:
            \begin{itemize}
                  \item Reliability testing \cite[p.~18, ch.~10]{fenton_software_1997}
                  \item Performance testing \cite[p.~18, ch.~7]{fenton_software_1997}
            \end{itemize}
      \item Reliability and maintainability can start to be tested even without
            code by ``measur[ing] structural attributes of representations of the
            software'' \cite[p.~18]{fenton_software_1997}
      \item The US Software Engineering Institute has a checklist for determining
            which types of lines of code are included when counting
            \cite[pp.~30-31]{fenton_software_1997}
      \item Measurements should include an entity to be measured, a specific
            attribute to measure, and the actual measure (i.e., units, starting
            state, ending state, what to include) \cite[p.~36]{fenton_software_1997}
            \begin{itemize}
                  \item These attributes must be defined before they can be
                        measured \cite[p.~38]{fenton_software_1997}
            \end{itemize}
\end{itemize}

\section{Software Testing}
\label{chap:notes:sec:software-testing}

\subsection{General Testing Notes}

\begin{itemize}
      \item Simple, normal test cases (test-to-pass) should always be developed
            and run before more complicated, unusual test cases (test-to-fail)
            \cite[p.~66]{patton_software_2006}
\end{itemize}

\subsection{Types of Testing}

\subsubsection{Dynamic Black-Box (Behavioural) Testing
      \cite[p.~64-65]{patton_software_2006}}

"Entering inputs, receiving outputs, and checking the results"
\cite[p.~64]{patton_software_2006}

\paragraph{Requirements}
\begin{itemize}
      \item Requirements documentation (definition of what the software does)
            \cite[p.~64]{patton_software_2006}; relevant information could be:
            \begin{itemize}
                  \item Requirements: Input-Values and Output-Values
                  \item Input/output data constraints
            \end{itemize}
\end{itemize}

\subsubsection{Exploratory Testing \cite[p.~65]{patton_software_2006}}

An alternative to dynamic black-box testing when a specification is not
available \cite[p.~65]{patton_software_2006}. The software is explored to
determine its features, and these features are then tested
\cite[p.~65]{patton_software_2006}. Finding any bugs using this method is a
positive thing \cite[p.~65]{patton_software_2006}, since despite not knowing
what the software \emph{should} do, you were able to determine that something
is wrong.

\paragraph{Requirements}
\begin{itemize}
      \item Source code \cite[p.~65]{patton_software_2006} (how can Drasil
            deduce functionality from this? Doxygen documentation? Is this
            necessary if we have the specification for dynamic black-box
            testing? Is this useful?)
\end{itemize}

\subsubsection{Equivalence Partitioning/Classing \cite[p.~67-69]{patton_software_2006}}

The process of dividing the infinite set of test cases into a finite set that is
just as effective (i.e., by revealing the same bugs) \cite[p.~67]{patton_software_2006}.

\paragraph{Requirements}
\begin{itemize}
      \item Ranges of possible values \cite[p.~67]{patton_software_2006};
            could be obtained through:
            \begin{itemize}
                  \item Input/output data constraints
                  \item Case statements
            \end{itemize}
\end{itemize}

\subsubsection{Data Testing \cite[p.~70-79]{patton_software_2006}}

The process of "checking that information the user inputs [and] results",
both final and intermediate, "are handled correctly" \cite[p.~70]{patton_software_2006}.

\paragraph{Boundary Conditions \cite[p.~70-74]{patton_software_2006}}

"[S]ituations at the edge of the planned operational limits of the software"
\cite[p.~72]{patton_software_2006}. Often affects types of data (e.g., numeric,
speed, character, location, position, size, quantity
\cite[p.~72]{patton_software_2006}) each with its own set of (e.g., first/last,
min/max, start/finish, over/under, empty/full, shortest/longest,
slowest/fastest, soonest/latest, largest/smallest, highest/lowest,
next-to/farthest-from \cite[p.~72-73]{patton_software_2006}). Data at these
boundaries should be included in an equivalence partition, but so should
data in between them \cite[p.~73]{patton_software_2006}. Boundary conditions
should be tested using "the valid data just inside the boundary,
... the last possible valid data, and ... the invalid data just outside the
boundary" \cite[p.~73]{patton_software_2006}. \emph{Buffer overruns} are
"the number one cause of software security issues"
\cite[p.~75]{patton_software_2006}.

% TODO: investigate "buffer overruns"

\subparagraph{Requirements}
\begin{itemize}
      \item Ranges of possible values \cite[p.~67, 73]{patton_software_2006};
            could be obtained through:
            \begin{itemize}
                  \item Case statements
                  \item Input/output data constraints (e.g., inputs that
                        would lead to a boundary output)
            \end{itemize}
\end{itemize}

\paragraph{Sub-Boundary Conditions \cite[p.~75-77]{patton_software_2006}}

Boundary conditions "that are internal to the software [but] aren't necessarily
apparent to an end user" \cite[p.~75]{patton_software_2006}. These include
powers of two \cite[p.~75-76]{patton_software_2006} and ASCII and Unicode tables
\cite[p.~76-77]{patton_software_2006}.

\subparagraph{Requirements}
\begin{itemize}
      \item Knowledge of powers of two \cite[p.~75-76]{patton_software_2006}
            (stored alongside \texttt{Integer, Rational, Real, Natural, Matrix
                  ::~Space})
      \item Knowledge of ASCII and Unicode tables \cite[p.~76-77]{patton_software_2006}
            (stored alongside \texttt{Char, String, DiscreteS ::~Space})
\end{itemize}

\paragraph{Default, Empty, Blank, Null, Zero, and None
      \cite[p.~77-78]{patton_software_2006}}

These should be their own equivalence class, since "the software usually
handles them differently" than "the valid cases or ... invalid cases"
\cite[p.~78]{patton_software_2006}.

\subparagraph{Requirements}
\begin{itemize}
      \item Knowledge of an "empty" value for each \texttt{Space} (stored
            alongside each type in \texttt{Space}?)
      \item Knowledge of how input data could be omitted from an input
            (e.g., a missing command line argument, an empty line in a file);
            could be obtained from:
            \begin{itemize}
                  \item User responsibilities
            \end{itemize}
      \item Knowledge of how a programming language deals with \texttt{Null}
            values and how these can be passed as arguments
\end{itemize}

\paragraph{Invalid, Wrong, Incorrect, and Garbage Data
      \cite[p.~78-79]{patton_software_2006}}

This is testing-to-fail \cite[p.~77]{patton_software_2006}.

\subparagraph{Requirements}
This seems to be the most open-ended category of testing.
\begin{itemize}
      \item Specification of correct inputs that can be ignored;
            could be obtained through:
            \begin{itemize}
                  \item Input/output data constraints (e.g., inputs that would
                        lead to a violated output constraint)
                  \item Type information for each input (e.g., passing a string
                        instead of a number)
            \end{itemize}
\end{itemize}

\subsubsection{State Testing \cite[p.~79-87]{patton_software_2006}}

The process of testing "a program's states and the transitions between them"
\cite[p.~79]{patton_software_2006}.

\paragraph{Logic Flow Testing \cite[p.~80-84]{patton_software_2006}}

This is done by creating a state transition diagram that includes:

\begin{itemize}
      \item Every possible unique state
      \item The condition(s) that take(s) the program between states
      \item The condition(s) and output(s) when a state is entered or exited
\end{itemize}

to map out the logic flow from the user's perspective
\cite[p.~81-82]{patton_software_2006}. Next, these states should be
partitioned using one (or more) of the following methods:

\begin{enumerate}
      \item Test each state once
      \item Test the most common state transitions
      \item Test the least common state transitions
      \item Test all error states and error return transitions
      \item Test random state transitions
            \cite[p.~82-83]{patton_software_2006}
\end{enumerate}

For all of these tests, the values of the state variables should be verified
\cite[p.~83]{patton_software_2006}.

\subparagraph{Requirements}
\begin{itemize}
      \item Knowledge of the different states of the program
            \cite[p.~82]{patton_software_2006}; could be obtained through:
            \begin{itemize}
                  \item The program's modules and/or functions
                  \item The program's exceptions
            \end{itemize}
      \item Knowledge about the different state transitions
            \cite[p.~82]{patton_software_2006}; could be obtained through:
            \begin{itemize}
                  \item Testing the state transitions near the beginning of a
                        workflow more?
            \end{itemize}
\end{itemize}

\paragraph{Testing States to Fail \cite[p.~84-87]{patton_software_2006}}

The goal here is to try and put the program in a fail state by doing things
that are out of the ordinary. These include:

\begin{itemize}
      \item Race Conditions and Bad Timing \cite[p.~85-86]{patton_software_2006}
            (Is this relevant to our examples?)
      \item Repetition Testing: "doing the same operation over and over",
            potentially up to "thousands of attempts"
            \cite[p.~86]{patton_software_2006}
      \item Stress Testing: "running the software under less-than-ideal conditions"
            \cite[p.~86]{patton_software_2006}
      \item Load testing: running the software with as large of a load as
            possible (e.g., large inputs, many peripherals)
            \cite[p.~86]{patton_software_2006}
\end{itemize}

\subparagraph{Requirements}
\begin{itemize}
      \item Repetition Testing: The types of operations that are likely to lead
            to errors when repeated (e.g., overwriting files?)
      \item Stress testing: can these be automated with pytest or are they
            outside our scope? % TODO: investigate
      \item Load testing: Knowledge about the types of inputs that could
            overload the system (e.g., upper bounds on values of certain types)
\end{itemize}

\subsubsection{Other Black-Box Testing \cite[p.~87-89]{patton_software_2006}}
\begin{itemize}
      \item Act like an inexperienced user (likely cannot be generated by Drasil)
      \item Look for bugs where they've already been found (keep track of
            previous failed test cases?)
      \item Think like a hacker (is this out of scope?)
      \item Follow experience (this will implicitly be done just by using Drasil)
\end{itemize}
