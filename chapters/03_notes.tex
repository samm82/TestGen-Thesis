% Defined here to reduce duplication of distinction between ``level'' and
%    ``phase'' and avoid ``already defined'' error
\newcommand{\procLevel}[1]{``Test level'' can also refer to the scope
of a test process; for example, ``across the whole organization'' or only
``to specific projects'' #1[p.~24]{IEEE2022}}
\newcommand{\phaseDef}{can also refer to the ``period of time in the software
      life cycle'' when testing occurs \citeyearpar[p.~470]{IEEE2017}, usually
      after the implementation phase (\citeyear[pp.~420,~509]{IEEE2017};
      \citealp[p.~56]{Perry2006}).}

% Define common footnotes about IEEE testing terms for reuse
\newcommand{\distinctIEEE}[1]{distinct from the notion of ``test #1'' described
      in \nameref{tab:ieeeTestTerms}.}
\newcommand{\notDefDistinctIEEE}[1]{\footnote{Not formally defined, but
            \distinctIEEE{#1}}}
\newcommand{\gerrardDistinctIEEE}[1]{\footnote{``Each type of test addresses a
            different risk area'' \citep[p.~12]{Gerrard2000}, which is
            \distinctIEEE{#1}}}

\chapter{Software Testing Research}
\label{chap:testing}

It was realized early on in the process that it would be beneficial to
understand the different kinds of testing (including what they test, what
artifacts are needed to perform them, etc.). This section provides some results
of this research, as well as some information on why and how it was performed.
\todo{A justification for why we decided to do this should be added}

\section{Scope}
\label{chap:testing:sec:scope}

This project is focused on the generation of test cases for code, so only
the ``testing'' component of \acf{vnv} is considered (see \thesisissueref{22}).
For example, design reviews (see \citealp[p.~132]{IEEE2017}) and
documentation reviews (see \citealp[p.~144]{IEEE2017}) are out of scope,
since they focus on the \acs{vnv} of the design and documentation of the code,
respectively, and not on the code itself. Likewise, ergonomics testing
and proximity-based testing (see \citetalias{ISTQB}) are out of scope since
they are for testing hardware systems, as is \acf{emsec} testing
(\citealp{ISO2021}; \citealp[p.~95]{ZhouEtAl2012}), which deals with the
``security risk'' of ``information leakage via electromagnetic emanation''
\citep[p.~95]{ZhouEtAl2012}. Processes that focus on ``an organization's \dots
processes and infrastructure'', like security audits, are also out of scope
\citepalias{ISTQB}. \textbf{NOTE:} While all the examples of domain-specific
testing given by \citet[p.~26]{Firesmith2015} are focused on hardware, this
might not be representative of all types (e.g., ML model testing seems
domain-specific).

It is also interesting to note that different test approaches seem to be more
specific to certain domains. For example, the terms ``software qualification
testing'' and ``system qualification testing'' show up throughout
\citep{SPICE2022}, which was written for the automotive industry, and the more
general idea of ``qualification testing'' seems to refer to the process of
making a hardware component, such as an electronic component
\citep{AhsanEtAl2020}, gas generator \citep{ParateEtAl2021} or photovoltaic
device, ``into a reliable and marketable product'' \citep[p.~1]{SuhirEtAl2013}.

This also means that only some aspects of some testing approaches are relevant.
This mainly manifests as a testing approach that can verify both the \acs{vnv}
itself and the code. For example:

\begin{enumerate}
      \item \emph{Error seeding} is the ``process of intentionally adding
            known faults to those already in a computer program'',
            done to both ``monitor[] the rate of detection and removal'',
            which is a part of \acs{vnv} of the \acs{vnv} itself, ``and
            estimat[e] the number of faults remaining''
            \citep[p.~165]{IEEE2017}, which helps verify the actual code.
      \item \emph{Fault injection testing}, where ``faults are artificially
            introduced into the \acs{sut}'', can be used to evaluate the
            effectiveness of a test suite \citep[p.~5-18]{SWEBOK2024},
            which is a part of \acs{vnv} of the \acs{vnv} itself, or ``to test
            the robustness of the system in the event of internal and
            external failures'' \citep[p.~42]{IEEE2022}, which helps verify
            the actual code.
      \item ``\emph{Mutation [t]esting} was originally conceived as a
            technique to evaluate test suites in which a mutant is a slightly
            modified version of the \acs{sut}'' \citep[p.~5-15]{SWEBOK2024},
            which is in the realm of \acs{vnv} of the \acs{vnv} itself.
            However, it ``can also be categorized as a structure-based
            technique'' and can be used to assist fuzz and metamorphic testing
            \citep[p.~5-15]{SWEBOK2024}.
      \item Even though \emph{reliability testing} and \emph{maintainability
                  testing} can start \emph{without} code by ``measur[ing]
            structural attributes of representations of the software''
            \citep[p.~18]{FentonAndPfleeger1997}, only reliability and
            maintainability testing done \emph{on} code is in scope.
      \item Since control systems often have a software \emph{and} hardware
            component \citep{ISO2015, Preu√üeEtAl2012,ForsythEtAl2004},
            only the software component is in scope. In some cases, it is
            unclear whether the ``loops''\footnote{Humorously, the testing of
                  loops in chemical systems \citep{Dominguez-PumarEtAl2020} and
                  copper loops \citep{Goralski1999} are out of scope.} being
            tested are implemented by software or hardware, such as those in
            wide-area damping controllers \citep{PierreEtAl2017, TrudnowskiEtAl2017}.
\end{enumerate}

Sometimes, the term ``testing'' excludes static testing
(\citealp[p.~222]{AmmannAndOffutt2017}; \citealp[p.~13]{Firesmith2015});
restricting it to ``dynamic validation'' \citep[p.~5-1]{SWEBOK2024} or
``dynamic verification'' ``in which a system or component is
executed'' \citep[p.~427]{IEEE2017}. Since ``terminology is not uniform
among different communities, and some use the term \emph{testing} to refer to
static techniques\notDefDistinctIEEE{technique} as well''
\citep[p.~5-2]{SWEBOK2024} (such as \citep[pp.~8-9]{Gerrard2000} and even
\citep[p.~440]{IEEE2017}!), the scope of ``testing'' for the purpose of this
project originally included both ``static testing'' and ``dynamic testing'', as
done by \citet[p.~17]{IEEE2022}. However, static testing tends to be less
systematic/consistent and often requires human intervention, which makes it
less relevant to this project's end goal: to generate test cases automatically.
However, understanding the breadth of testing approaches provides a more
complete picture of how software can be tested, how the various approaches are
related to one another, and potentially how even parts of these ``out-of-scope''
approaches may be generated in the future! These ``out-of-scope'' approaches
will be identified more systematically, but gathering information about them is
an important precursor, making them within the scope of this research, although
they will be excluded at a later phase. Even some dynamic methods, such as
demonstrations and dynamic analysis, which fall under the realm of ``evaluation''
as opposed to ``testing'' \citep[p.~13]{Firesmith2015} may be out of scope, due
to their reliance on human intervention.

\subsection{Derived Test Approaches}
\label{chap:testing:sec:derived-tests}
One group of test approaches given in \nameref{tab:ieeeTestTerms} is ``test
types'', which can be derived from software qualities: ``capabilit[ies] of
software product[s] to satisfy stated and implied needs when used under
specified conditions'' \citep[p.~424]{IEEE2017} \todo{OG ISO/IEC 2014}. This
is supported by \citeauthor{FentonAndPfleeger1997} who say that reliability
and performance testing are based on their underlying qualities
\citeyearpar[p.~18]{FentonAndPfleeger1997} and that measurements should include
an entity to be measured, a specific attribute to measure, and the actual
measure (i.e., units, starting state, ending state, what to include) (p.~36)
% \citeauthoryear[p.~36]{FentonAndPfleeger1997}
where attributes must be defined before they can be measured (p.~38).
% \citeauthoryear[p.~38]{FentonAndPfleeger1997}

After discussing this further (see \thesisissueref{21} and \thesisissueref{23}),
it was decided that tracking
software qualities, in addition to testing approaches, would be worthwhile
(see \thesisissueref{27}). This was done by capturing their definitions and any
rationale for why it might be useful to consider an explicitly separate
``test type'' in a separate document, so this information could be captured
without introducing clutter.

Similarly, since some types of requirements have associated types of
testing (e.g., functional, non-functional, security), it was discussed whether
each requirement type implies a related testing approach (such as ``technical
testing''). Even assuming this is the case, some types of requirements do not
apply to the code itself, and as such are out of scope (see \thesisissueref{43}):

\begin{itemize}
      \item \textbf{Nontechnical Requirement:} a ``requirement affecting product
            and service acquisition or development that is not a property of
            the product or service'' \citep[p.~293]{IEEE2017}
      \item \textbf{Physical Requirement:} a ``requirement that specifies a
            physical characteristic that a system or system component must
            possess'' \citep[p.~322]{IEEE2017}
\end{itemize}

Some test approaches seem to just be combinations of other (seemingly
orthogonal) approaches, such as the following (all from
\citepalias{ISTQB} unless otherwise indicated):
\begin{itemize}
      \item Black box conformance testing \citep[p.~25]{JardEtAl1999}
      \item Checklist-based reviews
      \item Closed-loop HiL verification \citep[p.~6]{Preu√üeEtAl2012}
      \item Closed-loop protection system testing \citep[p.~331]{ForsythEtAl2004}
      \item Endurance stability testing \citep[p.~55]{Firesmith2015}
      \item End-to-end functionality testing \citep[Tab.~2]{Gerrard2000}
      \item Formal reviews
      \item Informal reviews
      \item Infrastructure compatibility testing \citep[p.~53]{Firesmith2015}
      \item Legacy system integration (testing) \citep[Tab.~2]{Gerrard2000}
      \item Offline MBT
      \item Online MBT
      \item Role-based reviews
      \item Scenario walkthroughs \citep[Fig.~4]{Gerrard2000}
      \item Scenario-based reviews
      \item Security attacks
      \item Usability test script(ing)
\end{itemize}

However, some classes seem to be closely associated, such as remote testing
with asynchronous testing, and local with synchronous \citep{JardEtAl1999},
possibly implying a parent-child relation.

\section{Methodology}

This process initially involved looking through textbooks that were trusted at
McMaster \citep{Patton2006, PetersAndPedrycz2000, vanVliet2000}. However, this
process was somewhat ad hoc and arbitrary, meaning it wouldn't be as systematic
as required. Going forward, this process will be more rigorous, starting from
more established sources of software testing terminology in approximately the
following order:
(\citealp{IEEE2022, SWEBOK2024, SWEBOK2014, IEEE2017, IEEE2013, ISO_IEC2023b,
      IEEE2012, ISO_IEC2023a}; \citetalias{ISTQB}; \citealp{Firesmith2015}).
Other sources that focused on cataloging and categorizing testing terminology,
such as \citet{Kuƒºe≈°ovsEtAl2013}, were also examined to see how well they
agreed with the ``standard'' terminology.

I went through these resources by going through them looking for relevant
terminology, taking special care with glossaries and lists of terms. Of
particular note were terms that included ``test(ing)'', ``validation'',
``verification'', ``review'', ``audit'', or terms that had come up before
as part of already-discovered testing approaches, such as ``performance'',
``recovery'', ``component'', ``bottom-up'', ``boundary'', and ``configuration''.
If a term's definition had already been recorded, either the ``new'' one
replaced it if the ``old'' one wasn't as clear/concise or parts of both were
merged to paint a more complete picture. If any discrepancies or ambiguities
arose, they were investigated to a reasonable extent and documented. If a
testing approach was mentioned but not defined, it was still added to the
glossary to indicate it should be investigated further. A similar methodology
was used for tracking software qualities, albeit in a separate document
(see \nameref{chap:testing:sec:derived-tests}).

During this investigation, some terms came up that seemed to be relevant to
testing but were so vague, they didn't provide any new information. These were
decided to be not worth tracking (see \thesisissueref{39}, \thesisissueref{44},
\thesisissueref{28}) and are listed below:

\begin{itemize}
      \item \textbf{Evaluation:} the ``systematic determination of the extent
            to which an entity meets its specified criteria''
            \citep[p.~167]{IEEE2017}
      \item \textbf{Product Analysis:} the ``process of evaluating a product by
            manual or automated means to determine if the product has certain
            characteristics'' \citep[p.~343]{IEEE2017}
      \item \textbf{Quality Audit:} ``a structured, independent process to
            determine if project activities comply with organizational and
            project policies, processes, and procedures'' \citep[p.~361]{IEEE2017}
            \todo{OG PMBOK}
      \item \textbf{Software Product Evaluation:} a ``technical operation that
            consists of producing an assessment of one or more characteristics
            of a software product according to a specified procedure''
            \citep[p.~424]{IEEE2017}
\end{itemize}

However, over the course of this research, our scope was adjusted to include
some terms for our initial list of test approaches to be filtered out later,
such as types of attacks (see \thesisissueref{55}), meaning that some entries
were missed during the first pass(es) of these resources. While reiterating
over these resources would be ideal, this may not be possible due to time
constraints.

Different sources categorized software testing approaches in different ways;
while it is useful to record and think about these categorizations (see
\nameref{testing-categories}), following one (or more) during the research
stage could lead to bias and a prescriptive categorization, instead of letting
one emerge descriptively during the analysis stage. Since these categorizations
are not mutually exclusive, it also means that more than one could be useful
(both in general and to this specific project); more careful thought should be
given to which are ``best'', and this should happen during the analysis stage.

\section{Observations}

\subsection{Categories of Testing Approaches}

For classifying different kinds of tests, \citet{IEEE2022} provide some
terminology (see \refIEEETestTerms). However, other sources
\citep{BarbosaEtAl2006, SouzaEtAl2017} provide alternate
categories (see \refOtherTestTerms) which may be beneficial to investigate to
determine if this categorization is sufficient. A ``metric'' categorization was
considered at one point, but was decided to be out of the scope of this project
(see \nameref{chap:testing:sec:scope}, \thesisissueref{21}, and \thesisissueref{22}).

\begin{landscape}
      \ieeeTestTermsTable{}
\end{landscape}

\newgeometry{margin=1cm}
\begin{landscape}
      \otherTestTermsTable{}
\end{landscape}
\restoregeometry

\subsection{Categorizations}
\label{testing-categories}

Software testing approaches can be divided into the following
categories. Note that ``there is a lot of overlap between different classes of
testing'' \citep[p.~8]{Firesmith2015} so, for example, ``one category [of test
            techniques] might deal with combining two or more techniques''
\citep[p.~5-10]{SWEBOK2024}. A side effect of this is that it is difficult to
``untangle'' these classes; for example, take the following sentence: ``whitebox
fuzzing extends dynamic test generation based on symbolic execution and
constraint solving from unit testing to whole-application security testing''
\citep[p.~23]{GodefroidAndLuchaup2011}!

Despite its challenges, it is useful to understand the differences between
testing classes because tests from multiple subsets within the same category,
such as functional and structural, ``use different sources of information and
have been shown to highlight different problems'' \citep[p.~5-16]{SWEBOK2024}.
However, some subsets, such as deterministic and random, may have ``conditions
that make one approach more effective than the other''
\citep[p.~5-16]{SWEBOK2024}.

\begin{itemize}
      \item Visibility of code: black-, white-, or gray-box (functional,
            structural, or a mix of the two)
            (\citealp[pp.~5-10,~5-16]{SWEBOK2024};
            \citealp[pp.~57-58]{AmmannAndOffutt2017};
            \citealp[p.~213]{Kuƒºe≈°ovsEtAl2013};
            \citealp[pp.~53,~218]{Patton2006}; \citealp[p.~69]{Perry2006})
      \item Level/stage\footnote{See \nameref{tab:ieeeTestTerms}.} of testing:
            unit, integration, system, or acceptance
            (\citealp[pp.~5-6 to 5-7]{SWEBOK2024}; \citetalias{ISTQB};
            \citealp[p.~218]{Kuƒºe≈°ovsEtAl2013} \todo{OG Black, 2009};
            \citealp{Patton2006}; \citealp{Perry2006};
            \citealp{PetersAndPedrycz2000}; \citealp[pp.~9,~13]{Gerrard2000})
            (sometimes includes installation \citep[p.~439]{vanVliet2000} or
            regression \citep[p.~3]{BarbosaEtAl2006})
      \item Key aspect: specification, structure, or experience
            \citep[p.~5-10]{SWEBOK2024}
            \todo{Originally in ISO/IEC/IEEE 29119-4:2021}
      \item Test case selection process: deterministic or random
            \citep[p.~5-16]{SWEBOK2024}
      \item Coverage criteria: input space partitioning, graph coverage, logic
            coverage, or syntax-based testing \citep[pp.~18-19]{AmmannAndOffutt2017}
      \item Question: what-, when-, where-, who-, why-, how-, and how-well-based
            testing; these are then divided into a total of ``16 categories of
            testing types''\notDefDistinctIEEE{type}
            \citep[p.~17]{Firesmith2015}
      \item Execution of code: static or dynamic
            (\citealp[p.~214]{Kuƒºe≈°ovsEtAl2013}; \citealp[p.~12]{Gerrard2000};
            \citealp[p.~53]{Patton2006})
      \item Goal of testing: verification or validation
            (\citealp[p.~214]{Kuƒºe≈°ovsEtAl2013}; \citealp[pp.~69-70]{Perry2006})
      \item Property of code: functional or non-functional
            \citep[p.~213]{Kuƒºe≈°ovsEtAl2013}
      \item Human involvement: manual or automated
            \citep[p.~214]{Kuƒºe≈°ovsEtAl2013}
      \item Structuredness: scripted or exploratory
            \citep[p.~214]{Kuƒºe≈°ovsEtAl2013}
      \item Source of test data: specification-, implementation-, or
            error-oriented \citep[p.~440]{PetersAndPedrycz2000}
      \item Adequacy criterion: coverage-, fault-, or error-based
            (``based on knowledge of the typical errors that people make'')
            \citep[pp.~398-399]{vanVliet2000}
      \item Priority\footnote{In the context of testing e-business projects.}:
            smoke, usability, performance, or functionality testing
            \citep[p.~12]{Gerrard2000}
      \item Category of test ``type''\gerrardDistinctIEEE{type}: static testing,
            test browsing, functional testing, non-functional testing, or large
            scale integration (testing) \citep[p.~12]{Gerrard2000}
      \item Purpose: correctness, performance, reliability, or security
            \citep{Pan1999}
\end{itemize}

Tests can also be tailored to ``test factors'' (also called ``quality factors''
or ``quality attributes''): ``attributes of the software that, if they are
wanted, pose a risk to the success of the software''
\citep[p.~40]{Perry2006}. These include correctness, file integrity,
authorization, audit trail, continuity of processing, service levels
(e.g., response time), access control, compliance, reliability, ease of use,
maintainability, portability, coupling (e.g., with other applications in a
given environment), performance, and ease of operation (e.g., documentation,
training) \citep[pp.~40-41]{Perry2006}. \emph{These may overlap with
      \nameref{chap:testing:sec:derived-tests} and/or
      the ``Results of Testing (Area of Confidence)'' column in the summary
      spreadsheet.}

Engstr√∂m ``investigated classifications of research''
\citep[p.~1]{engstr√∂m_mapping_2015} on the following four testing techniques.
\emph{These four categories seem like comparing apples to oranges to me.}

\begin{itemize}
      \item \textbf{Combinatorial testing:} how the system under test is
            modelled, ``which combination strategies are used to generate test
            suites and how test cases are prioritized''
            \citep[pp.~1-2]{engstr√∂m_mapping_2015}
      \item \textbf{Model-based testing:} the information represented and
            described by the test model \citep[p.~2]{engstr√∂m_mapping_2015}
      \item \textbf{Search-based testing:} ``how techniques
            \notDefDistinctIEEE{technique} had been empirically evaluated
            (i.e. objective and context)'' \citep[p.~2]{engstr√∂m_mapping_2015}
      \item \textbf{Unit testing:} ``source of information (e.g. code,
            specifications or testers intuition)''
            \citep[p.~2]{engstr√∂m_mapping_2015}
\end{itemize}

\subsection{Existing Taxonomies, Ontologies, and the State of Practice}

One thing we may want to consider when building a taxonomy/ontology is the
semantic difference between related terms. For example, one ontology found
that the term ``\,`IntegrationTest' is a kind of Context (with
semantic of stage, but not a kind of Activity)'' while ``\,`IntegrationTesting'
has semantic of Level-based Testing that is a kind of Testing Activity [or]
\dots of Test strategy'' \citep[p.~157]{TebesEtAl2019}.

A note on testing artifacts is that they are ``produced and used throughout
the testing process'' and include test plans, test procedures, test cases, and
test results \citep[p.~3]{SouzaEtAl2017}. The role of testing
artifacts is not specified in \citep{BarbosaEtAl2006};
requirements, drivers, and source code are all treated the same
with no distinction \citep[p.~3]{BarbosaEtAl2006}.

In \citep{SouzaEtAl2017}, the ontology (ROoST) \todo{add acronym?} is made to
answer a series of questions, including ``What is the test level of a testing
activity?'' and ``What are the artifacts used by a testing activity?''
\citep[pp.~8-9]{SouzaEtAl2017}. \todo{is this punctuation right?}
The question ``How do testing artifacts relate to each other?''
\citep[p.~8]{SouzaEtAl2017} is later broken down into multiple questions,
such as ``What are the test case inputs of a given test case?'' and ``What are
the expected results of a given test case?'' \citep[p.~21]{SouzaEtAl2017}.
\emph{These questions seem to overlap with the questions we were trying to ask
      about different testing techniques.}

Most ontologies I can find seem to focus on the high-level testing process
rather than the testing approaches themselves. For example, the terms and
definitions \citep{TebesEtAl2020b}
from TestTDO \citep{TebesEtAl2020a} provide \emph{some} definitions of
testing approaches, but mainly focus on parts of the testing process
(e.g., test goal, test plan, testing role, testable entity) and how they relate
to one another. \citet[pp.~152-153]{TebesEtAl2019} may provide some
sources for software testing terminology and definitions (this seems to include
\href{https://github.com/samm82/TestGen-Thesis/issues/14#issuecomment-1839922715}
{the ones suggested by Dr.~Carette}) in addition to a list of ontologies
(some of which have been investigated).

One software testing model developed by the \acf{qai} includes the test
environment (``conditions \dots
that both enable and constrain how testing is performed'', including mission,
goals, strategy, ``management support, resources, work processes, tools,
motivation''), test process (testing ``standards and procedures''), and tester
competency (``skill sets needed to test software in a test environment'')
\citep[pp.~5-6]{Perry2006}.

\citet{UnterkalmsteinerEtAl2014} provide a foundation to allow one ``to
classify and characterize alignment research and solutions that focus on the
boundary between [requirements engineering and software testing]'' but ``do[]
not aim at providing a systematic and exhaustive state-of-the-art survey of
      [either domain]'' (p.~A:2).

Another source
introduced the notion of an ``intervention'': ``an act performed (e.g. use of a
technique\notDefDistinctIEEE{technique} or a process change) to adapt testing
to a specific context, to solve
a test issue, to diagnose testing or to improve testing''
\citep[p.~1]{engstr√∂m_mapping_2015} and noted that ``academia tend[s] to focus on
characteristics of the intervention [while] industrial standards categorize the
area from a process perspective'' \citep[p.~2]{engstr√∂m_mapping_2015}.
It provides a structure to ``capture both a problem perspective and a solution
perspective with respect to software testing'' \citep[pp.~3-4]{engstr√∂m_mapping_2015},
but this seems to focus more on test interventions and challenges rather than
approaches \citep[Fig.~5]{engstr√∂m_mapping_2015}.

\subsection{Discrepancies and Ambiguities}

\citet{IEEE2022} mentions the following 44 test approaches without defining them.
This means that out of the 99 identified test approaches, almost 45\% had no
associated definition!
However, the previous version of this standard, \citep{IEEE2013}, generally
explained two, provided references for three, and explicitly defined three of
these terms, for a total of eight definitions that could (should) have been
included in \citep{IEEE2022}! These are marked with \underline{underline},
\emph{italics}, and \textbf{bold}, respectively. Additionally, entries marked
with an asterisk* were defined (at least partially) in \citep{IEEE2017}, which
would have been available when creating \citep{IEEE2022}. These terms bring the
total count of terms that could (should) have been defined in \citep{IEEE2022}
to eighteen; almost 20\% of undefined test approaches could have been defined!

\begin{itemize}
      \item \underline{Acceptance Testing*}
      \item All Combinations Testing
      \item All-C-Uses Testing (Data Definition C-Use Pair*)
      \item All-Definitions Testing
      \item All-DU-Paths Testing (Data Definition-Use Path*)
      \item All-P-Uses Testing (Data Definition P-Use Pair*)
      \item All-Uses Testing (Data Definition-Use Pair*)
      \item Alpha Testing*
      \item Base Choice Testing (also mentioned but not defined in
            \citep{IEEE2017})
      \item Beta Testing*
      \item Branch Condition Combination Testing
      \item Branch Condition Testing
      \item \textbf{Capacity Testing}*
      \item Capture-Replay Driven Testing
      \item Cause-Effect Testing
      \item Classification Tree Method (also mentioned but not defined in
            \citep{IEEE2013})
      \item Conversion Testing
      \item \emph{Data Flow Testing}* (ISO/IEC/IEEE 29119-4)
      \item Data-driven Testing
      \item Disaster/Recovery Testing (Disaster Recovery*)
      \item Each Choice Testing
      \item Factory Acceptance Testing
      \item Fault Injection Testing
      \item Functional Suitability Testing (also mentioned but not defined in
            \citep{IEEE2017})
      \item \textbf{Installability Testing}*
      \item \underline{Integration Testing}*
      \item Localization Testing
      \item Model Verification
      \item Negative Testing
      \item Operational Acceptance Testing
      \item Performance-related Testing (although Performance Testing is
            defined in \citep{IEEE2022}; see \hyperref[perf-test-ambiguity]
            {the ``performance testing'' ambiguity} below)
      \item Production Verification Testing
      \item Recovery Testing (\textbf{Backup and Recovery Testing}*, Recovery*)
      \item Response-Time Testing
      \item \emph{Reviews} (ISO/IEC 20246) (Code Reviews*)
      \item Scalability Testing
      \item Statistical Testing
      \item Syntax Testing
      \item System Integration Testing (System Integration*)
      \item System Testing* (also mentioned but not defined in \citep{IEEE2013})
      \item \emph{Unit Testing}* (IEEE Std 1008-1987, IEEE Standard for
            Software Unit Testing implicitly listed in the bibliography!)
      \item Usability Testing* (also mentioned but not defined in \citep{IEEE2013};
            in \citep{IEEE2017}, ``usability test'' is defined with a note to
            compare it to ``usability testing'' (p.~493), but no corresponding
            entry is present)
      \item Use Case Testing (also mentioned but not defined in \citep{IEEE2013})
      \item User Acceptance Testing
\end{itemize}

Additionally, discrepancies and ambiguities exist both among sources and
within individual ones; these may be areas for further investigation:

\begin{enumerate}
      \item ``Compatibility testing'' is defined as ``testing that measures the
            degree to which a test item can function satisfactorily alongside
            other independent products in a shared environment (co-existence),
            and where necessary, exchanges information with other systems or
            components (interoperability)'' \citep[p.~3]{IEEE2022}. This
            definition is nonatomic as it combines the ideas of ``co-existence''
            and ``interoperability''. The term ``interoperability testing'' is
            not defined, but is used three times \citep[pp.~22,~43]{IEEE2022}
            (although the third usage seems like it should be ``portability
            testing''). This implies that ``co-existence testing'' and
            ``interoperability testing'' should be defined as their own terms,
            which is supported by definitions of ``co-existence'' and
            ``interoperability'' often being separate (\citetalias{ISTQB};
            \citealp[pp.~73,~237]{IEEE2017}), the definition of
            ``interoperability testing'' from \citet[p.~238]{IEEE2017},
            and the decomposition of ``compatibility'' into ``co-existence''
            and ``interoperability'' by \citet{ISO_IEC2023a}!
      \item Experience-based testing is categorized as both a test design
            technique and a test practice on the same page
            \citep[p.~22]{IEEE2022}! This also causes confusion about
            its children, such as error guessing and exploratory testing;
            \citet[p.~34]{IEEE2022} say error guessing is an ``experience-based
            test design technique'' and ``experience-based test practices
            include \dots exploratory testing, tours, attacks, and
            checklist-based testing''. There are several instances of
            inconsistencies between parent and child test approach
            categorizations (which may indicate they aren't necessarily the
            same, or that more thought must be given to
            classification/organization).
      \item ``Fuzz testing'' is ``tagged'' (?) as ``artificial intelligence''
            \citep[p.~5]{IEEE2022}, although I don't think this is a
            set-in-stone requirement.
      \item ``Load testing'' is defined as using loads ``usually between
            anticipated conditions of low, typical, and peak usage''
            \citep[p.~5]{IEEE2022}, while \citet[p.~86]{Patton2006} says the
            loads should as large as possible.
      \item Integration, system, and system integration testing are all listed
            as ``common test levels'' \citep[p.~12]{IEEE2022}, but no
            definitions are given for the latter two, making it unclear what
            ``system integration testing'' is; it is a combination of the two?
            somewhere on the spectrum between them? It is listed as a child
            of integration testing by \citetalias{ISTQB} and of system testing
            by \citet[p.~23]{Firesmith2015}.
      \item Similarly, component, integration, and component integration
            testing are all listed in \citep{IEEE2017}, but ``component
            integration testing'' is only defined as ``testing of groups of
            related components'' \citep[p.~82]{IEEE2017}; it is a combination of
            the two? somewhere on the spectrum between them? Likewise, it is
            listed as a child of integration testing by \citetalias{ISTQB}.
      \item ``Disaster/recovery testing'' and ``recovery testing'' (as a subset
            of performance-related testing) are both listed as test types
            \citep[p.~22]{IEEE2022} but not defined, making it unclear what
            distinguishes them. \citet[p.~2]{IEEE2013} define ``backup and
            recovery testing'' as  testing ``that measures the degree to which
            system state can be restored from backup within specified parameters
            of time, cost, completeness, and accuracy in the event of failure'',
            which may be what is meant by ``recovery
            testing'' in the context of performance-related testing.
            Meanwhile, SWEBOK V4 defines ``recovery testing'' as the testing of
            ``software restart capabilities after a system crash or other
            disasters [sic]'' \citep[p.~5-9]{SWEBOK2024}, which may be what is
            meant \emph{outside} of the context of performance.
      \item Similarly, ``branch condition testing'' and ``branch condition combination
            testing'' are both listed as subsets of structure-based testing
            \citep[p.~22]{IEEE2022} but are not defined, making it unclear what
            distinguishes them.
      \item ``Installability testing'' is given as a test type
            \citep[p.~22]{IEEE2022} but is sometimes called a test level as
            ``installation testing'' \citep[p.~445]{PetersAndPedrycz2000}.
      \item Retesting and regression testing seem to be separated from the rest
            of the testing approaches \citep[p.~23]{IEEE2022}, but it is not
            clearly detailed why; \citet[p.~3]{BarbosaEtAl2006} considers
            regression testing to be a test level.
      \item A component is an ``entity with discrete structure \dots within a
            system considered at a particular level of analysis''
            \citep{ISO_IEC2023b} and ``the terms module, component, and unit
                  [sic] are often used interchangeably or defined to be subelements
            of one another in different ways depending upon the context'' with
            no standardized relationship \citep[p.~82]{IEEE2017}. This means
            unit/component/module testing can refer to the testing of both a
            module and a specific function in a module (see \thesisissueref{14}).
            However, ``component'' is sometimes defined differently than
            ``module'': ``components differ from classical modules for being
            re-used in different contexts independently of their development''
            \citep[p.~107]{BaresiAndPezz√®2006}, so distinguishing the two
            may be necessary.
      \item SWEBOK V4 says ``scalability testing evaluates the capability to
            use and learn the system and the user documentation. It also focuses
            on the system's effectiveness in supporting user tasks and the
            ability to recover from user errors'' \citep[p.~5-9]{SWEBOK2024}.
            This description seems to describe ``usability testing'' instead,
            despite earlier defining/describing ``scalability'' as \ldots
            \begin{enumerate}
                  \item ``the software's ability to increase and scale up on its
                        nonfunctional requirements, such as load, number of
                        transactions, and volume of data'' \citep[p.~5-5]{SWEBOK2024}
                  \item ``connected to the complexity of the platform and
                        environment in which the program runs, such as
                        distributed, wireless networks and virtualized
                        environments, large-scale clusters, and mobile clouds''
                        \citep[p.~5-5]{SWEBOK2024}
            \end{enumerate}
            Other definitions of ``scalability'' support these definitions, so
            the definition of ``scalability testing'' follows trivially from there
            (sometimes explicitly \citepalias{ISTQB}):
            \begin{itemize}
                  \item The ``capability of a product to handle growing or
                        shrinking workloads or to adapt its capacity to handle
                        variability'' \citep{ISO_IEC2023a}
                  \item ``The degree to which a component or system can be
                        adjusted for changing'' \citepalias{ISTQB}
            \end{itemize}
      \item SWEBOK V4 says that one objective of elasticity testing is ``to
            evaluate scalability'' \citep[p.~5-9]{SWEBOK2024}, which seems like
            an objective of scalability testing instead.
      \item SWEBOK V4 defines ``privacy testing'' as testing that ``assess[es]
            the security and privacy of users' personal data to prevent local
            attacks'' \citep[p.~5-10]{SWEBOK2024}; this seems to overlap with
            \citeauthor{IEEE2022}'s definition of ``security testing'', which is
            ``conducted to evaluate the degree to which a test item, and
            associated data and information, are protected so that'' only
            ``authorized persons or systems'' can use them as intended
            \citeyearpar{IEEE2022}, both in scope and name.
      \item \citeauthor*{IEEE2017} provide a definition for ``inspections and
            audits'' \citeyearpar[p.~228]{IEEE2017}, despite also giving
            definitions for ``inspection'' \citeyearpar[p.~227]{IEEE2017} and
            ``audit'' \citeyearpar[p.~36]{IEEE2017}; while the first term
            \emph{could} be considered a superset of the latter two, this
            distinction doesn't seem useful.
      \item \citeauthor*{IEEE2017} say that ``test level'' and ``test phase''
            are synonyms, both meaning a ``specific instantiation of [a] test
            sub-process'' (\citeyear[pp.~469,~470]{IEEE2017};
            \citeyear[p.~9]{IEEE2013}), but there are also alternative
            definitions for them. \procLevel{\citeyearpar}, while
            ``test phase'' \phaseDef{}
      \item \citet{IEEE2017} use the same definition for ``partial correctness''
            (p.~314) and ``total correctness'' (p.~480).
      \item Various sources say that alpha testing is performed by different
            people, including ``only by users within the organization
            developing the software'' \citep[p.~17]{IEEE2017}, by ``a small,
            selected group of potential users'' \citep[p.~5-8]{SWEBOK2024}, or
            ``in the developer's test environment by roles outside the
            development organization'' \citepalias{ISTQB}.
      \item While correct, ISTQB's definition of ``specification-based testing''
            is not helpful: ``testing based on an analysis of the specification
            of the component or system'' \citepalias{ISTQB}.
      \item Sometimes ``condition testing'' and ``decision testing'' are treated
            as synonyms \citep[p.~5-13]{SWEBOK2024}, but sometimes they are kept
            separate, with the former only involving atomic conditions
            \citepalias{ISTQB}, implying that it may be a sub-approach of the
            latter?
      \item ``ML model testing'' and ``ML functional performance'' are defined
            in terms of ``ML functional performance criteria'', which is defined
            in terms of ``ML functional performance metrics'', which is defined
            as ``a set of measures that relate to the functional correctness of
            an ML system'' \citepalias{ISTQB}. The use of ``performance'' (or
            ``correctness'') in these definitions are at best ambiguous and at
            worst incorrect.
      \item While ergonomics testing is out of scope (as it tests hardware, not
            software), its definition of ``testing to determine whether a
            component or system and its input devices are being used properly
            with correct posture'' \citepalias{ISTQB} seems to focus on how the
            system is \emph{used} as opposed to the system \emph{itself}.
      \item The definition of ``math testing'' given by \citetalias{ISTQB} is
            too specific to be useful, likely taken from an example instead of
            a general definition: ``testing to determine the correctness of the
            pay table implementation, the random number generator results, and
            the return to player computations''.
      \item A similar issue exists with multiplayer testing, where its
            definition specifies ``the casino game world'' \citepalias{ISTQB}.
      \item Thirdly, ``par sheet testing'' from \citetalias{ISTQB} seems to
            refer to this specific example and does not seem more widely
            applicable, since a ``PAR sheet'' is ``a list of all the symbols
            on each reel of a slot machine'' \citep{Bluejay2024}.
      \item The definition of ``scalability'' given by \citetalias{ISTQB} is
            ``the degree to which a component or system can be adjusted for
            changing \emph{capacity}'' (emphasis added), but the source that it
            references says that scalability is ``the measure of a system's
            ability to be upgraded to accommodate increased \emph{loads}''
            \citep[p.~381,~emphasis added]{GerrardAndThompson2002}; this is
            accomplished \emph{through} increasing capacity, not in reaction to
            it (p.~192).
      \item \citetalias{ISTQB} describe the term ``software in the loop'' as a
            kind of testing, while the source it references seems to describe
            ``Software-in-the-Loop-Simulation'' as a ``simulation environment''
            that may support software integration testing
            \citep[p.~153]{SPICE2022}; is this a testing approach or a tool
            that supports testing?
      \item The source cited for the definition of ``test type'' from
            \citetalias{ISTQB} does not seem to provide a definition itself.
      \item The same is true for ``visual testing''.
      \item The same is true for ``security attack''.
      \item There is disagreement on the structure of tours; they can either be
            quite general \citep[p.~34]{IEEE2022} or ``organized around a
            special focus'' \citepalias{ISTQB}.
      \item ``Scenario testing'' and ``use case testing'' are given as synonyms
            by \citetalias{ISTQB}, but listed separately by
            \citet[p.~22]{IEEE2022} (although the latter is not defined).
      \item While model testing is said to test the object under test,
            it seems to describe testing the models themselves
            \citet[p.~20]{Firesmith2015}; using the models to test the object
            under test seems to be called ``driver-based testing'' (p.~33).
      \item ``System testing'' is listed as a subtype of ``system testing'' by
            \citet[p.~23]{Firesmith2015}.
      \item ``Hardware-'' and ``human-in-the-loop testing'' have the same
            acronym: ``HIL''\footnote{``HiL'' is used for the former by
                  \citet[p.~2]{Preu√üeEtAl2012}.} \citep[p.~23]{Firesmith2015}.
      \item The same is true for ``customer'' and ``contract(ual) acceptance
            testing'' (``CAT'') \citep[p.~30]{Firesmith2015}.
      \item The acronym ``SoS'' is used but not defined by
            \citet[p.~23]{Firesmith2015}.
      \item It is ambiguous whether ``tool/environment testing'' refers to
            testing the tools/environment \emph{themselves} or \emph{using}
            them to test the object under test; the latter is implied, but the
            wording of its subtypes \citep[p.~25]{Firesmith2015} seems to imply
            the former.
      \item ``Operational'' and ``production acceptance testing'' are treated
            as synonyms by \citetalias{ISTQB}, but listed separately by
            \citet[p.~30]{Firesmith2015}.
      \item ``Production acceptance testing'' \citep[p.~30]{Firesmith2015}
            seems to be the same as ``production verification testing''
            \citep[p.~22]{IEEE2022}, but neither are defined.
      \item \citeauthor{Firesmith2015} makes a distinction between organization-
            and role-based testing \citeyearpar[pp.~17,~37,~39]{Firesmith2015},
            which seems arbitrary, but further investigation may prove it to be
            meaningful (see \thesisissueref{59}).
      \item \citeauthor{ISO_IEC2023a} say that performance and security testing
            are subtypes of reliability testing \citeyearpar{ISO_IEC2023a}, but
            these are all listed separately by \citet[p.~53]{Firesmith2015}.
      \item While \citeauthor{Firesmith2015} says that fault tolerance testing
            is a subtype\notDefDistinctIEEE{type} of robustness testing
            \citeyearpar[p.~56]{Firesmith2015}
            (which is distinct from reliability testing (p.~53)), other sources
            say it is a subtype of reliability testing (\citealp[p.~375]{IEEE2017};
            \citealp[p.~7-10]{SWEBOK2024}) or a synonym of ``robustness
            testing'' \citepalias{ISTQB}.
      \item The distinctions between development testing \citep[p.~136]{IEEE2017},
            developmental testing \citep[p.~30]{Firesmith2015}, and developer
            testing (\citealp[p.~39]{Firesmith2015}; \citealp[p.~11]{Gerrard2000})
            are unclear and seem miniscule.
      \item \citeauthor{ChalinEtAl2006}~list \acf{rac} and \acf{sv} as ``two
            complementary forms of assertion checking''
            \citeyearpar[p.~343]{ChalinEtAl2006}; based on how the term ``static
            assertion checking'' is used by \citet[p.~345]{LahiriEtAl2013}, it
            seems like this should be the complement to \acs{rac} instead.
      \item Availability testing isn't assigned to a test priority
            \citep[Tab.~2]{Gerrard2000}, despite the claim that ``the test
            types\gerrardDistinctIEEE{type} have been allocated a slot against
            the four test priorities'' (p.~13); I think usability and/or
            performance would have made sense.
      \item ``Visual browser validation'' is described as both static \emph{and}
            dynamic in the same table \citep[Tab.~2]{Gerrard2000}, even though
            they are implied to be orthogonal classifications: ``test
            types can be static \emph{or} dynamic'' (p.~12,~emphasis added).
      \item \citeauthor{Gerrard2000} makes a distinction between ``transaction
            verification'' and ``transaction testing''
            \citeyearpar[Tab.~2]{Gerrard2000} and also uses the phrase
            ``transaction flows'' (Fig.~5) but doesn't explain them.
      \item The phrase ``continuous automated testing''  \citep[p.~11]{Gerrard2000}
            is redundant since continuous testing is a sub-category of automated
            testing (\citealp[p.~35]{IEEE2022}, \citetalias{ISTQB}).
      \item \citeauthor{Gerrard2000} very helpfully indicates that performance
            testing is performance testing and that usability testing is
            usability testing; interestingly, performance testing is \emph{not}
            given as a sub-category of usability testing
            \citeyearpar[Tab.~2]{Gerrard2000}.
      \item End-to-end functionality testing is \emph{not} indicated to be
            functionality testing \citep[Tab.~2]{Gerrard2000}.
\end{enumerate}

\subsubsection{Functional Testing}

Throughout the literature, ``functional testing'' seems to be described in many
ways, alongside other, potentially related, terms:

\begin{itemize}
      \item \textbf{Specification-based Testing:} This is defined as ``testing
            in which the principal test basis is the external inputs and
            outputs of the test item'' \citep[p.~9]{IEEE2022}, which agrees
            with a definition of ``functional testing'': ``testing that
            \dots focuses solely on the outputs generated in response to
            selected inputs and execution conditions'' \citep[p.~196]{IEEE2017}.
            \todo{\citet[p.~399]{vanVliet2000} may list these as synonyms;
                  investigate}
            Notably, \citet{IEEE2017} lists both as synonyms of
            ``black-box testing'' (pp. 431, 196, respectively). However,
            they are sometimes defined as separate terms: ``specification-based
            testing'' as ``testing based on an analysis of the specification
            of the component or system'' (including ``black-box testing'' as a
            synonym) and ``functional testing'' as ``testing performed to
            evaluate if a component or system satisfies functional
            requirements'' (specifying no synonyms) \citepalias{ISTQB}. This
            definition of
            ``functional testing'' references \citet[p.~196]{IEEE2017}
            (``testing conducted to evaluate the compliance of a system or
            component with specified functional requirements'') which
            \emph{has} ``black-box testing'' as a synonym, and mirrors
            \citet[p.~21]{IEEE2022} (testing ``used to check the implementation
            of functional requirements''). Overall, specification-based testing
            \citep[pp.~2-4,~6-9,~22]{IEEE2022} and black-box testing
            (\citealp[p.~5-10]{SWEBOK2024}; \citealp[p.~3]{SouzaEtAl2017})
            are test design techniques used to ``derive corresponding test cases''
            \citep[p.~11]{IEEE2022} (from given ``selected inputs and execution
            conditions'' \citep[p.~196]{IEEE2017}).

      \item \textbf{Correctness Testing:} \citet[p.~5-7]{SWEBOK2024} says
            ``test cases can be designed to check that the functional
            specifications are correctly implemented, which is variously
            referred to in the literature as conformance testing, correctness
            testing or functional testing''; this mirrors previous definitions
            of ``functional testing'' (\citealp[p.~21]{IEEE2022};
            \citeyear[p.~196]{IEEE2017}) but groups it with ``correctness
            testing''. Since ``correctness'' is a software quality
            (\citealp[p.~104]{IEEE2017}; \citealp[p.~3-13]{SWEBOK2024}) which is
            what defines a ``test type'' \citep[p.~15]{IEEE2022}, it seems
            consistent to label ``functional testing'' as a ``test type''
            \citep[pp.~15,~20,~22]{IEEE2022}. This is listed separately from
            ``functionality testing'' by \citet[p.~53]{Firesmith2015}.

      \item \textbf{Conformance Testing:} As mentioned above,
            \citet[p.~5-7]{SWEBOK2024} says testing ``that the functional
            specifications are correctly implemented'' can be called
            ``conformance testing'' or ``functional testing''. The definition
            of ``conformance testing'' is later given as testing used ``to
            verify that the \acs{sut} conforms to standards, rules,
            specifications, requirements, design, processes, or practices''
            \citep[p.~5-7]{SWEBOK2024}. This definition seems to be a superset
            of the testing mentioned earlier; the former only lists
            ``specifications'' while the latter also includes ``standards'',
            ``rules'', ``requirements'', ``design'', ``processes'', and
            ``practices''!

      \item \textbf{Functional Suitability Testing:} Procedure testing is
            called a ``type of functional suitability testing''
            \citep[p.~7]{IEEE2022}, but no definition of ``functional
            suitability testing'' is given. ``Functional suitability'' is the
            ``capability of a product to provide functions that meet stated and
            implied needs of intended users when it is used under specified
            conditions'', including meeting ``the functional specification''
            \citep{ISO_IEC2023a}. This seems to align with the definition of
            ``functional testing'' as related to ``black-box/
            specification-based testing''. ``Functional suitability'' has
            three child terms: ``functional completeness'' (the ``capability of
            a product to provide a set of functions that covers all the
            specified tasks and intended users' objectives''), ``functional
            correctness'' (the ``capability of a product to provide accurate
            results when used by intended users''), and ``functional
            appropriateness'' (the ``capability of a product to provide
            functions that facilitate the accomplishment of specified tasks and
            objectives'') \citep{ISO_IEC2023a}. Notably, ``functional
            correctness'', which includes precision and accuracy
            (\citealp{ISO_IEC2023a}; \citetalias{ISTQB}), seems to align with
            the quality/ies that would be tested by ``correctness'' testing.

      \item \textbf{Functionality Testing:} ``Functionality'' is defined as the
            ``capabilities of the various \dots features provided by a product''
            \citep[p.~196]{IEEE2017} and is said to be a synonym of
            ``functional suitability'' \citepalias{ISTQB}, although it seems
            like it should really be its ``parent''. Its associated test type
            is implied to be a sub-approach of build verification testing
            \citepalias{ISTQB} and made distinct from ``functional testing'';
            interestingly, security is described as a sub-approach of both
            non-functional and functionality testing \citep[Tab.~2]{Gerrard2000}.
            This is listed separately from ``correctness testing'' by
            \citet[p.~53]{Firesmith2015}.
\end{itemize}

\subsubsection{Operational (Acceptance) Testing}
Some sources refer to ``operational acceptance testing'' (\citealp[p.~22]{IEEE2022};
\citetalias{ISTQB}) while some refer to ``operational testing''
(\citealp[p.~6-9,~in the context of software engineering operations]{SWEBOK2024};
\citealp{ISO_IEC2018}; \citealp[p.~303]{IEEE2017};
\citealp[pp.~4-6,~4-9]{SWEBOK2014}). A distinction is sometimes made
\citep[p.~30]{Firesmith2015} but without accompanying definitions, it is hard
to evaluate its merit. Since this terminology is not standardized, I
propose that the two terms are treated as synonyms (as done by other sources
\citep{LambdaTest2024, BocchinoAndHamilton1996}) as a type of
acceptance testing (\citealp[p.~22]{IEEE2022}; \citetalias{ISTQB}) that focuses on
``non-functional'' attributes of the system \citep{LambdaTest2024}
\todo{find more academic sources}.

A summary of given definitions of ``operational (acceptance) testing'' is that
it is ``test[ing] to determine the correct
installation, configuration and operation of a module and that it operates
securely in the operational environment'' \citep{ISO_IEC2018} or ``evaluate a
system or component in its operational environment'' \citep[p.~303]{IEEE2017},
particularly ``to determine if operations and/or systems administration staff
can accept [it]'' \citepalias{ISTQB}.

\subsubsection{Performance Testing}

``Performance testing'' is defined as testing ``conducted to
evaluate the degree to which a test item accomplishes its
designated functions within given constraints of time and other
resources'' \citep[p.~7]{IEEE2022}. It is listed as a subset of
``performance-related testing'', along with other approaches like
load and capacity testing (p.~22); \citet[p.~5-9]{SWEBOK2024}
gives ``capacity and response time'' as examples of ``performance
characteristics'' that performance testing would seek to ``assess''.
I see two possible resolutions to this:
\begin{enumerate}
      \item Assign the definition of ``performance testing'' to
            ``performance-related testing'' and give ``performance
            testing'' a more specific definition.
      \item Replace the term ``performance-related testing'' with
            ``performance testing''; this seems more logical, since
            I haven't found a definition of ``performance-related
            testing'' (at least yet) and most (if not all)
            definitions of ``performance testing'' seem to treat it
            as a category of tests.
\end{enumerate}
\label{perf-test-ambiguity}

Similarly, ``performance'' and ``performance efficiency'' are
both given as software qualities by \citeauthor{IEEE2017}, with the latter
defined as the ``performance relative to the amount
of resources used under stated conditions'' \citeyearpar[p.~319]{IEEE2017}
or the ``capability of a product to perform its functions within specified
time and throughput parameters and be efficient in the use of resources under
specified conditions'' \citep{ISO_IEC2023a}.
Initially, there didn't seem to be any meaningful distinction
between the two, although the term ``performance testing'' is defined
\citeyearpar[p.~320]{IEEE2017} and used by \citeauthor{IEEE2017} and
the term ``performance efficiency testing'' is \emph{also} used by
\citeauthor{IEEE2017} (but not defined explicitly). Further discussion (see
\thesisissueref{43}) brought us to the conclusion that ``performance
efficiency testing'' is a subset of ``performance testing'', and the
difference of ``relative to the amount of resources used'' or ``be efficient in
the use of resources'' between the two is meaningful.

\section{Definitions}

\begin{itemize}
      \item Software testing: ``the process of executing a program with the
            intent of finding errors'' \citep[p.~438]{PetersAndPedrycz2000}
            \todo{OG Myers 1976}. ``Testing can reveal
            failures, but the faults causing them are what can and must be
            removed'' \citep[p.~5-3]{SWEBOK2024}; it can also include
            certification, quality assurance, and quality improvement
            \citep[p.~5-4]{SWEBOK2024}. Involves ``specific preconditions
                  [and] \dots stimuli so that its actual behavior can be
            compared with its expected or required behavior'', including
            control flows, data flows, and postconditions
            \citep[p.~11]{Firesmith2015}
      \item Test case: ``the specification of all the entities
            that are essential for the execution, such as input values,
            execution and timing conditions, testing procedure, and the
            expected outcomes'' \citep[pp.~5-1 to 5-2]{SWEBOK2024}
      \item Defect: ``an observable difference between what the software is
            intended to do and what it does'' \citep[p.~1-1]{SWEBOK2024}; ``can
            be used to refer to either a fault or a failure, [sic] when the
            distinction is not important'' \citep[p.~4-3]{SWEBOK2014}
            \todo{OG?}
      \item Error: ``a human action that produces an incorrect result''
            \citep[p.~399]{vanVliet2000}
      \item Fault: ``the manifestation of an error'' in the software itself
            \citep[p.~400]{vanVliet2000}; ``the \emph{cause} of a malfunction''
            \citep[p.~5-3]{SWEBOK2024}
      \item Failure: incorrect output or behaviour resulting from encountering
            a fault; can be defined as not meeting specifications or
            expectations and ``is a relative notion''
            \citep[p.~400]{vanVliet2000}; ``an undesired effect observed in the
            system's delivered service'' \citep[p.~5-3]{SWEBOK2024}
      \item Verification: ``the process of evaluating a system or component
            to determine whether the products of a given development phase
            satisfy the conditions imposed at the start of that phase''
            \citep[p.~400]{vanVliet2000}
      \item Validation: ``the process of evaluating a system or component
            during or at the end of the development process to determine
            whether it satisfies specified requirements''
            \citep[p.~400]{vanVliet2000}
      \item Test Suite Reduction: the process of reducing the size of a test
            suite while maintaining the same coverage
            \citep[p.~519]{BarrEtAl2015}; can be accomplished through
            \nameref{chap:testing:sec:mutation-testing}
      \item Test Case Reduction: the process of ``removing side-effect free
            functions'' from an individual test case to ``reduc[e] test oracle
            costs'' \citep[p.~519]{BarrEtAl2015}
      \item Probe: ``a statement inserted into a program'' for the purpose of
            dynamic testing \citep[p.~438]{PetersAndPedrycz2000}
\end{itemize}

\subsection{Documentation}

\begin{itemize}
      \item \acf{vnv} Plan: a document for the ``planning of test activities''
            described by IEEE Standard 1012 \citep[p.~411]{vanVliet2000}
      \item Test Plan: ``a document describing the scope, approach, resources,
            and schedule of intended test activities'' in more detail that the
            \acs{vnv} Plan \citep[pp.~412-413]{vanVliet2000};
            should also outline entry and exit conditions for the testing
            activities as well as any risk sources and levels
            \citep[p.~445]{PetersAndPedrycz2000}
      \item Test Design documentation: ``specifies \dots the details of the
            test approach and identifies the associated tests''
            \citep[p.~413]{vanVliet2000}
      \item Test Case documentation: ``specifies inputs, predicted outputs and
            execution conditions for each test item''
            \citep[p.~413]{vanVliet2000}
      \item Test Procedure documentation: ``specifies the sequence of actions
            for the execution of each test'' \citep[p.~413]{vanVliet2000}
      \item Test Report documentation: ``provides information on the results of
            testing tasks'', addressing software verification and validation
            reporting \citep[p.~413]{vanVliet2000}
\end{itemize}

\section{General Testing Notes}

\begin{itemize}
      \item The scope of testing is very dependent on what type of software
            is being tested, as this informs what information/artifacts are
            available, which approaches are relevant, and which tacit knowledge
            is present (see \thesisissueref{54}). For example, a method table
            is a tool for tracking the ``test approaches, testing techniques
            and test types that are required depending \dots on the context of
            the test object'' \citepalias{ISTQB} \todo{OG ISO 26262}, although
            this is more specific to the automotive domain
      \item ``Proving the correctness of software \dots applies only in
            circumstances where software requirements are stated formally'' and
            assumes ``these formal requirements are themselves correct''
            \citep[p.~398]{vanVliet2000}
      \item If faults exist in programs, they ``must be considered faulty, even
            if we cannot devise test cases that reveal the faults''
            \citep[p.~401]{vanVliet2000}
      \item Black-box test cases should be created based on the specification
            \emph{before} creating white-box test cases to avoid being ``biased
            into creating test cases based on how the module works''
            \citep[p.~113]{Patton2006}
      \item Simple, normal test cases (test-to-pass) should always be developed
            and run before more complicated, unusual test cases (test-to-fail)
            \citep[p.~66]{Patton2006}
      \item Since ``there is no uniform best test technique'', it is advised to
            use many techniques when testing \citep[p.~440]{vanVliet2000}.
            This supports the principle of \emph{independence of testing}: the
            ``separation of responsibilities, which encourages the
            accomplishment of objective testing'' \citepalias{ISTQB}
      \item When comparing adequacy criteria, ``criterion X is stronger than
            criterion Y if, for all programs P and all test sets T, X-adequacy
            implies Y-adequacy'' (the ``stronger than'' relation is also called
            the ``subsumes'' relation) \citep[p.~432]{vanVliet2000};
            this relation only ``compares the thoroughness of test techniques,
            not their ability to detect faults'' \citep[p.~434]{vanVliet2000}
            \todo{This should probably be explained after ``test adequacy
                  criterion'' is defined}
\end{itemize}

\subsection[Steps to Testing]{Steps to Testing
      \citep[p.~443]{PetersAndPedrycz2000}}
\begin{enumerate}
      \item Identify the goal(s) of the test
      \item Decide on an approach
      \item Develop the tests
      \item Determine the expected results
      \item Run the tests
      \item Compare the expected results to the actual results
\end{enumerate}

\subsection{Testing Stages}
\begin{itemize}
      \item Unit testing: ``testing the individual modules [of a program]''
            \citep[p.~438]{vanVliet2000}; also called ``module testing''
            \citep[p.~109]{Patton2006} or ``component testing''
            \citep[p.~444]{PetersAndPedrycz2000}, although
            \citet[p.~107]{BaresiAndPezz√®2006} say ``components differ from
            classical modules for being re-used in different contexts
            independently of their development.'' Note that since a
            \emph{component} is ``a part of a system that can be tested in
            isolation'' \citepalias{ISTQB}, this seems like it could apply
            to the testing of both modules \emph{and} specific functions
      \item Integration testing: ``testing the composition of modules'';
            done incrementally using \emph{bottom-up} and/or
            \emph{top-down} testing \citep[pp.~438-439]{vanVliet2000},
            although other paradigms for design, such as \emph{big bang} and
            \emph{sandwich} exist \citep[p.~489]{PetersAndPedrycz2000}.
            See also \citep[p.~109]{Patton2006}.
            \begin{itemize}
                  \item Bottom-up testing: uses \emph{test drivers}: ``tool[s]
                        that generate[] the test environment for a component to
                        be tested'' \citep[p.~410]{vanVliet2000} by
                        ``sending test-case data to the modules under test,
                        read[ing] back the results, and verify[ing] that
                        they're correct'' \citep[p.~109]{Patton2006}
                  \item Top-down testing: uses \emph{test stubs}: tools that
                        ``simulate[] the function of a component not yet
                        available'' \citep[p.~410]{vanVliet2000} by
                        providing ``fake'' values to a given module to be
                        tested \citep[p.~110]{Patton2006}
                  \item Big bang testing: the process of ``integrat[ing] all
                        modules in a single step and test[ing] the resulting
                        system[]'' \citep[p.~489]{PetersAndPedrycz2000}.
                        \emph{Although this is ``quite challenging and risky''
                              \citep[p.~489]{PetersAndPedrycz2000}, it may be
                              made less so through the ease of generation,
                              and may be more practical as a testing process
                              for Drasil, although the introduction of the
                              test cases themselves may be introduced, at least
                              initially, in a more structured manner; also of
                              note is its relative ease ``to test paths'' and
                              ``to plan and control''
                              \citep[p.~490]{PetersAndPedrycz2000}
                              \qtodo{Bring up!}}
                  \item Sandwich testing: ``combines the ideas of bottom-up and
                        top-down testing by defining a certain target layer in
                        the hierarchy of the modules'' and working towards it
                        from either end using the relevant testing approach
                        \citep[p.~491]{PetersAndPedrycz2000}
            \end{itemize}
      \item System testing: ``test[ing] the whole system against the user
            documentation and requirements specification after integration
            testing has finished'' \citep[p.~439]{vanVliet2000}
            (\citep[p.~109]{Patton2006} says this can also be done on
            ``at least a major portion'' of the product); often uses random,
            but representative, input to test reliability
            \todo{Expand on reliability testing (make own section?)}
            \citep[p.~439]{vanVliet2000}
      \item Acceptance testing: Similar to system testing that is ``often
            performed under supervision of the user organization'',
            focusing on usability \citep[p.~439]{vanVliet2000} and
            the needs of the customer(s) \citep[p.~492]{PetersAndPedrycz2000}
      \item Installation testing: Focuses on the portability of the product,
            especially ``in an environment different from the one in which is
            has been developed'' \citep[p.~439]{vanVliet2000}; not
            one of the four levels of testing identified by the IEEE standard
            \citep[p.~445]{PetersAndPedrycz2000}
\end{itemize}

\subsection{Test Oracles}
A test oracle is a ``source of information for determining whether a test has
passed or failed'' \citep[p.~13]{IEEE2022} or that ``the \acs{sut} behaved
correctly \dots and according to the expected outcomes'' and can be ``human or
mechanical'' \citep[p.~5-5]{SWEBOK2024}. Oracles provide either ``a
`pass' or `fail' verdict''; otherwise, ``the test output is classified as
inconclusive'' \citep[p.~5-5]{SWEBOK2024}. This process can be ``deterministic''
(returning a Boolean value) or ``probabilistic'' (returning ``a real number in
the closed interval $[0, 1]$'') \citep[p.~509]{BarrEtAl2015}. Probabilistic
test oracles can be used to reduce the computation cost (since test oracles
are ``typically computationally expensive'') \citep[p.~509]{BarrEtAl2015}
or in ``situations where some degree of imprecision can be tolerated'' since
they ``offer a probability that [a given] test case is acceptable''
\citep[p.~510]{BarrEtAl2015}. SWEBOK V4 lists ``unambiguous requirements
specifications, behavioral models, and code annotations'' as examples
\citep[p.~5-5]{SWEBOK2024}, and \citeauthor{BarrEtAl2015} provides four
categories \citeyearpar[p.~510]{BarrEtAl2015}:

\begin{itemize}
      \item Specified test oracle: ``judge[s] all behavioural aspects of a
            system with respect to a given formal specification''
            \citep[p.~510]{BarrEtAl2015}
      \item Derived test oracle: any ``artefact[] from which a
            test oracle may be derived---for instance, a previous version of
            the system'' or ``program documentation''; this includes
            \nameref{chap:testing:sec:regression-testing},
            \nameref{chap:testing:sec:metamorphic-testing}
            \citep[p.~510]{BarrEtAl2015}, and invariant detection (either
            known in advance or ``learned from the program'')
            \citep[p.~516]{BarrEtAl2015}
            \begin{itemize}
                  \item This seems to prove ``relative correctness'' as
                        opposed to ``absolute correctness''
                        \citep[p.~345]{LahiriEtAl2013} since this derived
                        oracle may be wrong!
                  \item ``Two versions can be checked for semantic equivalence
                        to ensure the correctness of [a] transformation'' in a
                        process that can be done ``incrementally''
                        \citep[p.~345]{LahiriEtAl2013}
                  \item Note that the term ``invariant'' may be used in
                        different ways (see \citep[p.~348]{ChalinEtAl2006})
            \end{itemize}
      \item Pseudo-oracle: a type of derived test oracle that is ``an
            alternative version of the program produced independently'' (by a
            different team, in a different language, etc.)
            \citep[p.~515]{BarrEtAl2015} \todo{see ISO 29119-11}.
            \emph{We could potentially use the
                  programs generated in other languages as pseudo-oracles!}
      \item Implicit test oracles: detect ```obvious' faults such as a program
            crash'' (potentially due to a null pointer, deadlock, memory leak,
            etc.) \citep[p.~510]{BarrEtAl2015}
      \item ``Lack of an automated test oracle'': for example; a human oracle
            generating sample data that is ``realistic'' and ``valid'',
            \citep[pp.~510-511]{BarrEtAl2015}, crowdsourcing
            \citep[p.~520]{BarrEtAl2015}, or a ``Wideband Delphi'': ``an
            expert-based test estimation technique that ‚Ä¶ uses the collective
            wisdom of the team members'' \citepalias{ISTQB}
\end{itemize}

\subsection{Generating Test Cases}

\begin{itemize}
      \item ``A \textbf{test adequacy criterion} \dots specifies requirements
            for testing \dots and can be used \dots as a test case generator....
                  [For example, i]f a 100\% statement coverage has not been achieved
            yet, an additional test case is selected that covers one or more
            statements yet untested'' \citep[p.~402]{vanVliet2000}
      \item ``Test data generators'' are mentioned on
            \citep[p.~410]{vanVliet2000} but not described
            \todo{Investigate}
      \item ``Dynamic test generation consists of running a program while
            simultaneously executing the program symbolically in order to
            gather constrains on inputs from conditional statements encountered
            along the execution \citep[p.~23]{GodefroidAndLuchaup2011}
            \todo{OG [11, 6]}
      \item ``Generating tests to detect [loop inefficiencies]'' is difficult
            due to ``virtual call resolution'', reachability conditions, and
            order-sensitivity \citep[p.~896]{DhokAndRamanathan2016}
      \item Assertion checking requires ``auxiliary invariants'', and while
            ``many \dots can be synthesized automatically by invariant
            generation methods, the undecidable nature (or the high practical
            complexity) of assertion checking precludes complete automation for
            a general class of user-supplied assertions''
            \citep[p.~345]{LahiriEtAl2013}
            \begin{itemize}
                  \item \acf{dac} can be supported by ``automatic invariant
                        generation'' \citep[p.~345]{LahiriEtAl2013}
            \end{itemize}
      \item COBRA is a tool that ``generates test cases automatically and
            applies them to the simulated industrial control system in a SiL
            Test'' \citep[p.~2]{Preu√üeEtAl2012}
      \item Test case generation is useful for instances where one kind of
            testing is difficult, but can be generated from a different,
            simpler kind (e.g., asynchronous testing from synchronous testing
            \citep{JardEtAl1999})
\end{itemize}

\section[Static Black-Box (Specification) Testing]{Static Black-Box
  (Specification) Testing \citep[pp.~56-62]{Patton2006}}

Most of this section is irrelevant to generating test cases, as they require
human involvement \todo{Describe anyway} (e.g., Pretend to Be the Customer
\citep[pp.~57-58]{Patton2006}, Research Existing Standards and
Guidelines \citep[pp.~58-59]{Patton2006}). However, it provides a
``Specification Terminology Checklist'' \citep[p.~61]{Patton2006} that
includes some keywords that, if found, could trigger an applicable warning to
the user (similar to the idea behind the correctness/consistency checks
project). In general, each requirement should be unambiguous, testable,
binding, and ``acceptable to all stakeholders'', and the ``overall collection''
should be complete, consistent, and feasible \citep[p.~1-8]{SWEBOK2024}:

\begin{itemize}
      \item \textbf{Potentially unrealistic:} always, every, all, none, every,
            certainly, therefore, clearly, obviously, evidently
      \item \textbf{Potentially vague:} some, sometimes, often, usually,
            ordinarily, customarily, most, mostly, good, high-quality, fast,
            quickly, cheap, inexpensive, efficient, small, stable
      \item \textbf{Potentially incomplete:} etc., and so forth, and so on,
            such as, handled, processed, rejected, skipped, eliminated,
            if \dots then \dots (without ``else'' or ``otherwise''),
            to be determined \citep[p.~408]{vanVliet2000}
\end{itemize}

\subsection[Coverage-Based Testing of Specification]{Coverage-Based Testing of
      Specification \citep[pp.~425-426]{vanVliet2000}}

Requirements can be ``depicted as a graph, where the nodes denote elementary
requirements and the edges denote relations between [them]'' from which test
cases can be derived \citep[p.~425]{vanVliet2000}. However, it can
be difficult to assess whether a set of equivalence classes are truly
equivalent, since the specific data available in each node is not apparent
\citep[p.~426]{vanVliet2000}.

\section[Dynamic Black-Box (Behavioural) Testing]{Dynamic Black-Box
  (Behavioural) Testing \citep[pp.~64-65]{Patton2006}}

This is the process of ``entering inputs, receiving outputs, and checking the
results'' \citep[p.~64]{Patton2006}. \citep[p.~399]{vanVliet2000}
also calls this ``functional testing''.

\subsubsection{Requirements}
\begin{itemize}
      \item Requirements documentation (definition of what the software does)
            \citep[p.~64]{Patton2006}; relevant information could be:
            \begin{itemize}
                  \item Requirements: Input-Values and Output-Values
                  \item Input/output data constraints
            \end{itemize}
\end{itemize}

\subsection[Exploratory Testing]{Exploratory Testing \citep[p.~65]{Patton2006}}

An alternative to dynamic black-box testing when a specification is not
available \citep[p.~65]{Patton2006}. The software is explored to
determine its features, and these features are then tested
\citep[p.~65]{Patton2006}. Finding any bugs using this method is a
positive thing \citep[p.~65]{Patton2006}, since despite not knowing
what the software \emph{should} do, you were able to determine that something
is wrong.

This is not applicable to Drasil, because not only does it already generate a
specification, making this type of testing unnecessary, there is also a lot of
human-based trial and error required for this kind of testing
\citep{june_11_meeting}.

\subsection[Equivalence Partitioning/Classing]{Equivalence Partitioning/Classing
      \citep[pp.~67-69]{Patton2006}}

The process of dividing the infinite set of test cases into a finite set that is
just as effective (i.e., that reveals the same bugs) \citep[p.~67]{Patton2006}.
The opposite of this, testing every combination of inputs, is called
``exhaustive testing'' and is ``probably not feasible'' \exhInfCite{}.

\subsubsection{Requirements}
\begin{itemize}
      \item Ranges of possible values \citep[p.~67]{Patton2006};
            could be obtained through:
            \begin{itemize}
                  \item Input/output data constraints
                  \item Case statements
            \end{itemize}
\end{itemize}

\subsection[Data Testing]{Data Testing \citep[pp.~70-79]{Patton2006}}

The process of ``checking that information the user inputs [and] results'',
both final and intermediate, ``are handled correctly''
\citep[p.~70]{Patton2006}. This type of testing can also occur at the
white-box level, such as the implementation of boundaries
\citep[p.~431]{vanVliet2000} or intermediate values within
components.

\subsubsection[Boundary Conditions]{Boundary Conditions
      \citep[pp.~70-74]{Patton2006}}

``[S]ituations at the edge of the planned operational limits of the software''
\citep[p.~72]{Patton2006}. Often affects types of data (e.g., numeric,
speed, character, location, position, size, quantity
\citep[p.~72]{Patton2006}) each with its own set of (e.g., first/last,
min/max, start/finish, over/under, empty/full, shortest/longest,
slowest/fastest, soonest/latest, largest/smallest, highest/lowest,
next-to/farthest-from \citep[pp.~72-73]{Patton2006}). Data at these
boundaries should be included in an equivalence partition, but so should
data in between them \citep[p.~73]{Patton2006}. Boundary conditions
should be tested using ``the valid data just inside the boundary,
\dots the last possible valid data, and \dots the invalid data just outside the
boundary'' \citep[p.~73]{Patton2006}, and values at the boundaries
themselves should still be tested even if they occur ``with zero probability'',
in case there actually \emph{is} a case where it can occur; this process of
testing may reveal it \citep[p.~460]{PetersAndPedrycz2000}.

\paragraph{Requirements}
\begin{itemize}
      \item Ranges of possible values \citep[p.~67,~73]{Patton2006};
            could be obtained through:
            \begin{itemize}
                  \item Case statements
                  \item Input/output data constraints (e.g., inputs that
                        would lead to a boundary output)
            \end{itemize}
\end{itemize}

\paragraph{Buffer Overruns \citep[pp.~201-205]{Patton2006}}

\emph{Buffer overruns} are ``the number one cause of software security issues''
\citep[p.~75]{Patton2006}. They occur when the size of the destination
for some data is smaller than the data itself, causing existing data (including
code) to be overwritten and malicious code to potentially be injected
\citep[p.~202,~204-205]{Patton2006}. They often arise from bad
programming practices in ``languages [sic] such as C and C++, that lack safe
string handling functions'' \citep[p.~201]{Patton2006}. Any unsafe
versions of these functions that are used should be replaced with the
corresponding safe versions \citep[pp.~203-204]{Patton2006}.

\subsubsection[Sub-Boundary Conditions]{Sub-Boundary Conditions
      \citep[pp.~75-77]{Patton2006}}
\label{sub-bound-conds}

Boundary conditions ``that are internal to the software [but] aren't necessarily
apparent to an end user'' \citep[p.~75]{Patton2006}. These include
powers of two \citep[pp.~75-76]{Patton2006} and ASCII and Unicode tables
\citep[pp.~76-77]{Patton2006}.

While this is of interest to the domain of scientific computing, this is too
involved for Drasil right now, and the existing software constraints limit much
of the potential errors from over/underflow \citep{june_11_meeting}. Additionally,
strings are not really used as inputs to Drasil and only occur in output with
predefined values, so testing these values are unlikely to be fruitful.

There also exist sub-boundary conditions that arise from ``complex''
requirements, where behaviour depends on multiple conditions
\citep[p.~430]{vanVliet2000}. These ``error prone'' points around
these boundaries should be tested \citep[p.~430]{vanVliet2000} as
before: ``the valid data just inside the boundary, \dots the last possible
valid data, and \dots the invalid data just outside the boundary''
\citep[p.~73]{Patton2006}. In this type of testing, the second type of
data is called an ``ON point'', the first type is an ``OFF point'' for the
domain on the \emph{other} side of the boundary, and the third type is an ``OFF
point'' for the domain on the \emph{same} side of the boundary
\citep[p.~430]{vanVliet2000}.

\paragraph{Requirements}
\begin{itemize}
      \item Increased knowledge of data type structures (e.g., monoids, rings,
            etc. \citep{june_11_meeting}); this would capture these sub-boundaries,
            as well as other information like relevant tests cases, along with
            our notion of these data types (\texttt{Space})
\end{itemize}

\subsubsection[Default, Empty, Blank, Null, Zero, and None]{Default, Empty,
      Blank, Null, Zero, and None \citep[pp.~77-78]{Patton2006}}

These should be their own equivalence class, since ``the software usually
handles them differently'' than ``the valid cases or \dots invalid cases''
\citep[p.~78]{Patton2006}.

Since these values may not always be applicable to a given scenario (e.g., a
test case for zero doesn't make sense if there is a constraint that the value
in question cannot be zero), the user should likely be able to select
categories of tests to generate instead of Drasil just generating all possible
test cases based on the inputs \citep{june_11_meeting}.

\paragraph{Requirements}
\begin{itemize}
      \item Knowledge of an ``empty'' value for each \texttt{Space} (stored
            alongside each type in \texttt{Space}?)
      \item Knowledge of how input data could be omitted from an input
            (e.g., a missing command line argument, an empty line in a file);
            could be obtained from:
            \begin{itemize}
                  \item User responsibilities
            \end{itemize}
      \item Knowledge of how a programming language deals with \texttt{Null}
            values and how these can be passed as arguments
\end{itemize}

\subsubsection[Invalid, Wrong, Incorrect, and Garbage Data]{Invalid, Wrong,
      Incorrect, and Garbage Data \citep[pp.~78-79]{Patton2006}}

This is testing-to-fail \citep[p.~77]{Patton2006}.

\paragraph{Requirements}
This seems to be the most open-ended category of testing.
\begin{itemize}
      \item Specification of correct inputs that can be ignored;
            could be obtained through:
            \begin{itemize}
                  \item Input/output data constraints (e.g., inputs that would
                        lead to a violated output constraint)
                  \item Type information for each input (e.g., passing a string
                        instead of a number)
            \end{itemize}
\end{itemize}

\subsubsection[Syntax-Driven Testing]{Syntax-Driven Testing
      \citep[pp.~448-449]{PetersAndPedrycz2000}}

If the inputs to the system ``are described by a certain grammar''
\citep[p.~448]{PetersAndPedrycz2000}, ``test cases \dots [can] be designed
according to the syntax or constraint of input domains defined in requirement
specification'' \citep[p.~260]{IntanaEtAl2020}
\todo{Investigate this source more!}.

\subsubsection[Decision Table-Based Testing]{Decision Table-Based Testing
      \citep[pp.~448,~450-453]{PetersAndPedrycz2000}}

``When the original software requirements have been formulated in the format of
`if-then' statements,'' a decision table can be created with a column for each
test situation \citep[p.~448]{PetersAndPedrycz2000}. ``The upper part of the
column contains conditions that must be satisfied. The lower portion of a
decision table specifies the action that results from the satisfaction of
conditions in a rule'' (from the specification) \citep[p.~450]{PetersAndPedrycz2000}.

\subsection[State Testing]{State Testing \citep[pp.~79-87]{Patton2006}}

The process of testing ``the program's logic flow through its various states''
\citep[p.~79]{Patton2006} by checking that state variables are
correct after different transitions \citetext{p.~83}. This is usually done by
creating a state transition diagram that includes:

\begin{itemize}
      \item Every possible unique state
      \item The condition(s) that take(s) the program between states
      \item The condition(s) and output(s) when a state is entered or exited
\end{itemize}

to map out the logic flow from the user's perspective
\citep[pp.~81-82]{Patton2006}. Next, these states should be
partitioned using one (or more) of the following methods:

\begin{enumerate}
      \item Test each state once
      \item Test the most common state transitions
      \item Test the least common state transitions
      \item Test all error states and error return transitions
      \item Test random state transitions
            \citep[pp.~82-83]{Patton2006}
\end{enumerate}

For all of these tests, the values of the state variables should be verified
\citep[p.~83]{Patton2006}.

\paragraph{Requirements}
\begin{itemize}
      \item Knowledge of the different states of the program
            \citep[p.~82]{Patton2006}; could be obtained through:
            \begin{itemize}
                  \item The program's modules and/or functions
                  \item The program's exceptions
            \end{itemize}
      \item Knowledge about the different state transitions
            \citep[p.~82]{Patton2006}; could be obtained through:
            \begin{itemize}
                  \item Testing the state transitions near the beginning of a
                        workflow more?
            \end{itemize}
\end{itemize}

\subsubsection{Performance Testing}

Testing to determine how efficiently software uses resources (including time
and capacity) ``when accomplishing its designated functions''
\citepalias{ISTQB}. \todo{OG ISO 25010?}

% ``The intent of this type of testing is to identify weak points of a software
% system and quantify its shortcomings'' \citep[p.~447]{PetersAndPedrycz2000}.

\todo{Originally used a \emph{very} vague definition from
      \citep[p.~447]{PetersAndPedrycz2000}; re-investigate!}

\subsubsection[Testing States to Fail]{Testing States to Fail
      \citep[pp.~84-87]{Patton2006}}

The goal here is to try and put the program in a fail state by doing things
that are out of the ordinary. These include:

\begin{itemize}
      \item Race Conditions and Bad Timing \citep[pp.~85-86]{Patton2006}
            (Is this relevant to our examples?)
      \item Repetition Testing: ``doing the same operation over and over'',
            potentially up to ``thousands of attempts''
            \citep[p.~86]{Patton2006}
      \item Stress Testing: ``running the software under less-than-ideal
            conditions'' to see how it functions \citep[p.~86]{Patton2006}
      \item Load testing: running the software with as large of a load as
            possible (e.g., large inputs, many peripherals)
            \citep[p.~86]{Patton2006}
\end{itemize}

\paragraph{Requirements}
\begin{itemize}
      \item Repetition Testing: The types of operations that are likely to lead
            to errors when repeated (e.g., overwriting files?)
      \item Stress testing: can these be automated with pytest or are they
            outside our scope? \todo{Investigate}
      \item Load testing: Knowledge about the types of inputs that could
            overload the system (e.g., upper bounds on values of certain types)
\end{itemize}

\subsection[Other Black-Box Testing]{Other Black-Box Testing
      \citep[pp.~87-89]{Patton2006}}
\begin{itemize}
      \item Act like an inexperienced user (\emph{likely out of scope})
      \item Look for bugs where they've already been found (\emph{keep track of
                  previous failed test cases? This could pair well with
                  \nameref{chap:testing:sec:metamorphic-testing}!})
      \item Think like a hacker (\emph{likely out of scope})
      \item Follow experience (\emph{implicitly done by using Drasil})
\end{itemize}

\section[Static White-Box Testing (Structural Analysis)]{Static White-Box
  Testing (Structural Analysis) \citep[pp.~91-104]{Patton2006}}

White-box testing is also called ``glass box testing''
\citep[p.~439]{PetersAndPedrycz2000}. \citep[p.~447]{PetersAndPedrycz2000} claims
that ``structural testing subsumes white box testing'', but I am unsure if this
is a meaningful statement; they seem to describe the same thing to me, \qtodo{Is this true?}
especially since it says ``structure tests are aimed at exercising the internal
logic of a software system'' and ``in white box testing \dots, using detailed
knowledge of code, one creates a battery of tests in such a way that they
exercise all components of the code (say, statements, branches, paths)'' on the
same page!

There are also some more specific
categories of this, such as Scenario-Based
Evaluation \citep[pp.~417-418]{vanVliet2000} and Stepwise Abstraction
\citep[pp.~419-420]{vanVliet2000}, that could be investigated further.
\todo{Do this!}

\begin{itemize}
      \item ``The process of carefully and methodically reviewing the software
            design, architecture, or code for bugs without executing it''
            \citep[p.~92]{Patton2006}
      \item Less common than black-box testing, but often used for ``military,
            financial, factory automation, or medical software, \dots in a
            highly disciplined development model'' or when ``testing software
            for security issues'' \citep[p.~91]{Patton2006}; often
            avoided because of ``the misconception that it's too
            time-consuming, too costly, or not productive''
            \citep[p.~92]{Patton2006}
      \item Especially effective early on in the development process
            \citep[p.~92]{Patton2006}
      \item Can ``find bugs that would be difficult to uncover or isolate with
            dynamic black-box testing'' and ``gives the team's black-box
            testers ideas for test cases to apply''
            \citep[p.~92]{Patton2006}
      \item Largely ``done by the language compiler'' or by separate tools
            \citep[pp.~413-414]{vanVliet2000}
\end{itemize}

\subsection[Reviews]{Reviews \citep[pp.~92-95]{Patton2006},
      \citep[pp.~415-417]{vanVliet2000},
      \citep[pp.~482-485]{PetersAndPedrycz2000}}
\label{reviews}

\begin{itemize}
      \item ``The process under which static white-box testing is performed''
            \citep[p.~92]{Patton2006}; consists of four main parts:

            \begin{enumerate}
                  \item Identify Problems: Find what is wrong or missing
                  \item Follow Rules: There should be a structure to the review,
                        such as ``the amount of code to be reviewed \dots, how
                        much time will be spent \dots, what can be commented on,
                        and so on'', to set expectations; ``if a process is run
                        in an ad-hoc fashion, bugs will be missed and the
                        participants will likely feel that the effort was a
                        waste of time''
                  \item Prepare: Based on the participants' roles, they should
                        know what they will be contributing during the actual
                        review; ``most of the problems found through the review
                        process are found during preparation''
                  \item Write a Report: A summary should be created and provided
                        to the rest of the development team so that they know
                        what problems exist, where they are, etc.
                        \citep[p.~93]{Patton2006}
            \end{enumerate}

      \item Reviews improve communication, learning, and camaraderie, as well as
            the quality of code \emph{even before the review}: if a developer
            ``knows that his work is being carefully reviewed by his peers, he
            might make an extra effort to \dots make sure that it's right''
            \citep[pp.~93-94]{Patton2006}

      \item Many forms:
            \begin{itemize}
                  \item Peer Review: Also called ``buddy review''
                        \citep[p.~94]{Patton2006}. The most informal
                        review at the smallest scale \citep[p.~94]{Patton2006}.
                        One variation is where a group of two or three people
                        go through code that one of them wrote
                        \citep[p.~94]{Patton2006}. Another is to
                        have each person in a larger group submit ``a `best'
                        program and one of lesser quality'', randomly distribute
                        all programs to be assessed by two people in the group,
                        and return all feedback anonymously to the appropriate
                        developer \citep[p.~414]{vanVliet2000}
                  \item Walkthrough: The author of the code presents it line
                        by line to a small group that ``question anything that
                        looks suspicious'' \citep[p.~95]{Patton2006};
                        this is done by using test data to ``walk through''
                        the execution of the program
                        \citep[p.~416]{vanVliet2000}. A more
                        structured walkthrough may have specific roles
                        (presenter, coordinator, secretary, maintenance oracle,
                        standards bearer, and user representative)
                        \citep[p.~484]{PetersAndPedrycz2000}
                  \item Inspection: Someone who is \emph{not} the author of the
                        code presents it to a small group of people
                        \citep[p.~95]{Patton2006}; the author
                        should be ``a largely silent observer'' who
                        ``may be consulted by the inspectors''
                        \citep[p.~415]{vanVliet2000}. Each member has
                        a role, which may be tied to a different perspective
                        (e.g., designer, implementer, tester,
                        \citep[p.~439]{PetersAndPedrycz2000} user, or product
                        support person) \citep[p.~95]{Patton2006}.
                        Changes are made based on issues identified \emph{after}
                        the inspection \citep[p.~415]{vanVliet2000},
                        and a reinspection may take place
                        \citep[p.~95]{Patton2006}; one guideline is to
                        reinspect \emph{100\%} of the code ``[i]f more than 5\%
                        of the material inspected has been reworked''
                        \citep[p.~483]{PetersAndPedrycz2000}.
            \end{itemize}

      \item Can use various tools (see \nameref{code-stds-and-guidelines} and
            \nameref{gen-code-review-checklist})

      \item \emph{Could be used to evaluate Drasil and/or generated code, but
                  couldn't be automated due to the human element}

\end{itemize}

\subsection[Coding Standards and Guidelines]{Coding Standards and Guidelines
      \citep[pp.~96-99]{Patton2006}}
\label{code-stds-and-guidelines}
\todo{This shouldn't really be at the same level as \nameref{reviews}, but I
      didn't want to fight with more subsections yet}

\begin{itemize}
      \item Code may work but still be incorrect if it doesn't meet certain
            criteria, since these affect its reliability, readability,
            maintainability, and/or portability; e.g., the \texttt{goto},
            \texttt{while}, and \texttt{if-else} commands in C can cause bugs
            if used incorrectly \citep[p.~96]{Patton2006}
      \item These guidelines can range in strictness and formality, as long as
            they are agreed upon and followed \citep[p.~96]{Patton2006}
      \item This could be checked using linters
\end{itemize}

\subsection[Generic Code Review Checklist]{Generic Code Review Checklist
      \citep[pp.~99-103]{Patton2006}}
\label{gen-code-review-checklist}
\todo{This shouldn't really be at the same level as \nameref{reviews}, but I
      didn't want to fight with more subsections yet}

\begin{itemize}
      \item \phantomsection
            \label{data-ref-errors}
            Data reference errors: ``bugs caused by using a variable, constant,
            \dots [etc.] that hasn't been properly declared or initialized''
            for its context \citep[p.~99]{Patton2006}
      \item Data declaration errors: bugs ``caused by improperly declaring
            or using variables or constants'' \citep[p.~100]{Patton2006}
      \item Computation errors: ``essentially bad math''; e.g., type mismatches,
            over/underflow, zero division, out of meaningful range
            \citep[p.~101]{Patton2006}
            \label{comp-errors}
      \item Comparison errors: ``very susceptible to boundary condition
            problems''; e.g., correct inclusion, floating point comparisons
            \citep[p.~101]{Patton2006}
      \item Control flow errors: bugs caused by ``loops and other control
            constructs in the language not behaving as expected''
            \citep[p.~102]{Patton2006}
      \item Subroutine parameter errors: bugs ``due to incorrect passing of data
            to and from software subroutines'' \citep[p.~102]{Patton2006}
            (could also be called ``interface errors''
            \citep[p.~416]{vanVliet2000})
      \item Input/output errors: e.g., how are errors handled?
            \citep[pp.~102-103]{Patton2006}
      \item ASCII character handling, portability, compilation warnings
            \citep[p.~103]{Patton2006}
\end{itemize}

\subsubsection{Requirements}
\begin{itemize}
      \item Data reference errors: know what operations are allowed for each
            type and check that values are only used for those operations
      \item Data declaration errors: I think this will mainly be covered by
            checking for data reference errors and by our generator (e.g., no
            typos in type names)
      \item Computation errors: partially tested dynamically by system tests,
            but could also more formally check for things like type mismatches
            (does \acs{gool} do this already?) or if divisors can ever be zero
      \item Comparison errors: I think this would mainly have to be done
            manually (maybe except for checking for (in)equality between values
            where it can never occur), but we may be able to generate a summary
            of all comparisons for manual verification
      \item Control flow errors: mostly irrelevant since we don't implement
            loops yet; would this include system tests?
      \item Subroutine parameter errors: we could check the types of values
            returned by a subroutine with the expected type (at least for
            languages like Python)
      \item Input/output errors: knowledge of (and more formal specification of)
            requirements would be needed here
      \item ASCII character handling, portability, compilation warnings:
            we could automatically check that the compiler (for languages that
            meaningfully have a compile stage) doesn't output any warnings
            (e.g., by saving output to a file and checking it is what is
            expected from a normal compilation); do we have any string inputs?
\end{itemize}

\subsection[Correctness Proofs]{Correctness Proofs
      \citep[pp.~418-419]{vanVliet2000}}
Requires a formal specification \citep[p.~418]{vanVliet2000} and uses
``highly formal methods of logic'' \citep[p.~438]{PetersAndPedrycz2000} to prove
the existence of ``an equivalence between the program and its specification''
\citetext{p.~485}. It is not often used and its value is
``sometimes disputed'' \citep[p.~418]{vanVliet2000}.
\emph{Could be useful for Drasil down
      the road if we can specify requirements formally, and may overlap with
      others' interests in the areas of logic and proof-checking.}
\todo{Does symbolic execution belong here? Investigate from textbooks}

\section[Dynamic White-Box (Structural) Testing]{Dynamic White-Box (Structural)
  Testing \citep[pp.~105-121]{Patton2006}}

``Using information you gain from seeing what the code does and how it works to
determine what to test, what not to test, and how to approach the testing''
\citep[p.~106]{Patton2006}.

\subsection[Code Coverage or Control-Flow Coverage]{Code Coverage
      \citep[pp.~117-121]{Patton2006} or Control-Flow Coverage
      \citep[pp.~421-424]{vanVliet2000}}

``[T]est[ing] the program's states and the program's flow among them''
\citep[p.~117]{Patton2006}; allows for redundant and/or missing test
cases to be identified \citep[p.~118]{Patton2006}. Coverage-based
testing is often based ``on the notion of a control graph \dots [where]
nodes denote actions, \dots (directed) edges connect actions with
subsequent actions (in time) \dots [and a] path is a sequence of nodes
connected by edges. The graph may contain cycles \dots [which] correspond
to loops \dots'' \citep[pp.~420-421]{vanVliet2000}. ``A cycle is
called \emph{simple} if its inner nodes are distinct and do not include
      [the node at the beginning/end of the cycle]''
\citep[p.~421,~emphasis added]{vanVliet2000}. If there are
multiple actions represented as nodes that occur one after another, they may
be collapsed into a single node \citep[p.~421]{vanVliet2000}.

We discussed that
generating infrastructure for reporting coverage may be a worthwhile goal, and
that it can be known how to increase certain types of coverage (since we know
the structure of the generated code, to some extent, beforehand), but I'm
not sure if all of these are feasible/worthwhile to get to 100\% (e.g., path
coverage \citep[p.~421]{vanVliet2000}).

\begin{itemize}
      \item Statement/line coverage: attempting to ``execute every statement in
            the program at least once'' \citep[p.~119]{Patton2006}
            \begin{itemize}
                  \item Weaker than \citep[p.~421]{vanVliet2000} and
                        ``only about 50\% as effective as branch coverage''
                        \citep[p.~481]{PetersAndPedrycz2000}
                        \todo{OG Miller et al., 1994}
                  \item Requires 100\% coverage to be effective
                        \citep[p.~481]{PetersAndPedrycz2000}
                        \todo{OG Miller et al., 1994}
                  \item ``[C]an be used at the module level with less than 5000
                        lines of code''\footnote{The US Software Engineering
                              Institute has a checklist for determining which
                              types of lines of code are included when counting
                              \citep[pp.~30-31]{FentonAndPfleeger1997}.
                        } \citep[p.~481]{PetersAndPedrycz2000}
                        \todo{OG Miller et al., 1994}
                  \item Doesn't guarantee correctness
                        \citep[p.~421]{vanVliet2000}
            \end{itemize}
      \item Branch coverage: attempting to, ``at each branching node in the
            control graph, \dots [choose] all possible branches \dots at least
            once'' \citep[p.~421]{vanVliet2000}
            \begin{itemize}
                  \item Weaker than path coverage
                        \citep[p.~433]{vanVliet2000}, although
                        \citep[p.~119]{Patton2006} says it is
                        ``the simplest form of path testing''
                        (\emph{I don't think this is true})
                  \item Requires at least 85\% coverage to be effective and is
                        ``most effective \dots at the module level''
                        \citep[p.~481]{PetersAndPedrycz2000}
                        \todo{OG Miller et al., 1994}
                  \item Cyclomatic-number criterion: an adequacy criterion that
                        requires that ``all linearly-independent paths are
                        covered'' \citep[p.~423]{vanVliet2000};
                        results in complete branch coverage
                  \item Doesn't guarantee correctness
                        \citep[p.~421]{vanVliet2000}
            \end{itemize}
      \item Path coverage: ``[a]ttempting to cover all the paths in the
            software'' \citep[p.~119]{Patton2006};
            I always thought the ``path'' in ``path coverage'' was
            a path from program start to program end, but van
            Vliet seems to use the more general definition (which
            is, albeit, sometimes valid, like in ``du-path'') of
            being any subset of a program's execution (see
            \citep[p.~420]{vanVliet2000})
            \qtodo{How do we decide on our definition?}
            \begin{itemize}
                  \item The number of paths to test can be bounded based on its
                        structure and can be approached by dividing the system
                        into subgraphs and computing the bounds of each
                        individually \citep[pp.~471-473]{PetersAndPedrycz2000};
                        this is less feasible if a loop is present
                        \citep[pp.~473-476]{PetersAndPedrycz2000} since ``a loop
                        often results in an infinite number of possible paths''
                        \citep[p.~421]{vanVliet2000}
                  \item van Vliet claims that if this is done completely, it
                        ``is equivalent to exhaustively testing the program''
                        \citep[p.~421]{vanVliet2000};
                        however, this overlooks the effect of inputs on
                        behaviour as pointed out in
                        \citep[pp.~466-467]{PetersAndPedrycz2000}. Exhaustive
                        testing requires both full path coverage \emph{and}
                        every input to be checked
                  \item Generally ``not possible'' to achieve completely due to
                        the complexity of
                        loops, branches, and potentially unreachable code
                        \citep[p.~421]{vanVliet2000}; even infeasible
                        paths (``control flow paths that cannot be exercised by
                        any input data'' \citep[p.~5-5]{SWEBOK2024}) must be
                        checked for full path coverage to be achieved
                        \citep[p.~439]{PetersAndPedrycz2000}, presenting ``a
                        ``significant problem in path-based testing''
                        \citep[p.~5-5]{SWEBOK2024}!
                  \item Usually ``limited to a few functions with life
                        criticality features (medical systems, real-time
                        controllers)'' \citep[p.~481]{PetersAndPedrycz2000}
                        \todo{OG Miller et al., 1994}
            \end{itemize}
      \item (Multiple) condition coverage: ``takes the extra conditions on the
            branch statements into account'' (e.g., all possible inputs to a
            Boolean expression) \citep[p.~120]{Patton2006}
            \begin{itemize}
                  \item ``Also known as \textbf{extended branch coverage}''
                        \citep[p.~422]{vanVliet2000}
                  \item Does not subsume and is not subsumed by path coverage
                        \citep[p.~433]{vanVliet2000}
                  \item ``May be quite challenging'' since ``if each
                        subcondition is viewed as a single input, then this
                        \dots is analogous to exhaustive testing''; however,
                        there is usually a manageable number of subconditions
                        \citep[p.~464]{PetersAndPedrycz2000}
            \end{itemize}
\end{itemize}

\subsection[Data Coverage]{Data Coverage \citep[pp.~114-116]{Patton2006}}

In addition to \nameref{data-flow-coverage}, there are also some minor forms of
data coverage:

\begin{itemize}
      \item Sub-boundaries: mentioned previously in \ref{sub-bound-conds}
      \item Formulas and equations: related to
            \hyperref[comp-errors]{computation errors}
      \item Error forcing: setting variables to specific values to see how
            errors are handled; any error forced must have a chance of
            occurring in the real world, even if it is unlikely, and as such,
            must be double-checked for validity
            \citep[p.~116]{Patton2006}
\end{itemize}

\subsubsection[Data Flow Coverage]{Data Flow Coverage \citep[p.~114]{Patton2006},
      \citep[pp.~424-425]{vanVliet2000}}
\label{data-flow-coverage}

``[T]racking a piece of data completely through the software'' (or a part of
it), usually using debugger tools to check the values of variables
\citep[p.~114]{Patton2006}.

\begin{itemize}
      \item ``A variable is \emph{defined} in a certain statement if it is
            assigned a (new) value because of the execution of that
            statement'' \citep[p.~424]{vanVliet2000}
      \item ``A definition in statement X is \emph{alive} in statement Y if
            there exists a path from X to Y in which that variable does not
            get assigned a new value at some intermediate node''
            \citep[p.~424]{vanVliet2000}
      \item A path from a variable's definition to a statement where it is
            still alive is called \textbf{definition-clear} (with respect to
            this variable) \citep[p.~424]{vanVliet2000}
      \item Basic block: ``[a] consecutive part[] of code that execute[s]
            together without any branching'' \citep[p.~477]{PetersAndPedrycz2000}
      \item \acf{p-use}: e.g., the use of a variable in a conditional
            \citep[p.~424]{vanVliet2000}
      \item \acf{c-use}: e.g., the use of a variable in a computation or I/O
            statement \citep[p.~424]{vanVliet2000}
      \item All-use: either a \acs{p-use} or a \acs{c-use}~
            \citep[p.~478]{PetersAndPedrycz2000}
      \item DU-path: ``a path from a variable definition to [one of] its use[s]
            that contains no redefinition of the variable''
            \citep[pp.~478-479]{PetersAndPedrycz2000}
      \item The three possible actions on data are defining, killing, and using;
            ``there are a number of anomalies associated with these actions''
            \citep[pp.~478,~480]{PetersAndPedrycz2000}
            \todo{OG Beizer, 1990}
            (see \hyperref[data-ref-errors]{Data reference errors})
\end{itemize}

Table \ref{table:data-flow-coverage-types} contains different types of data
flow coverage criteria, approximately from weakest to strongest, as well as
their requirements; all information is adapted from
\citep[pp.~424-425]{vanVliet2000} \todo{Is this sufficient?}.

\begin{table}[hbtp!]
      \centering
      \caption{Types of Data Flow Coverage}
      \label{table:data-flow-coverage-types}
      \begin{tabularx}{\textwidth}{|>{\hsize=0.65\hsize}X|>{\hsize=1.35\hsize}X|}
            \hline
            \rowcolor{McMasterMediumGrey}
            \thead{Criteria}          & \thead{Requirements}                 \\
            \hline
            All-defs coverage         & Each definition to be used at least
            once                                                             \\
            All-\acsp{p-use} coverage & A definition-clear path from each
            definition to each \acs{p-use}                                   \\
            All-\acsp{p-use}/Some-\acsp{c-use}
            coverage                  & Same as All-\acsp{p-use} coverage,
            but if a definition is only used in computations, at least one
            definition-clear path to a \acs{c-use} must be included          \\
            All-\acsp{c-use}/Some-\acsp{p-use}
            coverage                  & A definition-clear path from each
            definition to each \acs{c-use}; if a definition is only used
            in predicates, at least one definition-clear path to a
            \acs{p-use} must be included                                     \\
            All-Uses coverage         & A definition-clear path between each
            variable definition to each of its uses and each of these uses'
            successors                                                       \\
            All-DU-Paths coverage     & Same as All-Uses coverage, but each
            path must be cycle-free or a simple cycle                        \\
            \hline
      \end{tabularx}
\end{table}

\qtodo{How is All-DU-Paths coverage stronger than All-Uses coverage according to
      \citep[p.~433]{vanVliet2000}?}

\subsection[Fault Seeding]{Fault Seeding \citep[pp.~427-428]{vanVliet2000}}

The introduction of faults to estimate the number of undiscovered faults in the
system based on the ratio between the number of new faults and the number of
introduced faults that were discovered (which will ideally be small)
\citep[p.~427]{vanVliet2000}. Makes many assumptions, including
``that both real and seeded faults have the same distribution'' and requires
careful consideration as to which faults are introduced and how
\citep[p.~427]{vanVliet2000}.

\subsection[Mutation Testing]{Mutation Testing \citep[pp.~428-429]{vanVliet2000}}
\label{chap:testing:sec:mutation-testing}

``A (large) number of variants of a program is generated'', each differing from
the original ``slightly'' (e.g., by deleting a statement or replacing an
operator with another) \citep[p.~428]{vanVliet2000}. These
\emph{mutants} are then tested; if set of tests fails to expose a difference in
behaviour between the original and many mutants, ``then that test set is of low
quality'' \citep[pp.~428-429]{vanVliet2000}. The goal is to maximize
the number of mutants identified by a given test set
\citep[p.~429]{vanVliet2000}. \textbf{Strong mutation testing} works
at the program level while \textbf{weak mutation testing} works at the
component level (and ``is often easier to establish'')
\citep[p.~429]{vanVliet2000}.

There is an unexpected byproduct of this form of testing. In some cases of one
experiment \todo{OG KA85}, ``the original program failed,
while the modified program [mutant] yielded the right result''
\citep[p.~432]{vanVliet2000}! In addition to revealing shortcomings
of a test set, mutation testing can also point the developer(s) in the
direction of a better solution!

\section[Gray-Box Testing]{Gray-Box Testing \citep[pp.~218-220]{Patton2006}}
A type of testing where ``you still test the software as a black-box, but you
supplement the work by taking a peek (not a full look, as in white-box testing)
at what makes the software work'' \citep[p.~218]{Patton2006}. An
example of this is looking at HTML code and checking the tags used since
``HTML doesn't execute or run, it just determines how text and graphics appear
onscreen'' \citep[p.~220]{Patton2006}.

\section{Regression Testing}
\label{chap:testing:sec:regression-testing}

Repeating ``tests previously executed \dots at a later point in development and
maintenance'' \citep[p.~446]{PetersAndPedrycz2000} ``to make sure there are no
unwanted changes [to the software's behaviour]'' \citetext{p.~481} (although
allowing ``some unwanted differences to pass through'' is sometimes desired, if
tedious \citetext{p.~482}). See also \citep[p.~232]{Patton2006}.

\begin{itemize}
      \item Should be done automatically \citep[p.~481]{PetersAndPedrycz2000};
            ``[t]est suite augmentation techniques specialise in
            identifying and generating'' new tests based on changes ``that add
            new features'' \todo{Investigate!}, but they could be extended to
            also augment ``the expected output'' and ``the existing
            \emph{oracles}'' \citep[p.~516]{BarrEtAl2015}
      \item Its ``effectiveness \dots is expressed in terms of'':
            \begin{enumerate}
                  \item difficulty of test suite construction and maintenance
                  \item reliability of the testing system
                        \citep[pp.~481-482]{PetersAndPedrycz2000}
            \end{enumerate}
      \item Various levels:
            \begin{itemize}
                  \item Retest-all: ``all tests are rerun''; ``this may consume
                        a lot of time and effort''
                        \citep[p.~411]{vanVliet2000} (\emph{shouldn't
                              take too much effort, since it will be automated,
                              but may lead to longer CI runtimes depending on
                              the scope of generated tests})
                  \item Selective retest: ``only some of the tests are rerun''
                        after being selected by a \emph{regression test
                              selection technique}; ``[v]arious strategies have
                        been proposed for doing so; few of them have been
                        implemented yet'' \citep[p.~411]{vanVliet2000}
                        \todo{Investigate these}
            \end{itemize}
\end{itemize}

% Hard-coded acronym to show up in navigation sidebar
\section[Metamorphic Testing (MT)]{\acf{mt}}
\label{chap:testing:sec:metamorphic-testing}
The use of \acfp{mr} ``to determine whether a test case has passed or failed''
\citep[p.~67]{KanewalaAndYuehChen2019}. ``A[n] \acs{mr} specifies how the
output of the program is expected to change when a specified change is made to
the input'' \citep[p.~67]{KanewalaAndYuehChen2019}; this is commonly done by
creating an initial test case, then transforming it into a new one by applying
the \acs{mr} (both the initial and the resultant test cases are executed and
should both pass) \citep[p.~68]{KanewalaAndYuehChen2019}. ``\acs{mt} is one of
the most appropriate and cost-effective testing techniques for scientists and
engineers'' \citep[p.~72]{KanewalaAndYuehChen2019}.

% Hard-coded acronym to show up in navigation sidebar
\subsection[Benefits of MT]{Benefits of \acs{mt}}
\begin{itemize}
      \item Easier for domain experts; not only do they understand the domain
            (and its relevant \acp{mr}) \citep[p.~70]{KanewalaAndYuehChen2019},
            they also may not have an understanding of testing principles
            \citep[p.~69]{KanewalaAndYuehChen2019}. \emph{This majorly
                  overlaps with Drasil!}
      \item Easy to implement via scripts \citep[p.~69]{KanewalaAndYuehChen2019}.
            \emph{Again, Drasil}
      \item Helps negate the test oracle \citep[p.~69]{KanewalaAndYuehChen2019}
            and output validation \citep[p.~70]{KanewalaAndYuehChen2019} problems
            from \nameref{chap:testing:sec:sci-testing-roadblocks} (\emph{i.e.,
                  the two that are relevant for Drasil})
      \item Can extend a limited number of test cases (e.g., from an
            experiment that was only able to be conducted a few times)
            \citep[pp.~70-72]{KanewalaAndYuehChen2019}
      \item Domain experts are sometimes unable to identify faults in a program
            based on its output \citep[p.~71]{KanewalaAndYuehChen2019}
\end{itemize}

% Hard-coded acronym to show up in navigation sidebar
\subsection[Examples of MT]{Examples of \acs{mt}}
\begin{itemize}
      \item The average of a list of numbers should be equal (within
            floating-point errors) regardless of the list's order
            \citep[p.~67]{KanewalaAndYuehChen2019}
      \item For matrices, if $B = B_1 + B_2$, then $A \times B = A \times B_1
                  + A \times B_2$ \citep[pp.~68-69]{KanewalaAndYuehChen2019}
      \item Symmetry of trigonometric functions; for example, $\sin(x) = \sin(-x)$
            and $\sin(x) = \sin(x + 360^{\circ})$ \citep[p.~70]{KanewalaAndYuehChen2019}
      \item Modifying input parameters to observe expected changes to a model's
            output (e.g., testing epidemiological models calibrated with
            ``data from the 1918 Influenza outbreak''); by ``making changes to
            various model parameters \dots authors identified an error in the
            output method of the agent based epidemiological model''
            \citep[p.~70]{KanewalaAndYuehChen2019}
      \item Using machine learning to predict likely \acsp{mr} to identify
            faults in mutated versions of a program (about 90\% in this case)
            \citep[p.~71]{KanewalaAndYuehChen2019}
\end{itemize}

\section{Roadblocks to Testing}

\begin{itemize}
      \item Intractability: it is generally impossible to test a program
            exhaustively \exhInfCite{}
      \item Adequacy: to counter the issue of intractability, it is desirable
            ``to reduce the cardinality of the test suites while keeping the
            same effectiveness in terms of coverage or fault detection rate''
            \citep[p.~5-4]{SWEBOK2024} which is difficult to do objectively;
            see also ``minimization'', the process of ``removing redundant test
            cases'' \citep[p.~5-4]{SWEBOK2024}
      \item Undecidability \citep[p.~439]{PetersAndPedrycz2000}: it is
            impossible to know certain properties about a program, such as if
            it will halt (i.e., the Halting Problem
            \citep[p.~4]{gurfinkel_testing_2017}), so ``automatic testing
            can't be guaranteed to always work'' for all properties
            \citep{nelson_formal_1999} \todo{Add paragraph/section number?}
\end{itemize}

\subsection[Roadblocks to Testing Scientific Software]
{Roadblocks to Testing Scientific Software
      \citep[p.~67]{KanewalaAndYuehChen2019}}
\label{chap:testing:sec:sci-testing-roadblocks}
\begin{itemize}
      \item ``Correct answers are often unknown'': if the results were already
            known, there would be no need to develop software to model them
            \citep[p.~67]{KanewalaAndYuehChen2019}; in other words, complete
            test oracles don't exist ``in all but the most trivial cases''
            \citep[p.~510]{BarrEtAl2015}, and even if they are, the
            ``automation of mechanized oracles can be difficult and expensive''
            \citep[p.~5.5]{SWEBOK2024}
      \item ``Practically difficult to validate the computed output'': complex
            calculations and outputs are difficult to verify
            \citep[p.~67]{KanewalaAndYuehChen2019}
      \item ``Inherent uncertainties'': since scientific software models
            scenarios that occur in a chaotic and imperfect world, not every
            factor can be accounted for \citep[p.~67]{KanewalaAndYuehChen2019}
      \item ``Choosing suitable tolerances'': difficult to decide what
            tolerance(s) to use when dealing with floating-point numbers
            \citep[p.~67]{KanewalaAndYuehChen2019}
      \item ``Incompatible testing tools'': while scientific software is often
            written in languages like FORTRAN, testing tools are often written
            int languages like Java or C++ \citep[p.~67]{KanewalaAndYuehChen2019}
\end{itemize}

Out of this list, only the first two apply. The scenarios modelled by Drasil
are idealized and ignore uncertainties like air resistance, wind direction,
and gravitational fluctuations. There are not any instances where special
consideration for floating-point arithmetic must be taken; the default
tolerance used for relevant testing frameworks has been used
\todo{Add example} and is likely sufficient for future testing. On a related
note, the scientific software we are trying to test is already generated in
languages with widely-used testing frameworks. \todo{Add source(s)?}
