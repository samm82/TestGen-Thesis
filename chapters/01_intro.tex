\section{Introduction}

% TODO: tighten up, add sources

Testing software is complicated, expensive, and often overlooked. The
productivity of testing and testing research would benefit from a standard
language for communication. For example, \citet[p.~7]{KanerEtAl2011}
\multAuthHelper{give} the example of complete testing, which could require the
tester to discover ``every bug in the product'', exhaust the time allocated to
the testing phase, or simply implement every test previously agreed upon.
% They go on to say that
Having a clear  definition for ``complete testing'' reduces the chance for
miscommunication and, ultimately, the tester getting ``blamed for not
doing \dots{} [their] job'' \citep[p.~7]{KanerEtAl2011}. These benefits can be
extrapolated to software testing terminology as a whole and are even more
pronounced when seeking to automatically generate test cases.
\ifnotpaper Our own project Drasil~\citep{Drasil} is aimed at ``generating all
    of the software artifacts for (well understood) research software''. Before
    test cases can be included as a generated artifact, the domain of testing,
    including known testing approaches, needs to be ``well understood''. \fi
Unfortunately, a search for a systematic, rigorous, and ``complete'' taxonomy
for software testing revealed that the existing ones are inadequate:

\begin{itemize}
    \item \ifnotpaper\else Tebes et al. \fi\citet{TebesEtAl2020a} focus on
          \emph{parts} of the testing process (e.g., test goal, testable entity),
    \item \ifnotpaper\else Souza et al. \fi\citet{SouzaEtAl2017} prioritize
          organizing testing approaches over defining them, and
    \item \ifnotpaper\else Unterkalmsteiner et al. \fi\citet{UnterkalmsteinerEtAl2014}
          focus on the ``information linkage or transfer'' \citetext{p.~A:6}
          between requirements engineering and software testing.
\end{itemize}

\ifnotpaper
    Some existing collections of software testing terminology were found, but
    in addition to being incomplete, they also contained many oversights. For
    example, \citet{IEEE2017} \multAuthHelper{provide} the following
    term-definition pairs:
    \begin{enumerate}
        \item \textbf{Event Sequence Analysis:} ``per'' \citetext{p.~170}
        \item \textbf{Operable:} ``state of'' \citetext{p.~301}
              % This may be a bad example, since the cf. provides some more context
              % \item \textbf{Software Element:} a ``system element that is software''
              %       \citetext{p.~421}
    \end{enumerate}
    These definitions are nonsensical and do not define the terms they claim
    to! To be sure, they \emph{cannot} correspond to the terms given since the
    parts of speech are mismatched: the first defines a noun phrase as a
    preposition, and the second an adjective as a noun phrase fragment.
    \citet{IEEE2017} also \multAuthHelper{define} ``device'' as a ``mechanism
    or piece of equipment designed to serve a purpose or perform a function''
    \citetext{p.~136}, but do\ifnotpaper\else{es}\fi\ not define ``equipment''
    and only \multAuthHelper{define} ``mechanism'' in the software sense as
    ``the means used by a function to transform input into output''
    \citetext{p.~270\todo{OG IEEE 1998}}. This is an example of an incomplete
    definition; while the definition of ``device'' seems logical at first
    glance, it actually leaves much undefined.
\fi

We set about closing this gap in the literature.
\ifnotpaper
    The following three research questions guide this process:
    \begin{enumerate}
        \item What testing approaches are described in the literature?
        \item What discrepancies exist between descriptions of these testing
              approaches?
        \item Can any of these discrepancies be resolved/reduced systematically?
    \end{enumerate}
    We start by recording the \approachCount{} test approaches mentioned by
    \srcCount{} \nameref{sources}, recording their name, category (see
    \Cref{categories-observ}), definition, parent(s) (see \Cref{par-chd-rels})
    and synonym(s), if any. Any other notes, such as uncertainties,
    prerequisites, and other resources to investigate, are also recorded. An
    excerpt of some of these approaches, along with their recorded information
    (excluding other notes for brevity), is given in
    \Cref{tab:approachGlossaryExcerpt}.

    \begin{bigLandscape}
        \input{assets/tables/exampleGlossary}
    \end{bigLandscape}

\fi
This document describes this process, as well as its results, in more detail.
We first define the scope of
what kinds of software testing are of interest (\Cref{scope}) and examine the
existing literature (\Cref{methodology})\ifnotpaper, partially through the use
of tools created for analysis (\Cref{tools})\fi. Despite the amount of well
understood and organized knowledge, there are still many discrepancies and
ambiguities in the literature, either within the same source or between various
sources (\Cref{discrep}). This reinforces the need for a proper taxonomy! We
provide some potential solutions covering some of these discrepancies
(\Cref{recs}).