\section{Scope}
\label{scope}

\newcommand{\acsVNV}{\ifnotpaper \acs{vnv}\else V\&V\fi}

Since the motivation for this project is the generation of test cases for code,
only the ``testing'' component of
\ifnotpaper \acf{vnv} \else Verification and Validation \acf{V\&V} \fi is
considered (see \thesisissueref{22}).
For example, design reviews \ifnotpaper \citep[see][p.~132]{IEEE2017}
\else (see \cite[p.~132]{IEEE2017}) \fi and documentation reviews
\ifnotpaper \citetext{see p.~132} \else (see \citetext{p.~132}) \fi
% \citep[see][p.~144]{IEEE2017}
are out of scope, since they focus on the \acsVNV{}
of the design and documentation of the code,
respectively, and not on the code itself. Likewise, ergonomics testing
and proximity-based testing (see \citealpISTQB{}) are out of scope since
they are for testing hardware systems%
\ifnotpaper%
    , as is \acf{emsec} testing
    (\citealp{ISO2021}; \citealp[p.~95]{ZhouEtAl2012}), which deals with the
    ``security risk'' of ``information leakage via electromagnetic emanation''
    \citep[p.~95]{ZhouEtAl2012}%
\fi. Security audits that focus on ``an organization's
\dots\ processes and infrastructure'' \citepISTQB{}, are also out of scope,
but security audits that ``aim to ensure that all of the products installed on
a site are secure when checked against the known vulnerabilities for those
products'' \citep[p.~28]{Gerrard2000b} are not.
\ifnotpaper While \acf{oat}
    can be used when testing software \citep{Mandl1985}, it can also be used for
    hardware \citep[pp.~471-472]{Valcheva2013}, such as ``processors \dots\ made
    from pre-built and pre-tested hardware components'' (p.~471). A subset of
    \acs{oat} called ``\acf{toat}'' is used for ``experimental design problems in
    manufacturing'' \citep[p.~1573]{YuEtAl2011} or ``product and manufacturing
    process design'' \cite[p.~44]{Tsui2007} and is thus out of scope. \fi

Sometimes, wider decisions must be made on whether a whole category of
testing is in scope or not. For example, while all the examples of domain-specific
testing given by \citet[p.~26]{Firesmith2015} are focused on hardware, this
might not be representative of all types (e.g., ML model testing seems
domain-specific).
\ifnotpaper
    Conversely, the examples of environmental tolerance testing
    (p.~56) do not seem to apply to software. For example, radiation tolerance
    testing seems to focus on hardware, such as motors \citep{MukhinEtAl2022},
    robots \citep{ZhangEtAl2020}, or ``nanolayered carbide and nitride materials''
    \citep[p.~1]{TunesEtAl2022}. Acceleration tolerance testing seems to focus on
    \accelTolTest{} and acoustic tolerance testing on rats \citep{HolleyEtAl1996},
    which are even less related! Since these all seem to focus on
    environment-specific factors that would not impact the code, this category of
    testing is also out of scope.

    It is also interesting to note that different test approaches seem to be more
    specific to certain domains. For example, the terms ``software qualification
    testing'' and ``system qualification testing'' show up throughout
    \citep{SPICE2022}, which was written for the automotive industry, and the more
    general idea of ``qualification testing'' seems to refer to the process of
    making a hardware component, such as an electronic component
    \citep{AhsanEtAl2020}, gas generator \citep{ParateEtAl2021} or photovoltaic
    device, ``into a reliable and marketable product'' \citep[p.~1]{SuhirEtAl2013}.
\fi

This also means that only some aspects of some testing approaches are relevant.
This mainly manifests as a testing approach that can verify both the \acsVNV{}
itself and the code. For example:

\begin{enumerate}
    \item \emph{Error seeding} is the ``process of intentionally adding
          known faults to those already in a computer program'',
          done to both ``monitor[] the rate of detection and removal'',
          which is a part of \acsVNV{} of the \acsVNV{} itself, ``and
          estimat[e] the number of faults remaining''
          \citep[p.~165]{IEEE2017}, which helps verify the actual code.
    \item \emph{Fault injection testing}, where ``faults are artificially
          introduced into the \acs{sut}'', can be used to evaluate the
          effectiveness of a test suite \citep[p.~5-18]{SWEBOK2024},
          which is a part of \acsVNV{} of the \acsVNV{} itself, or ``to test
          the robustness of the system in the event of internal and
          external failures'' \citep[p.~42]{IEEE2022}, which helps verify
          the actual code.
    \item ``\emph{Mutation [t]esting} was originally conceived as a
          technique to evaluate test suites in which a mutant is a slightly
          modified version of the \acs{sut}'' \citep[p.~5-15]{SWEBOK2024},
          which is in the realm of \acsVNV{} of the \acsVNV{} itself.
          However, it ``can also be categorized as a structure-based
          technique'' and can be used to assist fuzz and metamorphic testing
          \citep[p.~5-15]{SWEBOK2024}.
          \ifnotpaper
    \item Even though \emph{reliability testing} and \emph{maintainability
              testing} can start \emph{without} code by ``measur[ing]
          structural attributes of representations of the software''
          \citep[p.~18]{FentonAndPfleeger1997}, only reliability and
          maintainability testing done \emph{on} code is in scope.
    \item Since control systems often have a software \emph{and} hardware
          component \citep{ISO2015, PreußeEtAl2012,ForsythEtAl2004},
          only the software component is in scope. In some cases, it is
          unclear whether the ``loops''\footnote{Humorously, the testing of
              loops in chemical systems \citep{Dominguez-PumarEtAl2020} and
              copper loops \citep{Goralski1999} are out of scope.} being
          tested are implemented by software or hardware, such as those in
          wide-area damping controllers \citep{PierreEtAl2017, TrudnowskiEtAl2017}.
          \begin{itemize}
              \item A related note: ``path coverage'' or ``path testing''
                    seems to be able to refer to either paths through code
                    (as a subset of control-flow testing)
                    \citep[p.~5-13]{SWEBOK2024} or through a model, such as
                    a finite-state machine (as a subset of model-based
                    testing) \citep[p.~184]{DoğanEtAl2014}.
          \end{itemize}
          \fi
\end{enumerate}

\ifnotpaper
    Specific programming languages are sometimes used to define ``kinds'' of
    testing. These will not be included (see \thesisissueref{63}); if the reliance
    on a specific programming language is intentional, then this really implies an
    underlying test approach that may be generalized to other languages. Some
    examples:

    \begin{itemize}
        \item ``They implemented an approach \dots\ for JavaScript testing
              (referred to as Randomized)'' \citep[p.~192]{DoğanEtAl2014} -
              this really refers to random testing used within JavaScript
        \item ``SQL statement coverage'' is really just statement coverage
              used specifically for SQL statements \citep[Tab.~13]{DoğanEtAl2014}
              \todo{OG Alalfi et al., 2010}
        \item ``Faults specific to PHP'' is just a subcategory of fault-based
              testing, since ``execution failures \dots\ caused by missing an
              included file, wrong MySQL quer[ies] and uncaught exceptions''
              are not exclusive to PHP \citep[Tab.~27]{DoğanEtAl2014}
              \todo{OG Artzi et al., 2008}
        \item While ``HTML testing'' is listed or implied by
              \citeauthor{Gerrard2000a} (\citeyear[Tab.~2]{Gerrard2000a};
              \citeyear[Tab.~1, p.~3]{Gerrard2000b}) and
              \citet[p.~220]{Patton2006}, it seems to be a combination of syntax
              testing, functionality testing, hyperlink testing/link checking,
              cross-browser compatibility testing, performance testing, and
              content checking \citep[p.~3]{Gerrard2000b}
    \end{itemize}
\fi

\subsection{Static Testing}
\label{static-test}
Sometimes, the term ``testing'' excludes static testing
\ifnotpaper
    (\citealp[p.~222]{AmmannAndOffutt2017}; \citealp[p.~13]{Firesmith2015})%
\else
    \cite[p.~222]{AmmannAndOffutt2017}, \cite[p.~13]{Firesmith2015}%
\fi, restricting it to ``dynamic validation'' \citep[p.~5-1]{SWEBOK2024} or
``dynamic verification'' ``in which a system or component is
executed'' \citep[p.~427]{IEEE2017}. Since ``terminology is not uniform
among different communities, and some use the term \emph{testing} to refer to
static techniques\notDefDistinctIEEE{technique} as well''
\citep[p.~5-2]{SWEBOK2024}, the scope of ``testing'' for the purpose of this
project will include both ``static testing'' and ``dynamic testing'', as
done by \citet[p.~17]{IEEE2022}, \citet[pp.~8-9]{Gerrard2000a}, and even a
source that explicitly excluded static testing \citep[p.~440]{IEEE2017}!

Static testing tends to be less
systematic/consistent and often requires human intervention, which makes it
less relevant to this project's end goal: generating test cases automatically.
However, understanding the breadth of testing approaches provides a more
complete picture of how software can be tested, how the various approaches are
related to one another, and potentially how even parts of these ``out-of-scope''
approaches may be generated in the future! Therefore, these ``out-of-scope''
approaches are within the scope of this research (at least in the preliminary
phase) and will be identified systematically and excluded before analysis. Even
some dynamic methods, such as demonstrations and dynamic analysis, which fall
under the realm of ``evaluation'' as opposed to ``testing''
\citep[p.~13]{Firesmith2015} may be ineligible for automatic generation, due
to their reliance on human intervention.