\section{Observations}
\label{observ}

\subsection{Categories of Testing Approaches}

\ifnotpaper
      \newgeometry{margin=1.5cm, top=2.5cm}
      \begin{landscape}
            \ieeeTestTermsTable{}
      \end{landscape}

      \begin{landscape}
            \otherTestTermsTable{}
      \end{landscape}
      \restoregeometry
\else
      \ieeeTestTermsTable{}
\fi

Different sources categorize software testing approaches in different ways%
\ifnotpaper
      ; while it is useful to record and think about these
      categorizations\seeSectionPar{testing-categories}, following one (or more)
      during the research
      stage could lead to bias and a prescriptive categorization, instead of letting
      one emerge descriptively during the analysis stage. Since these categorizations
      are not mutually exclusive, it also means that more than one could be useful
      (both in general and to this specific project).\newline \else.\fi\
\ifnotpaper \citet{IEEE2022} provide \else Reference \cite{IEEE2022} provides
\fi a classfication for different kinds of tests (see \refIEEETestTerms{}).
\ifnotpaper\else Since
      this seems to be widely used (``test level'' and ``test type'' in particular)
      and is useful when focusing on a particular subset of testing, this terminology
      is used for now. A deeper rationale for a
      proposed classification will be given during the analysis stage. \fi

\ifnotpaper
      However, other sources \citep{BarbosaEtAl2006, SouzaEtAl2017}
      % \cite{BarbosaEtAl2006, SouzaEtAl2017}
      provide alternate categories
      (see \refOtherTestTerms{}) which may be beneficial to investigate to
      determine if this categorization is sufficient.
      % \fi 
      % \ifnotpaper
      A ``metric'' categorization was considered at one point, but was decided
      to be out of the scope of this project
      \seeSectionPar{scope}{, \thesisissueref{21}, and
            \thesisissueref{22}}.
\fi
Related testing approaches may be grouped into a ``class'' or ``family'' to
group those with ``commonalities and well-identified variabilities that can be
instantiated'', where ``the commonalities are large and the variabilities
smaller''\seeThesisIssuePar{64}. Examples of these are the classes of
combinatorial \citep[p.~15]{IEEE2021} and data flow testing \citetext{p.~3} and the
family of performance-related testing \cite[p.~1187]{Moghadam2019}\footnote{The
      original source describes ``performance testing \dots\ as a family of
      performance-related testing techniques'', but it makes more sense to
      consider ``performance-related testing'' as the ``family'' with
      ``performance testing'' being one of the
      variabilities\seeSectionPar{perf-test-rec}.}, and may also be
implied for security testing, a test type that consists of ``a number of
techniques\footnote{This may or may not be \distinctIEEE{technique}}''
\cite[p.~40]{IEEE2021}.

It also seems that these categories are orthogonal. For example, ``a test type
can be performed at a single test level or across several test levels''
\ifnotpaper
      (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021})%
\else
      \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021}%
\fi. Due to this, a specific
test approach can be derived by combining test approaches from different
categories;\seeSection{orthogonal-tests} for some examples
of this.

\subsection{Derived Test Approaches}
\label{derived-tests}

In addition to methods of categorizing test approaches, the literature also
provides multiple methods to derive new ones. Since the field of software is
ever-evolving, being able to adapt to new developments, as well as being able
to talk about and understand them, is crucial.

\subsubsection{Coverage-driven Techniques}
\label{tech-cov}

Test techniques are able to ``identify test coverage items \dots\ and
derive corresponding test cases''
\ifnotpaper
      (\citealp[p.~11]{IEEE2022}; similar in \citeyear[p.~467]{IEEE2017})
\else
      \cite[p.~11]{IEEE2022} (similar in \cite[p.~467]{IEEE2017})
\fi
in a ``systematic'' way
\citeyearpar[p.~464]{IEEE2017}.
\ifnotpaper
      This allows for ``the coverage achieved by a specific test
      design technique'' to be calculated as ``the number of test coverage items
      covered by executed test cases'' divided by ``the total number of test
      coverage items identified'' \citeyearpar[p.~30]{IEEE2021}.
      ``Coverage levels can range
      from 0\% to 100\%'' and may or may not include ``infeasible'' test coverage
      items, which are ``not \dots\ executable or [are] impossible to be covered by a
      test case'' \citetext{p.~30}. Perhaps more interestingly, the further
      implication is
\else
      This means
\fi
that a given
coverage metric implies a test approach aimed to maximize it; for example,
``path testing'' is testing that ``aims to execute all entry-to-exit
control flow paths in a SUT's control flow graph'' \citep[p.~5013]{SWEBOK2024},
thus maximizing the path coverage (see also
\ifnotpaper\thesisissueref{63}, \fi \citep[Fig.~1]{SharmaEtAl2021}).

\subsubsection{Quality-driven Types}

Since test types are ``focused on specific quality characteristics''
\ifnotpaper
      (\citealp[p.~15]{IEEE2022}; \citeyear[p.~7]{IEEE2021};
      \citeyear[p.~473]{IEEE2017}\todo{OG IEEE 2013})%
\else
      \cite[p.~15]{IEEE2022}, \cite[p.~7]{IEEE2021}, \cite[p.~473]{IEEE2017}%
\fi, they can derived from software qualities: ``capabilit[ies] of
software product[s] to satisfy stated and implied needs when used under
specified conditions'' \citep[p.~424]{IEEE2017}\todo{OG ISO/IEC 2014}. This
is supported by reliability and performance testing, which are both examples of
test types \citep{IEEE2022, IEEE2021} that are based on their underlying
qualities \citep[p.~18]{FentonAndPfleeger1997}.
\ifnotpaper
      For quantifying quality-driven testing, measurements should include
      an entity to be measured, a specific attribute to measure, and the actual
      measure (i.e., units, starting state, ending state, what to include)
      \citeyearpar[p.~36]{FentonAndPfleeger1997} where attributes must be
      defined before they can be measured \citetext{p.~38}.
\fi

After discussing this further\seeThesisIssuePar{21}[ and \thesisissueref{23}],
it was decided that tracking
software qualities, in addition to testing approaches, would be worthwhile%
\seeThesisIssuePar{27}. This was done by capturing their definitions and any
rationale for why it might be useful to consider an explicitly separate
``test type'' in a separate document, so this information could be captured
without introducing clutter. Over time, software qualities were ``upgraded''
to test types when mentioned (or implied) by a source.\todo{Find examples?}

\subsubsection{Requirements-driven Approaches}
While not as universally applicable, some types of requirements have associated
types of testing (e.g., functional, non-functional, security). This may mean
that categories of requirements \emph{also} imply related testing approaches
(such as ``technical testing''). Even assuming this is the case, some types of
requirements do not apply to the code itself, and as such are out of scope%
\seeThesisIssuePar{43}:

\begin{itemize}
      \item \textbf{Nontechnical Requirement:} a ``requirement affecting product
            and service acquisition or development that is not a property of
            the product or service'' \citep[p.~293]{IEEE2017}
      \item \textbf{Physical Requirement:} a ``requirement that specifies a
            physical characteristic that a system or system component must
            possess'' \citep[p.~322]{IEEE2017}
\end{itemize}

\subsubsection{Attacks}
\label{attacks}
Since attacks are given as a test practice \citep[p.~34]{IEEE2022}, different
kinds of software attacks, such as code injection and password cracking, can
also be used as test approaches.
